<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The Story of NLP: From Rules to Reasoning</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'Inter', sans-serif;
            background-color: #111827; /* Dark background */
            color: #F9FAFB; /* Light text */
            overflow: hidden;
        }
        .slide {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: center;
            padding: 4rem;
            opacity: 0;
            transition: opacity 0.6s ease-in-out, transform 0.6s ease-in-out;
            transform: scale(0.95);
            visibility: hidden;
        }
        .slide.active {
            opacity: 1;
            transform: scale(1);
            visibility: visible;
            z-index: 10;
        }
        .slide-content {
            max-width: 1000px;
            width: 100%;
        }
        .progress-bar {
            position: fixed;
            bottom: 0;
            left: 0;
            height: 6px;
            background-color: #3b82f6;
            transition: width 0.3s ease;
            z-index: 50;
        }
        .nav-button {
            position: fixed;
            bottom: 2rem;
            z-index: 20;
            background-color: rgba(31, 41, 55, 0.8);
            color: white;
            border: 1px solid #4B5563;
            border-radius: 9999px;
            padding: 0.75rem;
            cursor: pointer;
            transition: background-color 0.2s;
        }
        .nav-button:hover {
            background-color: #374151;
        }
        #prev-btn {
            left: 2rem;
        }
        #next-btn {
            right: 2rem;
        }
        .slide-counter {
            position: fixed;
            bottom: 2rem;
            left: 50%;
            transform: translateX(-50%);
            z-index: 20;
            background-color: rgba(31, 41, 55, 0.8);
            color: #D1D5DB;
            padding: 0.5rem 1rem;
            border-radius: 9999px;
            font-size: 0.875rem;
        }
        .analogy {
            background-color: #1F2937;
            border-left: 4px solid #3b82f6;
            padding: 1rem;
            border-radius: 0.5rem;
            margin-top: 1.5rem;
        }
        .analogy-title {
            font-weight: 600;
            color: #60a5fa;
            display: flex;
            align-items: center;
        }
        .code-snippet {
            background-color: #1E1E1E;
            color: #D4D4D4;
            padding: 1rem;
            border-radius: 0.5rem;
            font-family: 'Courier New', Courier, monospace;
            white-space: pre-wrap;
            margin-top: 1rem;
        }
        .vector-math span {
            font-weight: 700;
            color: #93c5fd;
        }
        .attention-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(100px, 1fr));
            gap: 1rem;
            margin-top: 1.5rem;
        }
        .attention-word {
            background-color: #374151;
            padding: 0.5rem;
            border-radius: 0.375rem;
            text-align: center;
        }
        .attention-arrow {
            position: absolute;
            width: 100%;
            height: 100%;
            top: 0;
            left: 0;
        }
    </style>
</head>
<body>

    <!-- Slide 1: Title -->
    <div id="slide-1" class="slide active text-center">
        <div class="slide-content">
            <h1 class="text-5xl md:text-7xl font-bold tracking-tight text-transparent bg-clip-text bg-gradient-to-r from-blue-400 to-indigo-600">The Story of NLP</h1>
            <p class="mt-4 text-2xl md:text-3xl text-gray-300">From Rigid Rules to Fluid Reasoning</p>
            <p class="mt-8 text-lg text-gray-400">A brief journey through the evolution of Natural Language Processing</p>
        </div>
    </div>

    <!-- Slide 2: The "Dark Ages" - Rule-Based NLP -->
    <div id="slide-2" class="slide">
        <div class="slide-content">
            <h2 class="text-sm font-semibold uppercase tracking-widest text-blue-400">Chapter 1: The Age of Rules (~1950s-1990s)</h2>
            <h1 class="text-4xl md:text-5xl font-bold mt-2">The Meticulous Librarian</h1>
            <p class="mt-4 text-xl text-gray-300">Early NLP relied on handcrafted rules, grammars, and lexicons.</p>
            <ul class="mt-6 list-disc list-inside space-y-2 text-lg text-gray-400">
                <li>Manually coded linguistic knowledge.</li>
                <li>Example: Keyword matching for search.</li>
                <li>Struggled with ambiguity and scale.</li>
            </ul>
            <div class="analogy">
                <p class="analogy-title">
                    <svg xmlns="http://www.w3.org/2000/svg" class="h-6 w-6 mr-2" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M16 4v12l-4-2-4 2V4M6 20h12a2 2 0 002-2V6a2 2 0 00-2-2H6a2 2 0 00-2 2v12a2 2 0 002 2z" /></svg>
                    Storyteller's Analogy
                </p>
                <p class="mt-2 text-gray-300">Imagine a librarian who can only find books by their exact title. If you ask for "a story about a big white fish," they're lost. You must ask for "Moby Dick." It's precise, but brittle.</p>
            </div>
        </div>
    </div>

    <!-- Slide 3: Statistical NLP -->
    <div id="slide-3" class="slide">
        <div class="slide-content">
            <h2 class="text-sm font-semibold uppercase tracking-widest text-blue-400">Chapter 2: The Statistical Renaissance (~1990s-2010s)</h2>
            <h1 class="text-4xl md:text-5xl font-bold mt-2">Learning from Data</h1>
            <p class="mt-4 text-xl text-gray-300">Instead of rules, models started learning patterns from large text corpora.</p>
             <ul class="mt-6 list-disc list-inside space-y-2 text-lg text-gray-400">
                <li>Key concepts: Bag-of-Words, TF-IDF, N-grams.</li>
                <li>Probabilistic models determined the likelihood of sequences.</li>
                <li>Still lacked a deep understanding of *meaning* or context.</li>
            </ul>
            <div class="analogy">
                <p class="analogy-title">
                    <svg xmlns="http://www.w3.org/2000/svg" class="h-6 w-6 mr-2" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M16 4v12l-4-2-4 2V4M6 20h12a2 2 0 002-2V6a2 2 0 00-2-2H6a2 2 0 00-2 2v12a2 2 0 002 2z" /></svg>
                    Storyteller's Analogy
                </p>
                <p class="mt-2 text-gray-300">Our librarian can now count words. If a book has "whale," "ship," and "sea" many times, they guess it's a nautical story. It's better, but they don't understand the *relationship* between the words. "A ship hunts a whale" and "A whale hunts a ship" look the same.</p>
            </div>
        </div>
    </div>
    
    <!-- Slide 4: Word Embeddings -->
    <div id="slide-4" class="slide">
        <div class="slide-content">
            <h2 class="text-sm font-semibold uppercase tracking-widest text-blue-400">Chapter 3: The Enlightenment of Meaning (~2013)</h2>
            <h1 class="text-4xl md:text-5xl font-bold mt-2">Words as Vectors</h1>
             <p class="mt-4 text-xl text-gray-300">Word2Vec and GloVe gave words dense vector representations, capturing semantic relationships.</p>
            <div class="mt-6 p-6 bg-gray-800 rounded-lg text-center">
                <p class="text-xl md:text-2xl font-mono text-gray-300 tracking-wider">
                    <span class="text-blue-300">vector('King')</span> 
                    <span class="text-red-400">- vector('Man')</span> 
                    <span class="text-green-400">+ vector('Woman')</span> 
                    <span class="text-gray-400">â‰ˆ</span> 
                    <span class="text-blue-300">vector('Queen')</span>
                </p>
            </div>
            <div class="analogy">
                 <p class="analogy-title">
                    <svg xmlns="http://www.w3.org/2000/svg" class="h-6 w-6 mr-2" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M16 4v12l-4-2-4 2V4M6 20h12a2 2 0 002-2V6a2 2 0 00-2-2H6a2 2 0 00-2 2v12a2 2 0 002 2z" /></svg>
                    Storyteller's Analogy
                </p>
                <p class="mt-2 text-gray-300">The librarian now has a map of the library where books with similar themes are placed close together. More importantly, the *directions* between books have meaning. The path from "Royalty" to "Female Royalty" is the same as the path from "Male" to "Female." This was a huge leap.</p>
            </div>
        </div>
    </div>
    
    <!-- Slide 5: RNNs/LSTMs -->
    <div id="slide-5" class="slide">
        <div class="slide-content">
            <h2 class="text-sm font-semibold uppercase tracking-widest text-blue-400">Chapter 4: The Age of Sequence (~2014-2017)</h2>
            <h1 class="text-4xl md:text-5xl font-bold mt-2">Memory and Context</h1>
            <p class="mt-4 text-xl text-gray-300">Recurrent Neural Networks (RNNs) and LSTMs process text sequentially, word by word, remembering past information.</p>
            <ul class="mt-6 list-disc list-inside space-y-2 text-lg text-gray-400">
                <li>Processed sequences, ideal for translation and text generation.</li>
                <li>LSTMs solved the "vanishing gradient" problem, allowing longer memory.</li>
                <li>However, they were slow to train (sequential) and struggled with very long-range dependencies.</li>
            </ul>
             <div class="analogy">
                 <p class="analogy-title">
                    <svg xmlns="http://www.w3.org/2000/svg" class="h-6 w-6 mr-2" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M16 4v12l-4-2-4 2V4M6 20h12a2 2 0 002-2V6a2 2 0 00-2-2H6a2 2 0 00-2 2v12a2 2 0 002 2z" /></svg>
                    Storyteller's Analogy
                </p>
                <p class="mt-2 text-gray-300">Our librarian can now read a book one sentence at a time. As they read, they keep a running summary in their head. This helps them understand context. But if a key detail was mentioned in Chapter 1, they might forget it by Chapter 20.</p>
            </div>
        </div>
    </div>

    <!-- Slide 6: The Transformer Revolution -->
    <div id="slide-6" class="slide">
        <div class="slide-content">
            <h2 class="text-sm font-semibold uppercase tracking-widest text-blue-400">Chapter 5: The Revolution (2017)</h2>
            <h1 class="text-4xl md:text-5xl font-bold mt-2">"Attention Is All You Need"</h1>
            <p class="mt-4 text-xl text-gray-300">The Transformer architecture abandoned recurrence for a powerful mechanism: <span class="text-blue-300 font-semibold">Self-Attention</span>.</p>
            <p class="mt-6 text-lg text-gray-400">Instead of processing word-by-word, attention allows the model to weigh the importance of all other words in the input for each word it processes, all at once.</p>
            <div class="analogy">
                 <p class="analogy-title">
                    <svg xmlns="http://www.w3.org/2000/svg" class="h-6 w-6 mr-2" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M16 4v12l-4-2-4 2V4M6 20h12a2 2 0 002-2V6a2 2 0 00-2-2H6a2 2 0 00-2 2v12a2 2 0 002 2z" /></svg>
                    Storyteller's Analogy
                </p>
                <p class="mt-2 text-gray-300">Forget reading sequentially. The librarian can now look at an entire page simultaneously. For the word "it" in "The cat chased the mouse because it was fast," they can instantly calculate that "it" is highly related to "cat" and "mouse", and decide which one is more important to understand the meaning. This is a superpower.</p>
            </div>
        </div>
    </div>
    
    <!-- Slide 7: The Transformer Impact -->
    <div id="slide-7" class="slide">
        <div class="slide-content">
            <h1 class="text-4xl md:text-5xl font-bold mt-2">The Transformer Effect</h1>
            <p class="mt-4 text-xl text-gray-300">This new architecture unlocked unprecedented performance and scale.</p>
            <div class="mt-6 grid grid-cols-1 md:grid-cols-2 gap-6 text-center">
                <div class="bg-gray-800 p-6 rounded-lg">
                    <h3 class="text-2xl font-semibold text-blue-400">Parallelization</h3>
                    <p class="mt-2 text-gray-300">Processing all words at once, rather than sequentially, allowed for training on massive datasets and much larger models.</p>
                </div>
                <div class="bg-gray-800 p-6 rounded-lg">
                    <h3 class="text-2xl font-semibold text-green-400">Contextual Understanding</h3>
                    <p class="mt-2 text-gray-300">Self-attention provided a far superior way to handle long-range dependencies and disambiguate word meanings.</p>
                </div>
            </div>
            <p class="mt-8 text-2xl text-center text-gray-300">This led to the "Cambrian Explosion" of models...</p>
        </div>
    </div>

    <!-- Slide 8: BERT, GPT, etc. -->
    <div id="slide-8" class="slide">
        <div class="slide-content">
            <h1 class="text-4xl md:text-5xl font-bold">The Children of the Transformer</h1>
            <p class="mt-4 text-xl text-gray-300">Different philosophies emerged, leading to specialized model families.</p>
            <div class="mt-6 grid grid-cols-1 md:grid-cols-2 gap-6">
                <div class="bg-gray-800 p-6 rounded-lg border-l-4 border-blue-500">
                    <h3 class="text-2xl font-bold">BERT: The Encoder</h3>
                    <p class="mt-2 text-gray-400">Bidirectional Encoder Representations from Transformers</p>
                    <p class="mt-3 text-gray-300"><strong>Analogy:</strong> The Great Analyst. Reads the entire text at once to build a deep understanding. Perfect for tasks like sentiment analysis, classification, and question answering.</p>
                </div>
                <div class="bg-gray-800 p-6 rounded-lg border-l-4 border-green-500">
                    <h3 class="text-2xl font-bold">GPT: The Decoder</h3>
                    <p class="mt-2 text-gray-400">Generative Pre-trained Transformer</p>
                    <p class="mt-3 text-gray-300"><strong>Analogy:</strong> The Creative Author. Reads left-to-right to predict the next word. Perfect for text generation, summarization, and conversation.</p>
                </div>
            </div>
            <p class="text-center mt-6 text-gray-400">And many others, like T5 (Encoder-Decoder), which frames every NLP task as text-to-text.</p>
        </div>
    </div>

    <!-- Slide 9: The World Today -->
    <div id="slide-9" class="slide">
        <div class="slide-content">
            <h2 class="text-sm font-semibold uppercase tracking-widest text-blue-400">Today: The Age of Giants</h2>
            <h1 class="text-4xl md:text-5xl font-bold mt-2">Large Language Models (LLMs)</h1>
            <p class="mt-4 text-xl text-gray-300">Scaling up Transformers led to unexpected "emergent abilities."</p>
            <ul class="mt-6 list-disc list-inside space-y-2 text-lg text-gray-400">
                <li><strong class="text-blue-300">In-Context Learning:</strong> Models can perform new tasks from just a few examples (few-shot) or even just instructions (zero-shot), without retraining.</li>
                <li><strong class="text-green-300">Chain-of-Thought:</strong> Models can "reason" step-by-step to solve complex problems.</li>
                <li><strong class="text-yellow-300">Multimodality:</strong> Models like Gemini and GPT-4V can understand and process text, images, audio, and video seamlessly.</li>
            </ul>
             <div class="analogy">
                 <p class="analogy-title">
                    <svg xmlns="http://www.w3.org/2000/svg" class="h-6 w-6 mr-2" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M16 4v12l-4-2-4 2V4M6 20h12a2 2 0 002-2V6a2 2 0 00-2-2H6a2 2 0 00-2 2v12a2 2 0 002 2z" /></svg>
                    Storyteller's Analogy
                </p>
                <p class="mt-2 text-gray-300">The library itself has become sentient. It doesn't just store and retrieve information; it understands, creates, reasons, and can even describe the pictures within its books. The librarian is now a universal scholar.</p>
            </div>
        </div>
    </div>

    <!-- Slide 10: The Future -->
    <div id="slide-10" class="slide">
        <div class="slide-content">
            <h2 class="text-sm font-semibold uppercase tracking-widest text-blue-400">The Horizon</h2>
            <h1 class="text-4xl md:text-5xl font-bold mt-2">What's Next? The Frontiers of AI</h1>
            <div class="mt-8 grid grid-cols-1 md:grid-cols-3 gap-6">
                <div class="bg-gray-800 p-4 rounded-lg">
                    <h3 class="text-xl font-semibold text-blue-400">Agents & Tool Use</h3>
                    <p class="mt-2 text-gray-300">LLMs that can browse the web, run code, and interact with APIs to take action in the world.</p>
                </div>
                <div class="bg-gray-800 p-4 rounded-lg">
                    <h3 class="text-xl font-semibold text-green-400">Hyper-Personalization</h3>
                    <p class="mt-2 text-gray-300">Models that adapt to your personal context, data, and style. On-device models will make this fast and private.</p>
                </div>
                <div class="bg-gray-800 p-4 rounded-lg">
                    <h3 class="text-xl font-semibold text-yellow-400">Richer Modalities</h3>
                    <p class="mt-2 text-gray-300">Deeper integration of various data types, moving toward models that experience and understand a more human-like sensory input.</p>
                </div>
            </div>
            <p class="mt-8 text-center text-xl text-gray-400">The journey from simple pattern matching to sophisticated reasoning is accelerating, transforming every industry in its path.</p>
        </div>
    </div>

    <!-- Slide 11: Conclusion -->
    <div id="slide-11" class="slide text-center">
        <div class="slide-content">
            <h1 class="text-5xl md:text-7xl font-bold text-transparent bg-clip-text bg-gradient-to-r from-blue-400 to-indigo-600">Thank You</h1>
            <p class="mt-4 text-2xl md:text-3xl text-gray-300">Q & A</p>
        </div>
    </div>


    <!-- Navigation -->
    <div class="progress-bar" id="progress-bar"></div>
    <div class="slide-counter" id="slide-counter">1 / 11</div>
    <button id="prev-btn" class="nav-button">
        <svg xmlns="http://www.w3.org/2000/svg" class="h-6 w-6" fill="none" viewBox="0 0 24 24" stroke="currentColor">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M15 19l-7-7 7-7" />
        </svg>
    </button>
    <button id="next-btn" class="nav-button">
        <svg xmlns="http://www.w3.org/2000/svg" class="h-6 w-6" fill="none" viewBox="0 0 24 24" stroke="currentColor">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" />
        </svg>
    </button>

    <script>
        const slides = document.querySelectorAll('.slide');
        const prevBtn = document.getElementById('prev-btn');
        const nextBtn = document.getElementById('next-btn');
        const progressBar = document.getElementById('progress-bar');
        const slideCounter = document.getElementById('slide-counter');

        let currentSlide = 0;
        const totalSlides = slides.length;

        function updateSlides() {
            slides.forEach((slide, index) => {
                if (index === currentSlide) {
                    slide.classList.add('active');
                } else {
                    slide.classList.remove('active');
                }
            });

            // Update progress bar
            const progress = ((currentSlide + 1) / totalSlides) * 100;
            progressBar.style.width = `${progress}%`;

            // Update slide counter
            slideCounter.textContent = `${currentSlide + 1} / ${totalSlides}`;
            
            // Hide/show nav buttons
            prevBtn.style.display = currentSlide === 0 ? 'none' : 'block';
            nextBtn.style.display = currentSlide === totalSlides - 1 ? 'none' : 'block';
        }

        function nextSlide() {
            if (currentSlide < totalSlides - 1) {
                currentSlide++;
                updateSlides();
            }
        }

        function prevSlide() {
            if (currentSlide > 0) {
                currentSlide--;
                updateSlides();
            }
        }

        // Event Listeners
        nextBtn.addEventListener('click', nextSlide);
        prevBtn.addEventListener('click', prevSlide);

        document.addEventListener('keydown', (e) => {
            if (e.key === 'ArrowRight' || e.key === ' ') {
                nextSlide();
            } else if (e.key === 'ArrowLeft') {
                prevSlide();
            }
        });

        // Initial setup
        updateSlides();
    </script>

</body>
</html>
