{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://upload.wikimedia.org/wikipedia/commons/e/e9/4_RGB_McCombs_School_Brand_Branded.png\" width=\"300\" height=\"100\"/>\n",
    "  <img src=\"https://mma.prnewswire.com/media/1458111/Great_Learning_Logo.jpg?p=facebook\" width=\"200\" height=\"100\"/></center>\n",
    "\n",
    "<center><font size=10>Artificial Intelligence and Machine Learning</center></font>\n",
    "<center><font size=6>Natural Language Processing with Generative AI - Retrieval Augmented Generation</center></font>\n",
    "\n",
    "<center><img src=\"https://i.ibb.co/pBF9nKpf/apple.png\" width=\"720\"></center>\n",
    "\n",
    "<center><font size=6>Apple HBR Report Document Q&A</center></font>\n",
    "\n",
    "# RAG LLM Application Notebook\n",
    "\n",
    "This notebook demonstrates the complete workflow of a Retrieval-Augmented Generation (RAG) LLM application. It covers data loading, chunking, embedding, vector database setup, question answering, and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Library Installation\n",
    "\n",
    "First, we need to install all the necessary Python libraries. This ensures that all dependencies for data processing, LLM interaction, and vector database operations are met. If you are running this in a Colab environment, these commands will typically install the packages.\n",
    "\n",
    "**Note**: If you are running this locally, ensure you have Ollama installed and the `llama3.2` model pulled (`ollama pull llama3.2`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All specified packages installed (or upgraded) successfully!\n",
      "Please ensure the model is pulled for Ollama with the command:\n"
     ]
    }
   ],
   "source": [
    "# Install necessary packages\n",
    "# %pip install -r requirements.txt\n",
    "\n",
    "\n",
    "print(\"All specified packages installed (or upgraded) successfully!\")\n",
    "print(\"Please ensure the model is pulled for Ollama with the command:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Necessary Modules\n",
    "\n",
    "We import the `RAG_LLM` class from `functions.py` and constants from `config.py`. Make sure `functions.py`, `config.py`, and `prompt_templates.py` are in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules imported successfully.\n"
     ]
    }
   ],
   "source": [
    "from functions import RAG_LLM\n",
    "from config import APPLE_PDF_PATH, DEFAULT_K_RETRIEVER, DEFAULT_MAX_TOKENS, DEFAULT_TEMPERATURE, DEFAULT_OLLAMA_MODEL, DEFAULT_GEMINI_MODEL\n",
    "import os\n",
    "\n",
    "print(\"Modules imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize the RAG_LLM System\n",
    "\n",
    "Here, we create an instance of our `RAG_LLM` class. This object will manage the entire RAG pipeline, including data loading, processing, retrieval, and LLM interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama client initialized.\n",
      "Google Generative AI SDK configured successfully.\n",
      "Default model for this instance is set to: 'qwen3:4b-instruct'\n",
      "RAG_LLM initialized.\n",
      "RAG_LLM system initialized.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the RAG_LLM class\n",
    "rag_system = RAG_LLM()\n",
    "print(\"RAG_LLM system initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a27601b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default model for this instance has been changed to: 'qwen3:4b-instruct'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rag_system.set_model(DEFAULT_OLLAMA_MODEL)\n",
    "# rag_system.set_model(DEFAULT_GEMINI_MODEL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load the PDF Document\n",
    "\n",
    "We load the `HBR_How_Apple_Is_Organized_For_Innovation.pdf` document. Ensure this PDF file is available in the working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: HBR_How_Apple_Is_Organized_For_Innovation.pdf\n",
      "Successfully loaded 11 pages.\n",
      "PDF document loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load the PDF document\n",
    "documents = rag_system.load_data(pdf_path=APPLE_PDF_PATH)\n",
    "if not documents:\n",
    "    print(\"Failed to load documents. Please check the PDF path and file existence.\")\n",
    "else:\n",
    "    print(\"PDF document loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Chunk the Loaded Data\n",
    "\n",
    "The loaded document is chunked into smaller, overlapping segments. This is crucial for efficient retrieval and to fit content within the LLM's context window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking data with chunk_size=1024, chunk_overlap=20\n",
      "Created 16 chunks.\n",
      "Documents chunked successfully.\n"
     ]
    }
   ],
   "source": [
    "# Chunk the loaded data\n",
    "document_chunks = rag_system.chunk_data(documents)\n",
    "if not document_chunks:\n",
    "    print(\"Failed to chunk documents.\")\n",
    "else:\n",
    "    print(\"Documents chunked successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Embedding Model\n",
    "\n",
    "An embedding model (SentenceTransformer) is initialized to convert text chunks into numerical vectors, enabling semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing embedding model: mixedbread-ai/mxbai-embed-large-v1\n",
      "Embedding model initialized successfully.\n",
      "Embedding model created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create embedding model\n",
    "rag_system.create_embeddings()\n",
    "if not rag_system.embedding_model:\n",
    "    print(\"Failed to create embedding model.\")\n",
    "else:\n",
    "    print(\"Embedding model created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Set Up the Vector Database\n",
    "\n",
    "The Chroma vector database is set up using the chunked documents and the embedding model. This database will store the embeddings and facilitate quick retrieval of relevant context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up vector database in: vector_db_1024\n",
      "Vector database loaded from existing directory.\n",
      "Retriever initialized.\n",
      "Vector database set up and retriever initialized.\n"
     ]
    }
   ],
   "source": [
    "# Set up the vector database\n",
    "rag_system.setup_vector_database(document_chunks=document_chunks)\n",
    "if not rag_system.vectorstore:\n",
    "    print(\"Failed to set up vector database.\")\n",
    "else:\n",
    "    print(\"Vector database set up and retriever initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Demonstrate Question Answering with RAG\n",
    "\n",
    "Now, we can ask questions and see how the RAG system retrieves relevant information and generates answers based on the loaded document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query 1: Who are the authors of this article and who published this article ?\n",
      "Retrieving 3 relevant documents for the query.\n",
      "RAG prompt created.\n",
      "Generating LLM response using model: qwen3:4b-instruct\n",
      "Ollama response generated.\n",
      "Response 1: \n",
      "The authors of the article are Joel M. Podolny and Morten T. Hansen. The article was published by Harvard Business Review.\n"
     ]
    }
   ],
   "source": [
    "# Example Query 1\n",
    "user_input_1 = \"Who are the authors of this article and who published this article ?\"\n",
    "print(f\"\\nQuery 1: {user_input_1}\")\n",
    "llm_response_1 = rag_system.get_answer(user_input_1)\n",
    "print(f\"Response 1: \\n{llm_response_1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37933c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query 1: Who are the authors of this article and who published this article ?\n",
      "Retrieving 3 relevant documents for the query.\n",
      "RAG prompt created.\n",
      "Generating LLM response using model: gemini-1.5-flash-latest\n",
      "Gemini response generated.\n",
      "Response 1: \n",
      "Joel M. Podolny and Morten T. Hansen are the authors.  Harvard Business Review published the article.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example Query 1\n",
    "user_input_1 = \"Who are the authors of this article and who published this article ?\"\n",
    "print(f\"\\nQuery 1: {user_input_1}\")\n",
    "llm_response_1 = rag_system.get_answer(user_input_1)\n",
    "print(f\"Response 1: \\n{llm_response_1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query 2: List down the three leadership characteristics in bulleted points and explain each one of the characteristics under two lines.\n",
      "Retrieving 3 relevant documents for the query.\n",
      "RAG prompt created.\n",
      "Generating LLM response using model: gemini-1.5-flash-latest\n",
      "Gemini response generated.\n",
      "Response 2: \n",
      "* Deep expertise: Apple leaders possess extensive knowledge in their respective fields.  This allows for informed decision-making and effective guidance.\n",
      "* Immersion in details: Leaders are deeply involved in the specifics of their functions. This ensures a thorough understanding of ongoing projects and potential challenges.\n",
      "* Collaborative debate:  Leaders actively engage in discussions with colleagues across various teams. This fosters innovation and efficient problem-solving.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example Query 2\n",
    "user_input_2 = \"List down the three leadership characteristics in bulleted points and explain each one of the characteristics under two lines.\"\n",
    "print(f\"\\nQuery 2: {user_input_2}\")\n",
    "# Adjust max_tokens to allow for a more complete answer for a list\n",
    "llm_response_2 = rag_system.get_answer(user_input_2, max_tokens=150, temperature=0.1)\n",
    "print(f\"Response 2: \\n{llm_response_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query 3: Can you explain specific examples from the article where Apple's approach to leadership has led to successful innovations?\n",
      "Retrieving 3 relevant documents for the query.\n",
      "RAG prompt created.\n",
      "Generating LLM response using model: gemini-1.5-flash-latest\n",
      "Gemini response generated.\n",
      "Response 3: \n",
      "I don't know\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example Query 3 (expected to be \"I don't know\" if not in context)\n",
    "user_input_3 = \"Can you explain specific examples from the article where Apple's approach to leadership has led to successful innovations?\"\n",
    "print(f\"\\nQuery 3: {user_input_3}\")\n",
    "llm_response_3 = rag_system.get_answer(user_input_3)\n",
    "print(f\"Response 3: \\n{llm_response_3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Demonstrate Output Evaluation (LLM-as-a-Judge)\n",
    "\n",
    "Finally, we demonstrate how the RAG system can evaluate its own answers for 'groundedness' (adherence to context) and 'relevance' (how well it answers the question)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Query 1:\n",
      "\n",
      "--- Calculating Ratings for Question: 'Who are the authors of this article and who published this article ?' ---\n",
      "Retrieving 3 relevant documents for the query.\n",
      "RAG prompt created.\n",
      "Generating LLM response using model: gemini-1.5-flash-latest\n",
      "Gemini response generated.\n",
      "Rating overall answer quality (groundedness and relevance)...\n",
      "Rating groundedness...\n",
      "Retrieving 3 relevant documents for the query.\n",
      "Generating LLM response using model: gemini-1.5-flash-latest\n",
      "Gemini response generated.\n",
      "Rating relevance...\n",
      "Retrieving 3 relevant documents for the query.\n",
      "Generating LLM response using model: gemini-1.5-flash-latest\n",
      "Gemini response generated.\n",
      "\n",
      "--- Results ---\n",
      "Question: \n",
      " Who are the authors of this article and who published this article ?\n",
      "\n",
      "Answer: \n",
      " Joel M. Podolny and Morten T. Hansen are the authors.  Harvard Business Review published the article.\n",
      "\n",
      "\n",
      "Groundedness Rating: \n",
      " Steps to evaluate the answer:\n",
      "\n",
      "1. **Identify the authors:** Check if the answer correctly identifies the authors mentioned in the context.\n",
      "2. **Identify the publisher:** Check if the answer correctly identifies the publisher mentioned in the context.\n",
      "3. **Verify information source:** Ensure that the identified authors and publisher are explicitly mentioned within the provided context.\n",
      "\n",
      "\n",
      "Step-by-step explanation:\n",
      "\n",
      "1. The context clearly states \"AUTHORS Joel M. Podolny Dean, Apple University Morten T. Hansen Faculty, Apple University\".  The answer correctly identifies these two individuals as the authors.\n",
      "2. The context mentions \"Harvard Business Review Novemberâ€“December 2020\". The answer correctly identifies Harvard Business Review as the publisher.\n",
      "3. Both the authors and the publisher are explicitly named in the provided text.\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "The answer completely adheres to the metric.  All information is directly extracted from the provided context.\n",
      "\n",
      "\n",
      "Score: 5\n",
      "\n",
      "\n",
      "Relevance Rating: \n",
      " Steps to evaluate the context based on the relevance metric:\n",
      "\n",
      "1. **Identify the main aspects of the question:** The question asks for the authors and the publisher of the article.\n",
      "\n",
      "2. **Locate the relevant information in the context:** The context contains the names of the authors (\"Joel M. Podolny\" and \"Morten T. Hansen\") and the publisher (\"Harvard Business Review\") in different sections.\n",
      "\n",
      "3. **Check for completeness:** The context provides all the necessary information to answer both parts of the question.\n",
      "\n",
      "4. **Check for extraneous information:** The context contains additional information about the article's content and Apple's organizational structure, but this is not relevant to the question.\n",
      "\n",
      "\n",
      "Step-by-step explanation of whether the context adheres to the metric:\n",
      "\n",
      "1. The question explicitly asks for the authors and publisher.\n",
      "2. The context clearly states \"AUTHORS\" followed by the names Joel M. Podolny and Morten T. Hansen.\n",
      "3\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Query 1\n",
    "user_input_1 = \"Who are the authors of this article and who published this article ?\"\n",
    "print(\"\\nEvaluating Query 1:\")\n",
    "rag_system.calculate_rating(question=user_input_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Query 2:\n",
      "\n",
      "--- Calculating Ratings for Question: 'List down the three leadership characteristics in bulleted points and explain each one of the characteristics under two lines.' ---\n",
      "Retrieving 3 relevant documents for the query.\n",
      "RAG prompt created.\n",
      "Generating LLM response using model: gemini-1.5-flash-latest\n",
      "Gemini response generated.\n",
      "Rating overall answer quality (groundedness and relevance)...\n",
      "Rating groundedness...\n",
      "Retrieving 3 relevant documents for the query.\n",
      "Generating LLM response using model: gemini-1.5-flash-latest\n",
      "Gemini response generated.\n",
      "Rating relevance...\n",
      "Retrieving 3 relevant documents for the query.\n",
      "Generating LLM response using model: gemini-1.5-flash-latest\n",
      "Gemini response generated.\n",
      "\n",
      "--- Results ---\n",
      "Question: \n",
      " List down the three leadership characteristics in bulleted points and explain each one of the characteristics under two lines.\n",
      "\n",
      "Answer: \n",
      " * Deep expertise: Apple leaders possess extensive knowledge in their respective fields.  This allows for informed decision-making and effective guidance.\n",
      "* Immersion in details: Leaders are deeply involved in the specifics of their functions. This ensures a thorough understanding of ongoing projects and potential challenges.\n",
      "* Collaborative debate:  Leaders actively engage in discussions and collaborations with various teams. This fosters innovation and efficient problem-solving.\n",
      "\n",
      "\n",
      "Groundedness Rating: \n",
      " Steps to evaluate the answer:\n",
      "\n",
      "1. **Identify the key information:**  The question asks for three leadership characteristics and a brief explanation of each, based solely on the provided context.\n",
      "\n",
      "2. **Locate the relevant sections in the context:** The context mentions several aspects of Apple's leadership model, including \"experts leading experts,\" \"immersion in the details,\" and \"collaborative debate.\"\n",
      "\n",
      "3. **Check if the answer directly reflects the context:** Examine if the answer accurately paraphrases or directly quotes the identified information from the context.  Avoid any inferences or information not explicitly stated.\n",
      "\n",
      "Step-by-step explanation:\n",
      "\n",
      "1. The answer correctly identifies three leadership characteristics: \"Deep expertise,\" \"Immersion in details,\" and \"Collaborative debate.\"\n",
      "\n",
      "2.  The explanations for each characteristic are concise and accurately reflect the ideas presented in the context.  For example, \"Deep expertise\" aligns with the context's description of leaders as \"experts leading experts.\"  \"\n",
      "\n",
      "Relevance Rating: \n",
      " Steps to evaluate the context based on the relevance metric:\n",
      "\n",
      "1. **Identify the main aspects of the question:** The question asks for three leadership characteristics in bullet points, with a brief explanation of each (under two lines).\n",
      "\n",
      "2. **Check if the context addresses all the main aspects:** The provided context discusses Apple's leadership model, focusing on how leaders balance expertise, attention to detail, and collaboration in a large and complex organization.  It uses the example of Roger Rosner to illustrate these points.  The context does not explicitly list three leadership characteristics in bullet points, but the information is present to extract them.\n",
      "\n",
      "3. **Check if the context contains only the important aspects:** The context contains additional information about Apple's organizational structure and growth, which is relevant to understanding the context of the leadership model but not directly necessary to answer the question.\n",
      "\n",
      "4. **Assess the sufficiency of the information:** The context provides sufficient information to extract the three key leadership characteristics and provide brief\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Query 2\n",
    "user_input_2 = \"List down the three leadership characteristics in bulleted points and explain each one of the characteristics under two lines.\"\n",
    "print(\"\\nEvaluating Query 2:\")\n",
    "# Using parameters that yield a better answer for evaluation\n",
    "rag_system.calculate_rating(question=user_input_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Query 3:\n",
      "\n",
      "--- Calculating Ratings for Question: 'Can you explain specific examples from the article where Apple's approach to leadership has led to successful innovations?' ---\n",
      "Retrieving 3 relevant documents for the query.\n",
      "RAG prompt created.\n",
      "Generating LLM response using model: gemini-1.5-flash-latest\n",
      "Gemini response generated.\n",
      "Rating overall answer quality (groundedness and relevance)...\n",
      "Rating groundedness...\n",
      "Retrieving 3 relevant documents for the query.\n",
      "Generating LLM response using model: gemini-1.5-flash-latest\n",
      "Gemini response generated.\n",
      "Rating relevance...\n",
      "Retrieving 3 relevant documents for the query.\n",
      "Generating LLM response using model: gemini-1.5-flash-latest\n",
      "Gemini response generated.\n",
      "\n",
      "--- Results ---\n",
      "Question: \n",
      " Can you explain specific examples from the article where Apple's approach to leadership has led to successful innovations?\n",
      "\n",
      "Answer: \n",
      " I don't know\n",
      "\n",
      "\n",
      "Groundedness Rating: \n",
      " Steps to evaluate the answer based on the metric:\n",
      "\n",
      "1. **Identify the question**: Determine what the user is asking.\n",
      "2. **Analyze the context**: Identify if the context provides information relevant to the question.\n",
      "3. **Compare the answer to the context**: Check if the answer is derived solely from the information present in the context.\n",
      "4. **Assess adherence to the metric**: Evaluate the extent to which the answer relies only on the provided context.\n",
      "\n",
      "\n",
      "Step-by-step explanation:\n",
      "\n",
      "1. The question asks for specific examples from the article illustrating how Apple's leadership approach has driven successful innovations.\n",
      "\n",
      "2. The context discusses Apple's organizational structure and leadership model, mentioning its shift to a functional structure under Steve Jobs and its evolution under Tim Cook.  It highlights the challenges of scaling this model and the adoption of a \"discretionary leadership model.\" However, the article does *not* provide specific examples of innovations directly linked to these leadership decisions.  \n",
      "\n",
      "Relevance Rating: \n",
      " Steps to evaluate the context based on the relevance metric:\n",
      "\n",
      "1. **Identify the main aspects of the question:** The question asks for specific examples from the article illustrating how Apple's leadership approach has driven successful innovations.  This requires identifying instances where leadership decisions directly contributed to the creation or success of specific Apple products or technologies.\n",
      "\n",
      "2. **Analyze the context for relevant information:** The provided text extensively discusses Apple's organizational structure and leadership model under Steve Jobs and Tim Cook. It details the shift to a functional structure, the challenges of scaling this model, and the evolution of leadership approaches to address growth. However, the context lacks specific examples of innovations directly linked to these leadership decisions.  The article mentions the iPhone's launch in relation to the number of VPs, but doesn't connect leadership decisions to its success.  There's discussion of new applications (News, Clips, etc.), but no explanation of how leadership choices influenced their development or market success.\n",
      "\n",
      "3.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Query 3\n",
    "user_input_3 = \"Can you explain specific examples from the article where Apple's approach to leadership has led to successful innovations?\"\n",
    "print(\"\\nEvaluating Query 3:\")\n",
    "rag_system.calculate_rating(question=user_input_3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
