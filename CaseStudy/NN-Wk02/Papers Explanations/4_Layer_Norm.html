<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Layer Normalization Presentation</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
    <script>
        MathJax = {
          tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']],
            processEscapes: true
          },
          svg: {
            fontCache: 'global'
          }
        };
    </script>
    <script type="text/javascript" id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
    </script>
    <style>
        body {
            font-family: 'Inter', sans-serif;
            background-color: #f3f4f6;
        }
        .slide {
            display: none;
        }
        .slide.active {
            display: block;
        }
        /* Style for disabled buttons */
        .disabled {
            opacity: 0.5;
            cursor: not-allowed;
        }
    </style>
</head>
<body class="flex flex-col items-center justify-center min-h-screen bg-gray-100 p-4">

    <div id="presentation-container" class="w-full max-w-4xl bg-white rounded-xl shadow-2xl overflow-hidden">
        <!-- Slide content will be injected here -->
        <div id="slide-content" class="p-8 md:p-12 min-h-[400px] md:min-h-[500px] flex flex-col justify-center">

            <!-- Slide 1: Title -->
            <div class="slide active">
                <div class="text-center">
                    <h1 class="text-4xl md:text-5xl font-bold text-gray-800 mb-4">Layer Normalization</h1>
                    <p class="text-xl md:text-2xl text-gray-600">Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton</p>
                    <p class="text-lg text-gray-500 mt-2">University of Toronto, Twitter Inc.</p>
                    <p class="text-sm text-gray-400 mt-8">arXiv:1607.06450</p>
                </div>
            </div>

            <!-- Slide 2: The Problem with Deep Networks -->
            <div class="slide">
                <h2 class="text-3xl font-bold text-gray-800 mb-6">The Challenge: Training Deep Networks</h2>
                <p class="text-lg text-gray-700 mb-4">One major obstacle in training deep neural networks is the phenomenon known as <strong class="text-blue-600">internal covariate shift</strong>.</p>
                <ul class="list-disc list-inside space-y-3 text-lg text-gray-700">
                    <li>The distribution of each layer's inputs changes during training as the parameters of the previous layers change.</li>
                    <li>This forces layers to continuously adapt to a new distribution, slowing down the training process.</li>
                    <li>It requires careful initialization of parameters and lower learning rates.</li>
                </ul>
            </div>

            <!-- Slide 3: Batch Normalization -->
            <div class="slide">
                <h2 class="text-3xl font-bold text-gray-800 mb-6">A Solution: Batch Normalization</h2>
                <p class="text-lg text-gray-700 mb-4">Batch Normalization (BatchNorm) addresses internal covariate shift by normalizing the inputs to a layer <strong class="text-green-600">across the mini-batch</strong>.</p>
                <div class="bg-gray-50 p-4 rounded-lg">
                    <p class="text-lg text-gray-700">For each feature/channel, BatchNorm computes the mean and variance over all examples in the mini-batch.</p>
                </div>
                <h3 class="text-2xl font-semibold text-gray-800 mt-6 mb-3">Limitations of Batch Normalization:</h3>
                <ul class="list-disc list-inside space-y-3 text-lg text-gray-700">
                    <li><strong class="text-red-500">Dependency on Mini-Batch Size:</strong> Performance degrades significantly with small mini-batches.</li>
                    <li><strong class="text-red-500">Not Ideal for RNNs:</strong> It's difficult to apply to sequence models like RNNs where the sequence length can vary.</li>
                    <li>Introduces a complex dependency between training examples in a mini-batch.</li>
                </ul>
            </div>

            <!-- Slide 4: Layer Normalization - The Core Idea -->
            <div class="slide">
                 <h2 class="text-3xl font-bold text-gray-800 mb-6">The Proposal: Layer Normalization (LayerNorm)</h2>
                 <p class="text-lg text-gray-700 mb-4">Layer Normalization is a simple yet effective alternative that avoids the drawbacks of BatchNorm.</p>
                 <div class="bg-blue-50 p-6 rounded-lg shadow-md">
                    <p class="text-xl text-center font-semibold text-blue-800">Instead of normalizing across the batch, LayerNorm normalizes the inputs <strong class="text-blue-600">within a single training example</strong> across all of its features.</p>
                </div>
                <p class="text-lg text-gray-700 mt-6">This means the normalization is independent of the other examples in the mini-batch.</p>
            </div>

            <!-- Slide 5: The Math Behind LayerNorm -->
            <div class="slide">
                <h2 class="text-3xl font-bold text-gray-800 mb-4">Layer Normalization: The Math</h2>
                <p class="text-lg text-gray-700 mb-4">For a given layer, let $a$ be the vector of summed inputs from the previous layer. LayerNorm computes the mean ($\mu$) and standard deviation ($\sigma$) over all hidden units in that layer for the current training example.</p>
                <div class="space-y-4 text-lg">
                    <div class="bg-gray-50 p-4 rounded-lg">
                        <p class="font-semibold mb-2">Mean:</p>
                        <p>$\mu = \frac{1}{H} \sum_{i=1}^{H} a_i$</p>
                    </div>
                    <div class="bg-gray-50 p-4 rounded-lg">
                        <p class="font-semibold mb-2">Standard Deviation:</p>
                        <p>$\sigma = \sqrt{\frac{1}{H} \sum_{i=1}^{H} (a_i - \mu)^2}$</p>
                    </div>
                </div>
                <p class="text-sm text-gray-500 mt-4">* Where $H$ is the number of hidden units in the layer.</p>
            </div>

            <!-- Slide 6: Normalization and Adaptive Parameters -->
            <div class="slide">
                <h2 class="text-3xl font-bold text-gray-800 mb-4">Normalization & Adaptive Scaling</h2>
                <p class="text-lg text-gray-700 mb-4">The normalized activities $h$ are then computed as:</p>
                <div class="bg-gray-50 p-4 rounded-lg text-center text-xl mb-6">
                    <p>$h = f\left( g \odot \frac{a - \mu}{\sigma} + b \right)$</p>
                </div>
                <ul class="list-disc list-inside space-y-3 text-lg text-gray-700">
                    <li>$g$ (the <strong class="text-green-600">gain</strong>) and $b$ (the <strong class="text-blue-600">bias</strong>) are learnable parameter vectors of the same dimension as $h$.</li>
                    <li>They allow the network to scale and shift the normalized activations, preserving its expressive power.</li>
                    <li>The gain $g$ is typically initialized to 1 and the bias $b$ to 0.</li>
                    <li>$f(\cdot)$ is the non-linear activation function (e.g., ReLU, Tanh).</li>
                </ul>
            </div>

            <!-- Slide 7: Batch Norm vs. Layer Norm -->
            <div class="slide">
                <h2 class="text-3xl font-bold text-gray-800 mb-6">Batch Norm vs. Layer Norm</h2>
                <div class="flex flex-col md:flex-row gap-6">
                    <!-- Batch Norm -->
                    <div class="flex-1 border-2 border-green-200 bg-green-50 p-4 rounded-lg">
                        <h3 class="text-2xl font-semibold text-green-800 mb-3">Batch Normalization</h3>
                        <p class="text-lg text-gray-700 mb-4">Normalizes over the batch for each feature.</p>
                        <img src="https://i.imgur.com/g8c0g5E.png" alt="Batch Normalization Diagram" class="rounded-md shadow-lg mx-auto" onerror="this.onerror=null;this.src='https://placehold.co/300x200/a7f3d0/166534?text=BatchNorm+Diagram';">
                        <p class="text-center text-sm mt-2 text-gray-600">Normalization depends on the mini-batch.</p>
                    </div>
                    <!-- Layer Norm -->
                    <div class="flex-1 border-2 border-blue-200 bg-blue-50 p-4 rounded-lg">
                        <h3 class="text-2xl font-semibold text-blue-800 mb-3">Layer Normalization</h3>
                        <p class="text-lg text-gray-700 mb-4">Normalizes over the features for each example.</p>
                        <img src="https://i.imgur.com/uYp2gYJ.png" alt="Layer Normalization Diagram" class="rounded-md shadow-lg mx-auto" onerror="this.onerror=null;this.src='https://placehold.co/300x200/dbeafe/1e3a8a?text=LayerNorm+Diagram';">
                        <p class="text-center text-sm mt-2 text-gray-600">Normalization is independent of the mini-batch.</p>
                    </div>
                </div>
            </div>

            <!-- Slide 8: Advantages -->
            <div class="slide">
                <h2 class="text-3xl font-bold text-gray-800 mb-6">Key Advantages of Layer Normalization</h2>
                <ul class="space-y-4">
                    <li class="flex items-start">
                        <svg class="w-6 h-6 text-green-500 mr-3 mt-1 flex-shrink-0" fill="none" stroke="currentColor" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12l2 2 4-4m6 2a9 9 0 11-18 0 9 9 0 0118 0z"></path></svg>
                        <p class="text-lg text-gray-700"><strong class="font-semibold">No Mini-Batch Dependence:</strong> The computation for one example is independent of all others. Works well with any batch size (even 1).</p>
                    </li>
                    <li class="flex items-start">
                        <svg class="w-6 h-6 text-green-500 mr-3 mt-1 flex-shrink-0" fill="none" stroke="currentColor" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12l2 2 4-4m6 2a9 9 0 11-18 0 9 9 0 0118 0z"></path></svg>
                        <p class="text-lg text-gray-700"><strong class="font-semibold">Ideal for RNNs:</strong> Can be applied naturally to recurrent networks by computing the normalization stats at each time step.</p>
                    </li>
                    <li class="flex items-start">
                        <svg class="w-6 h-6 text-green-500 mr-3 mt-1 flex-shrink-0" fill="none" stroke="currentColor" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12l2 2 4-4m6 2a9 9 0 11-18 0 9 9 0 0118 0z"></path></svg>
                        <p class="text-lg text-gray-700"><strong class="font-semibold">Deterministic:</strong> The output is deterministic during both training and inference for a given input.</p>
                    </li>
                     <li class="flex items-start">
                        <svg class="w-6 h-6 text-green-500 mr-3 mt-1 flex-shrink-0" fill="none" stroke="currentColor" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12l2 2 4-4m6 2a9 9 0 11-18 0 9 9 0 0118 0z"></path></svg>
                        <p class="text-lg text-gray-700"><strong class="font-semibold">Faster Training:</strong> The paper demonstrates that LayerNorm can significantly speed up training compared to baseline models.</p>
                    </li>
                </ul>
            </div>

            <!-- Slide 9: Experimental Results Summary -->
            <div class="slide">
                <h2 class="text-3xl font-bold text-gray-800 mb-6">Experimental Results</h2>
                <p class="text-lg text-gray-700 mb-4">The paper showed Layer Normalization's effectiveness across various tasks:</p>
                <ul class="list-disc list-inside space-y-3 text-lg text-gray-700">
                    <li><strong>Recurrent Neural Networks:</strong> Substantial improvements in training speed and final performance on tasks like language modeling and machine translation.</li>
                    <li><strong>Convolutional Neural Networks:</strong> Showed competitive performance with BatchNorm on image classification tasks (e.g., CIFAR-10).</li>
                    <li><strong>Order Embeddings:</strong> Achieved state-of-the-art results by stabilizing the training of a model that embeds directed graphs.</li>
                    <li><strong>Robustness:</strong> Demonstrated much more stable training dynamics compared to models without normalization.</li>
                </ul>
            </div>
            
            <!-- Slide 10: Results Graphs -->
            <div class="slide">
                <h2 class="text-3xl font-bold text-gray-800 mb-6">Results: Performance Graphs</h2>
                <div class="flex flex-col md:flex-row gap-6 items-center">
                    <div class="flex-1 text-center">
                        <img src="https://i.imgur.com/8a5F4sN.png" alt="Training curves on RNN language model" class="rounded-lg shadow-xl mx-auto mb-2 border" onerror="this.onerror=null;this.src='https://placehold.co/300x250/e0e7ff/3730a3?text=Training+Curves';">
                        <p class="text-sm text-gray-600">Fig 2: LayerNorm greatly speeds up RNN training compared to the baseline and is more stable than BatchNorm.</p>
                    </div>
                    <div class="flex-1 text-center">
                         <img src="https://i.imgur.com/6g1pY2H.png" alt="Validation cost for order-embeddings" class="rounded-lg shadow-xl mx-auto mb-2 border" onerror="this.onerror=null;this.src='https://placehold.co/300x250/e0e7ff/3730a3?text=Validation+Cost';">
                         <p class="text-sm text-gray-600">Fig 4: Faster convergence and better validation cost on the order-embeddings task.</p>
                    </div>
                </div>
            </div>

            <!-- Slide 11: Conclusion -->
            <div class="slide">
                 <h2 class="text-3xl font-bold text-gray-800 mb-6">Conclusion & Impact</h2>
                <div class="bg-gray-50 p-6 rounded-lg shadow-md">
                    <p class="text-lg text-gray-700 mb-4">Layer Normalization is a simple and effective technique to stabilize and accelerate the training of deep neural networks.</p>
                    <p class="text-lg text-gray-700 font-semibold">Its key contribution is a normalization scheme that is independent of the mini-batch, making it particularly well-suited for <strong class="text-blue-600">Recurrent Neural Networks</strong> and models that operate on variable-length sequences.</p>
                </div>
                <p class="text-lg text-gray-700 mt-6">Since its publication, Layer Normalization has become a fundamental component in many state-of-the-art architectures, most notably the <strong class="text-purple-600">Transformer model</strong>, which powers models like BERT and GPT.</p>
            </div>
            
            <!-- Slide 12: Q&A -->
            <div class="slide">
                <div class="text-center">
                    <h1 class="text-5xl md:text-6xl font-bold text-gray-800 mb-6">Thank You</h1>
                    <p class="text-3xl md:text-4xl text-gray-600">Questions?</p>
                </div>
            </div>

        </div>

        <!-- Navigation -->
        <div class="bg-gray-800 p-4 flex justify-between items-center">
            <button id="prevBtn" class="bg-gray-600 hover:bg-gray-500 text-white font-bold py-2 px-4 rounded-lg transition duration-300">
                Previous
            </button>
            <div id="slide-counter" class="text-white text-lg">
                Slide <span id="current-slide-num">1</span> of <span id="total-slides-num"></span>
            </div>
            <button id="nextBtn" class="bg-blue-600 hover:bg-blue-500 text-white font-bold py-2 px-4 rounded-lg transition duration-300">
                Next
            </button>
        </div>
    </div>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            const slides = document.querySelectorAll('.slide');
            const prevBtn = document.getElementById('prevBtn');
            const nextBtn = document.getElementById('nextBtn');
            const currentSlideNumEl = document.getElementById('current-slide-num');
            const totalSlidesNumEl = document.getElementById('total-slides-num');
            
            let currentSlide = 0;
            const totalSlides = slides.length;

            totalSlidesNumEl.textContent = totalSlides;

            function showSlide(n) {
                // Hide all slides
                slides.forEach(slide => slide.classList.remove('active'));
                
                // Show the target slide
                slides[n].classList.add('active');
                
                // Update slide counter
                currentSlideNumEl.textContent = n + 1;

                // Update button states
                if (currentSlide === 0) {
                    prevBtn.classList.add('disabled');
                } else {
                    prevBtn.classList.remove('disabled');
                }

                if (currentSlide === totalSlides - 1) {
                    nextBtn.classList.add('disabled');
                } else {
                    nextBtn.classList.remove('disabled');
                }
            }

            function nextSlide() {
                if (currentSlide < totalSlides - 1) {
                    currentSlide++;
                    showSlide(currentSlide);
                }
            }

            function prevSlide() {
                if (currentSlide > 0) {
                    currentSlide--;
                    showSlide(currentSlide);
                }
            }

            // Event Listeners
            nextBtn.addEventListener('click', nextSlide);
            prevBtn.addEventListener('click', prevSlide);

            // Keyboard navigation
            document.addEventListener('keydown', function(e) {
                if (e.key === 'ArrowRight') {
                    nextSlide();
                } else if (e.key === 'ArrowLeft') {
                    prevSlide();
                }
            });

            // Initial setup
            showSlide(currentSlide);
        });
    </script>

</body>
</html>
