{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://upload.wikimedia.org/wikipedia/commons/e/e9/4_RGB_McCombs_School_Brand_Branded.png\" width=\"300\" height=\"100\"/>\n",
    "  <img src=\"https://mma.prnewswire.com/media/1458111/Great_Learning_Logo.jpg?p=facebook\" width=\"200\" height=\"100\"/></center>\n",
    "\n",
    "<center><font size=10>Artificial Intelligence and Machine Learning</center></font>\n",
    "<center><font size=6>Natural Language Processing with Generative AI - Retrieval Augmented Generation</center></font>\n",
    "\n",
    "<center><img src=\"https://i.ibb.co/pBF9nKpf/apple.png\" width=\"720\"></center>\n",
    "\n",
    "<center><font size=6>Apple HBR Report Document Q&A</center></font>\n",
    "\n",
    "# RAG LLM Application Notebook\n",
    "\n",
    "This notebook demonstrates the complete workflow of a Retrieval-Augmented Generation (RAG) LLM application. It covers data loading, chunking, embedding, vector database setup, question answering, and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Library Installation\n",
    "\n",
    "First, we need to install all the necessary Python libraries. This ensures that all dependencies for data processing, LLM interaction, and vector database operations are met. If you are running this in a Colab environment, these commands will typically install the packages.\n",
    "\n",
    "**Note**: If you are running this locally, ensure you have Ollama installed and the `llama3.2` model pulled (`ollama pull llama3.2`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All specified packages installed (or upgraded) successfully!\n",
      "Please ensure the model is pulled for Ollama with the command:\n"
     ]
    }
   ],
   "source": [
    "# Install necessary packages\n",
    "# %pip install -U -q huggingface_hub tiktoken pymupdf langchain-community langchain langchain-chroma langchain-huggingface ollama\n",
    "\n",
    "\n",
    "print(\"All specified packages installed (or upgraded) successfully!\")\n",
    "print(\"Please ensure the model is pulled for Ollama with the command:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Necessary Modules\n",
    "\n",
    "We import the `RAG_LLM` class from `functions.py` and constants from `config.py`. Make sure `functions.py`, `config.py`, and `prompt_templates.py` are in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules imported successfully.\n"
     ]
    }
   ],
   "source": [
    "from functions import RAG_LLM\n",
    "from config import APPLE_PDF_PATH, DEFAULT_K_RETRIEVER, DEFAULT_MAX_TOKENS, DEFAULT_TEMPERATURE\n",
    "import os\n",
    "\n",
    "print(\"Modules imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize the RAG_LLM System\n",
    "\n",
    "Here, we create an instance of our `RAG_LLM` class. This object will manage the entire RAG pipeline, including data loading, processing, retrieval, and LLM interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG_LLM initialized.\n",
      "RAG_LLM system initialized.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the RAG_LLM class\n",
    "rag_system = RAG_LLM()\n",
    "print(\"RAG_LLM system initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load the PDF Document\n",
    "\n",
    "We load the `HBR_How_Apple_Is_Organized_For_Innovation.pdf` document. Ensure this PDF file is available in the working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: HBR_How_Apple_Is_Organized_For_Innovation.pdf\n",
      "Successfully loaded 11 pages.\n",
      "PDF document loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load the PDF document\n",
    "documents = rag_system.load_data(pdf_path=APPLE_PDF_PATH)\n",
    "if not documents:\n",
    "    print(\"Failed to load documents. Please check the PDF path and file existence.\")\n",
    "else:\n",
    "    print(\"PDF document loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Chunk the Loaded Data\n",
    "\n",
    "The loaded document is chunked into smaller, overlapping segments. This is crucial for efficient retrieval and to fit content within the LLM's context window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking data with chunk_size=1024, chunk_overlap=20\n",
      "Created 16 chunks.\n",
      "Documents chunked successfully.\n"
     ]
    }
   ],
   "source": [
    "# Chunk the loaded data\n",
    "document_chunks = rag_system.chunk_data(documents)\n",
    "if not document_chunks:\n",
    "    print(\"Failed to chunk documents.\")\n",
    "else:\n",
    "    print(\"Documents chunked successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Embedding Model\n",
    "\n",
    "An embedding model (SentenceTransformer) is initialized to convert text chunks into numerical vectors, enabling semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing embedding model: mixedbread-ai/mxbai-embed-large-v1\n",
      "Embedding model initialized successfully.\n",
      "Embedding model created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create embedding model\n",
    "rag_system.create_embeddings()\n",
    "if not rag_system.embedding_model:\n",
    "    print(\"Failed to create embedding model.\")\n",
    "else:\n",
    "    print(\"Embedding model created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Set Up the Vector Database\n",
    "\n",
    "The Chroma vector database is set up using the chunked documents and the embedding model. This database will store the embeddings and facilitate quick retrieval of relevant context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up vector database in: vector_db_1024\n",
      "Vector database loaded from existing directory.\n",
      "Retriever initialized.\n",
      "Vector database set up and retriever initialized.\n"
     ]
    }
   ],
   "source": [
    "# Set up the vector database\n",
    "rag_system.setup_vector_database(document_chunks=document_chunks)\n",
    "if not rag_system.vectorstore:\n",
    "    print(\"Failed to set up vector database.\")\n",
    "else:\n",
    "    print(\"Vector database set up and retriever initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Demonstrate Question Answering with RAG\n",
    "\n",
    "Now, we can ask questions and see how the RAG system retrieves relevant information and generates answers based on the loaded document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query 1: Who are the authors of this article and who published this article ?\n",
      "Retrieving 3 relevant documents for the query.\n",
      "RAG prompt created.\n",
      "Generating LLM response using model: deepseek-r1\n",
      "LLM response generated.\n",
      "Response 1: \n",
      "<think>\n",
      "Okay, let's look at the user's question carefully. The user wants to know who the authors of the article are and who published it. \n",
      "\n",
      "First, I'll scan through the provided context for any information about authors or publishers. In the first part of the document header, there's a line that says \"AUTHORS FOR ARTICLE REPRINTS CALL...\" which clearly lists the names: Joel M. Podolny and Morten T. Hansen. Then in another section, it mentions Harvard Business Review as the publisher with the date November–December 2020.\n",
      "\n",
      "The user seems to be asking for straightforward factual information from the text. They might need this for citation purposes or just general knowledge about the article's origins. \n",
      "\n",
      "Since both pieces of information are explicitly stated in the context without any ambiguity, I can provide a concise answer as per Rule #1. The authors' names appear together with their titles (Dean and Faculty) at Apple University, while Harvard Business Review is mentioned multiple times including near the publication date.\n",
      "\n",
      "The user's question doesn't require interpretation or additional explanation - just direct answers from the text. So no need to elaborate beyond what's written.\n",
      "</think>\n",
      "Joel M. Podolny and Morten T. Hansen  \n",
      "Harvard Business Review\n"
     ]
    }
   ],
   "source": [
    "# Example Query 1\n",
    "user_input_1 = \"Who are the authors of this article and who published this article ?\"\n",
    "print(f\"\\nQuery 1: {user_input_1}\")\n",
    "llm_response_1 = rag_system.get_answer(user_input_1)\n",
    "print(f\"Response 1: \\n{llm_response_1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query 2: List down the three leadership characteristics in bulleted points and explain each one of the characteristics under two lines.\n",
      "Retrieving 3 relevant documents for the query.\n",
      "RAG prompt created.\n",
      "Generating LLM response using model: deepseek-r1\n",
      "LLM response generated.\n",
      "Response 2: \n",
      "<think>\n",
      "Okay, let's tackle this query step by step. The user wants a list of three leadership characteristics from Apple's context, presented as bullet points with brief explanations for each.\n",
      "\n",
      "First, scanning through the provided text about Apple's organizational structure and Tim Cook's changes... I see mentions of \"leadership approach described above\" which includes experts leading experts, immersion in details, and collaborative debate. That seems like a solid starting point.\n",
      "\n",
      "Then there's this part about Rosner evolving his leadership role: he had been immersed in details especially concerning top-level software aspects but now spends more time teaching others... This suggests that \"teaching box\" is another key characteristic worth including.\n",
      "\n",
      "The user specifically asked for three characteristics, so I'll need to select the most prominent ones from what's available. The text mentions several approaches - owning, learning, teaching, and delegating. But since they want just three bullet points with two-line explanations each, focusing on these would work well:\n",
      "\n",
      "- Immersion in details (owning box)\n",
      "- Collaborative debate\n",
      "- Teaching others\n",
      "\n",
      "Each of these can be explained concisely within two lines based solely on the provided context without adding any external knowledge or rephrasing the question.\n",
      "</think>\n",
      "*   **Immersion in the details:** Leaders must decide which activities demand their full attention to detail because those activities create the most value for Apple. They must exercise greater discretion regarding where and how they spend their time and efforts.\n",
      "\n",
      "*   **Expert leading expert (owning box):** This is a core characteristic, requiring leaders to be deeply immersed in specific areas of expertise within their function, overseeing high-detail tasks like architecture or user engagement aspects directly.\n",
      "\n",
      "*   **Teaching others:** Leaders guide and give feedback to team members so they can develop software applications according to Apple's norms. Being a teacher doesn't mean giving instruction at a whiteboard; rather, offering strong critiques helps cultivate expertise in others (moving things from owning box).\n"
     ]
    }
   ],
   "source": [
    "# Example Query 2\n",
    "user_input_2 = \"List down the three leadership characteristics in bulleted points and explain each one of the characteristics under two lines.\"\n",
    "print(f\"\\nQuery 2: {user_input_2}\")\n",
    "# Adjust max_tokens to allow for a more complete answer for a list\n",
    "llm_response_2 = rag_system.get_answer(user_input_2, max_tokens=150, temperature=0.1)\n",
    "print(f\"Response 2: \\n{llm_response_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query 3: Can you explain specific examples from the article where Apple's approach to leadership has led to successful innovations?\n",
      "Retrieving 3 relevant documents for the query.\n",
      "RAG prompt created.\n",
      "Generating LLM response using model: deepseek-r1\n",
      "LLM response generated.\n",
      "Response 3: \n",
      "<think>\n",
      "Okay, let me analyze this carefully. The user wants specific examples from the article about how Apple's leadership approach contributed to successful innovations. \n",
      "\n",
      "Looking at the context provided, I see several key points that directly address this question. First, there's mention of Rosner (VP of applications) managing a diverse portfolio including News, Clips, Books and Final Cut Pro - showing leaders need expertise beyond their core areas as the company grows complex.\n",
      "\n",
      "Then there are concrete examples like: \n",
      "- Adding AI/ML as a functional area\n",
      "- Merging human interface with industrial design\n",
      "\n",
      "These changes demonstrate how Apple adapted its leadership structure to enable collaboration across domains. The context also explains that this approach allows leaders to focus on high-value activities while others handle less critical details.\n",
      "\n",
      "The answer should be concise and directly from the text, listing these specific examples without adding external knowledge or rephrasing the question.\n",
      "</think>\n",
      "Apple's functional organization with a discretionary leadership model has enabled successful innovations by allowing leaders to focus deeply on key areas. For example:\n",
      "- Adding AI/ML as a functional area demonstrates adapting to new technologies while maintaining cross-functional expertise.\n",
      "- Merging human interface (software) with industrial design shows integration across domains, fostering collaboration for unified product development.\n",
      "\n",
      "This approach ensures that leaders like Rosner can prioritize high-value activities in their expanding portfolios, driving innovation effectively.\n"
     ]
    }
   ],
   "source": [
    "# Example Query 3 (expected to be \"I don't know\" if not in context)\n",
    "user_input_3 = \"Can you explain specific examples from the article where Apple's approach to leadership has led to successful innovations?\"\n",
    "print(f\"\\nQuery 3: {user_input_3}\")\n",
    "llm_response_3 = rag_system.get_answer(user_input_3)\n",
    "print(f\"Response 3: \\n{llm_response_3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Demonstrate Output Evaluation (LLM-as-a-Judge)\n",
    "\n",
    "Finally, we demonstrate how the RAG system can evaluate its own answers for 'groundedness' (adherence to context) and 'relevance' (how well it answers the question)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Query 1:\n",
      "\n",
      "--- Calculating Ratings for Question: 'Who are the authors of this article and who published this article ?' ---\n",
      "Retrieving 3 relevant documents for the query.\n",
      "RAG prompt created.\n",
      "Generating LLM response using model: deepseek-r1\n",
      "LLM response generated.\n",
      "Rating overall answer quality (groundedness and relevance)...\n",
      "Rating groundedness...\n",
      "Retrieving 3 relevant documents for the query.\n",
      "Generating LLM response using model: deepseek-r1\n",
      "LLM response generated.\n",
      "Rating relevance...\n",
      "Retrieving 3 relevant documents for the query.\n",
      "Generating LLM response using model: deepseek-r1\n",
      "LLM response generated.\n",
      "\n",
      "--- Results ---\n",
      "Question: \n",
      " Who are the authors of this article and who published this article ?\n",
      "\n",
      "Answer: \n",
      " <think>\n",
      "Okay, let's look at the user's question carefully. The user wants to know who the authors of the article are and who published it. \n",
      "\n",
      "First, I'll scan through the provided context for any information about authors or publishers. In the first part of the document header, there's a line that says \"AUTHORS FOR ARTICLE REPRINTS CALL...\" which clearly lists the names: Joel M. Podolny and Morten T. Hansen. Then in another section, it mentions Harvard Business Review as the publisher with the date November–December 2020.\n",
      "\n",
      "The user seems to be asking for straightforward factual information from the text. They might need this for citation purposes or just general knowledge about the article's origins. \n",
      "\n",
      "Since both pieces of information are explicitly stated in the context without any ambiguity, I can provide a concise answer as per Rule #1. The authors' names appear together with their titles (Dean and Faculty) at Apple University, while Harvard Business Review is mentioned multiple times including near the publication date.\n",
      "\n",
      "The user's question doesn't require interpretation or additional explanation - just direct answers from the text. So no need to elaborate beyond what's written.\n",
      "</think>\n",
      "Joel M. Podolny and Morten T. Hansen  \n",
      "Harvard Business Review\n",
      "\n",
      "Groundedness Rating: \n",
      " \n",
      "\n",
      "Relevance Rating: \n",
      " \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Query 1\n",
    "user_input_1 = \"Who are the authors of this article and who published this article ?\"\n",
    "print(\"\\nEvaluating Query 1:\")\n",
    "rag_system.calculate_rating(question=user_input_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Query 2:\n",
      "\n",
      "--- Calculating Ratings for Question: 'List down the three leadership characteristics in bulleted points and explain each one of the characteristics under two lines.' ---\n",
      "Retrieving 3 relevant documents for the query.\n",
      "RAG prompt created.\n",
      "Generating LLM response using model: deepseek-r1\n",
      "LLM response generated.\n",
      "Rating overall answer quality (groundedness and relevance)...\n",
      "Rating groundedness...\n",
      "Retrieving 3 relevant documents for the query.\n",
      "Generating LLM response using model: deepseek-r1\n",
      "LLM response generated.\n",
      "Rating relevance...\n",
      "Retrieving 3 relevant documents for the query.\n",
      "Generating LLM response using model: deepseek-r1\n",
      "LLM response generated.\n",
      "\n",
      "--- Results ---\n",
      "Question: \n",
      " List down the three leadership characteristics in bulleted points and explain each one of the characteristics under two lines.\n",
      "\n",
      "Answer: \n",
      " <think>\n",
      "Okay, let's tackle this query step by step. The user wants a list of three leadership characteristics from Apple's context, presented in bulleted points with brief explanations for each.\n",
      "\n",
      "First, I need to carefully review the provided context document about Apple's organizational structure and leadership approaches. Looking through it, several key phrases stand out that describe leadership behaviors at Apple:\n",
      "\n",
      "The text mentions \"experts leading experts\", which seems like one core characteristic. Then there's \"immersion in the details\" as another approach leaders take. Finally, \"collaborative debate\" emerges as a third way they engage with their teams.\n",
      "\n",
      "For each of these three characteristics:\n",
      "- Experts Leading Experts: This appears to be about leaders focusing on areas where they have deep expertise and guiding others accordingly.\n",
      "- Immersion in Details: The context shows leaders spending significant time understanding the specifics of their domains (40% ownership focus).\n",
      "- Collaborative Debate: Leaders work with specialists across different functions through discussion and debate.\n",
      "\n",
      "Each point should explain these characteristics under two lines, keeping explanations concise but informative. This approach directly answers what's clearly stated in the document without adding external knowledge or rephrasing.\n",
      "</think>\n",
      "*   Experts leading experts (focus on areas of deep expertise).\n",
      "    *   Leaders focus their guidance on domains where they themselves are specialists.\n",
      "\n",
      "*   Immersion in the details (deep involvement with core tasks).\n",
      "    *   Senior leaders spend significant time understanding and overseeing crucial aspects within their area of responsibility.\n",
      "\n",
      "*   Collaborative debate (working across functional boundaries through discussion).\n",
      "    *   Leaders engage specialists from different fields in joint projects by fostering open dialogue and shared decision-making.\n",
      "\n",
      "Groundedness Rating: \n",
      " <think>\n",
      "Okay, let's break this down. The user wants me to evaluate an AI-generated answer based on whether it follows the metric of being derived only from the provided context.\n",
      "\n",
      "First, I need to identify what leadership characteristics are mentioned in the context. Scanning through Apple's organizational structure description, I see several key approaches: leaders focusing on areas where they have deep expertise (\"experts leading experts\"), spending significant time understanding details (\"immersion in the details\" or \"owning box\"), and engaging specialists from different fields (\"collaborative debate\"). \n",
      "\n",
      "The answer provided lists three characteristics that perfectly match these points. Each bullet corresponds directly to one of these approaches mentioned in the context, with concise explanations that capture the essence described in the text.\n",
      "\n",
      "This seems like a good fit because all three points are explicitly stated concepts within Apple's leadership model as detailed in the context document. The answer doesn't introduce any ideas not present in this specific text about how Apple organizes its structure or operates.\n",
      "</think>\n",
      "###Answer Evaluation\n",
      "\n",
      "1.  **Identify Key Leadership Concepts from Context:** Read through the provided context to determine what core aspects of leadership are described. The text discusses approaches like \"experts leading experts,\" \"immersion in the details\" (owning box), and \"collaborative debate.\" It also mentions delegation (\"delegating box\") as a separate function.\n",
      "\n",
      "2.  **Check Answer for Direct Derivation:** Review the AI-generated answer to see if it lists leadership characteristics that are explicitly mentioned or implied by these concepts within the context document.\n",
      "\n",
      "3.  **Analyze Explanation Content:** For each characteristic listed in the answer, examine whether its explanation is derived solely from the information presented in the corresponding part of the context (e.g., does \"Experts leading experts\" explain the focus on areas where leaders are specialists?).\n",
      "\n",
      "###Step-by-Step Explanation\n",
      "\n",
      "The question asks for three leadership characteristics and brief explanations.\n",
      "\n",
      "*   **Characteristic 1: Experts Leading Experts:** The answer includes this point. The explanation (\"Leaders focus their guidance on domains where they themselves are specialists\") directly reflects Apple's context, which describes leaders focusing on areas of deep expertise (e.g., \"immersion in the details\" for core tasks, leading other experts).\n",
      "\n",
      "*   **Characteristic 2: Immersion in Details:** The answer includes this point. The explanation (\"Leaders spend significant time understanding and overseeing crucial aspects within their area of responsibility\") aligns with Apple's context describing leaders spending most of their time in \"owning box\" activities (40% according to Rosner) which involve deep involvement.\n",
      "\n",
      "*   **Characteristic 3: Collaborative Debate:** The answer includes this point. The explanation (\"Leaders engage specialists from different fields in joint projects by fostering open dialogue and shared decision-making\") corresponds directly with Apple's context describing the need for \"steep learning\" through collaboration across units (e.g., portrait mode camera requiring debate) and leaders needing to foster collaborative debate.\n",
      "\n",
      "The explanations provided are concise summaries of these concepts, using language derived from the context. There is no information in the answer that goes beyond what is presented or implied by the context document regarding Apple's leadership characteristics at scale.\n",
      "\n",
      "###Extent Evaluation\n",
      "\n",
      "*   The answer lists three distinct points: \"Experts leading experts,\" \"Immersion in the details,\" and \"Collaborative debate.\"\n",
      "*   Each point corresponds to a core concept described in the context (specialist teams, owning/learning boxes, collaborative debate).\n",
      "*   The explanations for each characteristic are directly derived from the text provided in the context.\n",
      "\n",
      "###Rating\n",
      "\n",
      "The answer **follows completely** the metric because it identifies and explains three leadership characteristics that are explicitly mentioned or strongly implied by the concepts within the given context.\n",
      "\n",
      "Relevance Rating: \n",
      " \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Query 2\n",
    "user_input_2 = \"List down the three leadership characteristics in bulleted points and explain each one of the characteristics under two lines.\"\n",
    "print(\"\\nEvaluating Query 2:\")\n",
    "# Using parameters that yield a better answer for evaluation\n",
    "rag_system.calculate_rating(question=user_input_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Query 3:\n",
      "\n",
      "--- Calculating Ratings for Question: 'Can you explain specific examples from the article where Apple's approach to leadership has led to successful innovations?' ---\n",
      "Retrieving 3 relevant documents for the query.\n",
      "RAG prompt created.\n",
      "Generating LLM response using model: deepseek-r1\n",
      "LLM response generated.\n",
      "Rating overall answer quality (groundedness and relevance)...\n",
      "Rating groundedness...\n",
      "Retrieving 3 relevant documents for the query.\n",
      "Generating LLM response using model: deepseek-r1\n",
      "LLM response generated.\n",
      "Rating relevance...\n",
      "Retrieving 3 relevant documents for the query.\n",
      "Generating LLM response using model: deepseek-r1\n",
      "LLM response generated.\n",
      "\n",
      "--- Results ---\n",
      "Question: \n",
      " Can you explain specific examples from the article where Apple's approach to leadership has led to successful innovations?\n",
      "\n",
      "Answer: \n",
      " <think>\n",
      "Okay, let me analyze this carefully. The user wants specific examples from the article about how Apple's leadership approach contributed to successful innovations. \n",
      "\n",
      "Looking at the context provided, I see several key points that directly address this question. First, there's mention of Rosner (VP of applications) managing a diverse portfolio including News, Clips, Books and Final Cut Pro - showing leaders need expertise beyond their core areas as the company grows complex.\n",
      "\n",
      "Then there are concrete examples like: \n",
      "- Adding AI/ML as a functional area\n",
      "- Merging human interface with industrial design\n",
      "\n",
      "These changes demonstrate how Apple adapted its leadership structure to enable collaboration across domains. The context also explains that this approach allows leaders to focus on high-value activities while others handle less critical details.\n",
      "\n",
      "The answer should be concise and directly from the text, listing these specific examples without adding external knowledge or rephrasing the question.\n",
      "</think>\n",
      "Apple's functional organization with a discretionary leadership model has enabled successful innovations by allowing leaders to focus deeply on key areas. For example:\n",
      "- Adding AI/ML as a functional area demonstrates adapting to new technologies while maintaining cross-functional expertise.\n",
      "- Merging human interface (software) with industrial design shows integration across domains, fostering collaboration for unified product development.\n",
      "\n",
      "This approach ensures that leaders like Rosner can prioritize high-value activities in their expanding portfolios, driving innovation effectively.\n",
      "\n",
      "Groundedness Rating: \n",
      " \n",
      "\n",
      "Relevance Rating: \n",
      " \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Query 3\n",
    "user_input_3 = \"Can you explain specific examples from the article where Apple's approach to leadership has led to successful innovations?\"\n",
    "print(\"\\nEvaluating Query 3:\")\n",
    "rag_system.calculate_rating(question=user_input_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook provides a comprehensive walkthrough of setting up and using a RAG LLM system for document Q&A and evaluation. You can modify the parameters and queries to experiment further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1203585a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
