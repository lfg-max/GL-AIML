<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Glorot & Bengio (Xavier) Initialization Presentation</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
    <script>
        MathJax = {
          tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']],
            processEscapes: true
          },
          svg: {
            fontCache: 'global'
          }
        };
    </script>
    <script type="text/javascript" id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
    </script>
    <style>
        body {
            font-family: 'Inter', sans-serif;
            background-color: #f3f4f6;
        }
        .slide {
            display: none;
        }
        .slide.active {
            display: block;
        }
        /* Style for disabled buttons */
        .disabled {
            opacity: 0.5;
            cursor: not-allowed;
        }
    </style>
</head>
<body class="flex flex-col items-center justify-center min-h-screen bg-gray-100 p-4">

    <div id="presentation-container" class="w-full max-w-4xl bg-white rounded-xl shadow-2xl overflow-hidden">
        <!-- Slide content will be injected here -->
        <div id="slide-content" class="p-8 md:p-12 min-h-[400px] md:min-h-[500px] flex flex-col justify-center">

            <!-- Slide 1: Title -->
            <div class="slide active">
                <div class="text-center">
                    <h1 class="text-4xl md:text-5xl font-bold text-gray-800 mb-4">Understanding the difficulty of training deep feedforward neural networks</h1>
                    <p class="text-xl md:text-2xl text-gray-600 mt-4">Xavier Glorot & Yoshua Bengio</p>
                    <p class="text-lg text-gray-500 mt-2">JMLR 2010</p>
                </div>
            </div>

            <!-- Slide 2: The Core Problem -->
            <div class="slide">
                <h2 class="text-3xl font-bold text-gray-800 mb-6">The Core Problem: Unstable Gradients</h2>
                <p class="text-lg text-gray-700 mb-4">Training deep neural networks with standard gradient descent and random initialization was notoriously difficult. The paper investigates why.</p>
                <p class="text-lg text-gray-700 mb-4">The central issue is the <strong class="text-red-600">vanishing or exploding gradient problem</strong>.</p>
                <ul class="list-disc list-inside space-y-3 text-lg text-gray-700">
                    <li>As gradients are backpropagated from the final layer to the initial layers, they can shrink or grow exponentially.</li>
                    <li><strong class="text-red-500">Vanishing Gradients:</strong> Gradients become tiny, causing the weights of the initial layers to update very slowly or not at all. The model fails to learn.</li>
                    <li><strong class="text-red-500">Exploding Gradients:</strong> Gradients become huge, leading to massive weight updates and unstable training (often resulting in NaN values).</li>
                </ul>
            </div>

            <!-- Slide 3: The Role of Activation Functions -->
            <div class="slide">
                <h2 class="text-3xl font-bold text-gray-800 mb-6">The Role of Activation Functions</h2>
                <p class="text-lg text-gray-700 mb-4">The choice of activation function is critical. The paper analyzes the popular <strong class="text-orange-500">sigmoid</strong> and <strong class="text-indigo-500">hyperbolic tangent (tanh)</strong> functions.</p>
                <div class="flex flex-col md:flex-row gap-6">
                    <!-- Sigmoid -->
                    <div class="flex-1 border-2 border-orange-200 bg-orange-50 p-4 rounded-lg">
                        <h3 class="text-2xl font-semibold text-orange-800 mb-3">Sigmoid: $f(x) = 1/(1+e^{-x})$</h3>
                        <p class="text-lg text-gray-700">Has a mean value > 0, which can cause all activations in the next layer to be positive. This leads to undesirable, non-zero-centered dynamics and makes training harder.</p>
                    </div>
                    <!-- Tanh -->
                    <div class="flex-1 border-2 border-indigo-200 bg-indigo-50 p-4 rounded-lg">
                        <h3 class="text-2xl font-semibold text-indigo-800 mb-3">Tanh: $f(x) = \tanh(x)$</h3>
                         <p class="text-lg text-gray-700">Is zero-centered, which helps keep the inputs to the next layer more centered. This leads to faster convergence.</p>
                    </div>
                </div>
                 <p class="text-lg text-gray-700 mt-6">However, both functions suffer from <strong class="text-red-600">saturation</strong>: their gradients are close to zero for large positive or negative inputs. </p>
            </div>
            
            <!-- Slide 4: The Saturation Problem Visualized -->
            <div class="slide">
                <h2 class="text-3xl font-bold text-gray-800 mb-6">Activation Saturation</h2>
                <p class="text-lg text-gray-700 mb-4">With poor initialization, many neurons can become saturated from the start. Their gradients are near zero, so they learn very slowly. The paper visualizes this effect in a 5-layer network.</p>
                <img src="https://i.imgur.com/T2oV2o2.png" alt="Activation values for tanh and sigmoid" class="rounded-lg shadow-xl mx-auto mb-2 border" onerror="this.onerror=null;this.src='https://placehold.co/600x300/e0e7ff/3730a3?text=Activation+Distributions';">
                <p class="text-sm text-gray-600 text-center">With Tanh (top), activations are centered around 0. With Sigmoid (bottom), they are pushed towards 1, causing saturation in the top layers.</p>
            </div>

            <!-- Slide 5: Analyzing Signal Propagation -->
            <div class="slide">
                <h2 class="text-3xl font-bold text-gray-800 mb-6">Analyzing Signal Propagation</h2>
                <p class="text-lg text-gray-700 mb-4">The key idea is to analyze how the <strong class="text-blue-600">variance</strong> of activations and gradients changes as it propagates through the network.</p>
                <ul class="list-disc list-inside space-y-3 text-lg text-gray-700">
                    <li>Let $z^i$ be the output of layer $i$, and $W^i$ be its weights.</li>
                    <li><strong class="text-green-600">Forward Propagation:</strong> We want the variance of the outputs to be the same as the variance of the inputs. $Var(z^i) = Var(z^{i-1})$.</li>
                    <li><strong class="text-purple-600">Backward Propagation:</strong> We want the variance of the gradients to remain constant as well. $Var(\frac{\partial C}{\partial s^i}) = Var(\frac{\partial C}{\partial s^{i+1}})$. ($s^i$ is the weighted sum at layer $i$).</li>
                </ul>
                <p class="text-lg text-gray-700 mt-4">If these conditions are not met, the signal (activations or gradients) will shrink or grow exponentially layer by layer.</p>
            </div>

            <!-- Slide 6: Deriving a Principled Initialization -->
            <div class="slide">
                <h2 class="text-3xl font-bold text-gray-800 mb-4">Deriving a Principled Initialization</h2>
                <p class="text-lg text-gray-700 mb-4">By making some simplifying assumptions (linear activations, weights and inputs are independent and identically distributed with zero mean), we can derive a condition on the variance of the weights.</p>
                <div class="space-y-4 text-lg">
                    <div class="bg-gray-50 p-4 rounded-lg">
                        <p class="font-semibold mb-2">Forward Pass Condition:</p>
                        <p>To keep activation variance constant, we need: $n_i Var(W^i) = 1$</p>
                    </div>
                    <div class="bg-gray-50 p-4 rounded-lg">
                        <p class="font-semibold mb-2">Backward Pass Condition:</p>
                        <p>To keep gradient variance constant, we need: $n_{i+1} Var(W^i) = 1$</p>
                    </div>
                </div>
                <p class="text-sm text-gray-500 mt-4">* Where $n_i$ is the number of input units (fan-in) and $n_{i+1}$ is the number of output units (fan-out) of layer $i$.</p>
            </div>

            <!-- Slide 7: The Glorot / Xavier Initialization -->
            <div class="slide">
                <h2 class="text-3xl font-bold text-gray-800 mb-6">The Glorot / Xavier Initialization</h2>
                <p class="text-lg text-gray-700 mb-4">The two conditions, $n_i Var(W^i) = 1$ and $n_{i+1} Var(W^i) = 1$, are generally impossible to satisfy simultaneously if $n_i \neq n_{i+1}$.</p>
                <p class="text-lg text-gray-700 mb-4">A compromise is to use their harmonic mean:</p>
                <div class="bg-blue-50 p-6 rounded-lg shadow-md text-center">
                    <p class="text-2xl text-blue-800 font-semibold">$Var(W^i) = \frac{2}{n_i + n_{i+1}}$</p>
                </div>
                <p class="text-lg text-gray-700 mt-6">Assuming the weights are sampled from a uniform distribution $U[-a, a]$, the variance is $Var(W) = a^2/3$. This gives us the final initialization rule:</p>
                <div class="bg-green-50 p-6 rounded-lg shadow-md text-center mt-4">
                     <p class="text-2xl text-green-800 font-semibold">$W \sim U\left[-\frac{\sqrt{6}}{\sqrt{n_i + n_{i+1}}}, \frac{\sqrt{6}}{\sqrt{n_i + n_{i+1}}}\right]$</p>
                </div>
            </div>

            <!-- Slide 8: Experimental Validation -->
            <div class="slide">
                <h2 class="text-3xl font-bold text-gray-800 mb-6">Experimental Validation</h2>
                <p class="text-lg text-gray-700 mb-4">The paper shows that this "normalized initialization" dramatically improves training performance compared to a standard random initialization scheme.</p>
                <img src="https://i.imgur.com/tL4tJ9M.png" alt="Training error for random vs normalized initialization" class="rounded-lg shadow-xl mx-auto mb-2 border" onerror="this.onerror=null;this.src='https://placehold.co/600x300/e0e7ff/3730a3?text=Training+Error+Curves';">
                <p class="text-sm text-gray-600 text-center">The network with normalized initialization (right) learns much faster and achieves a lower error rate than the one with standard initialization (left).</p>
            </div>

            <!-- Slide 9: Conclusion & Impact -->
            <div class="slide">
                 <h2 class="text-3xl font-bold text-gray-800 mb-6">Conclusion & Impact</h2>
                <div class="bg-gray-50 p-6 rounded-lg shadow-md">
                    <p class="text-lg text-gray-700 mb-4">This paper provided a deep analysis of why training deep networks was difficult and offered a simple, practical, and theoretically-grounded solution.</p>
                    <p class="text-lg text-gray-700 font-semibold">The key insight was to initialize the weights in a way that <strong class="text-blue-600">preserves the variance of activations and gradients</strong> as they propagate through the network.</p>
                </div>
                <p class="text-lg text-gray-700 mt-6">Glorot (Xavier) initialization became a standard practice and a crucial stepping stone that enabled the training of much deeper neural networks, paving the way for the deep learning revolution.</p>
            </div>
            
            <!-- Slide 10: Q&A -->
            <div class="slide">
                <div class="text-center">
                    <h1 class="text-5xl md:text-6xl font-bold text-gray-800 mb-6">Thank You</h1>
                    <p class="text-3xl md:text-4xl text-gray-600">Questions?</p>
                </div>
            </div>

        </div>

        <!-- Navigation -->
        <div class="bg-gray-800 p-4 flex justify-between items-center">
            <button id="prevBtn" class="bg-gray-600 hover:bg-gray-500 text-white font-bold py-2 px-4 rounded-lg transition duration-300">
                Previous
            </button>
            <div id="slide-counter" class="text-white text-lg">
                Slide <span id="current-slide-num">1</span> of <span id="total-slides-num"></span>
            </div>
            <button id="nextBtn" class="bg-blue-600 hover:bg-blue-500 text-white font-bold py-2 px-4 rounded-lg transition duration-300">
                Next
            </button>
        </div>
    </div>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            const slides = document.querySelectorAll('.slide');
            const prevBtn = document.getElementById('prevBtn');
            const nextBtn = document.getElementById('nextBtn');
            const currentSlideNumEl = document.getElementById('current-slide-num');
            const totalSlidesNumEl = document.getElementById('total-slides-num');
            
            let currentSlide = 0;
            const totalSlides = slides.length;

            totalSlidesNumEl.textContent = totalSlides;

            function showSlide(n) {
                // Hide all slides
                slides.forEach(slide => slide.classList.remove('active'));
                
                // Show the target slide
                slides[n].classList.add('active');
                
                // Update slide counter
                currentSlideNumEl.textContent = n + 1;

                // Update button states
                if (currentSlide === 0) {
                    prevBtn.classList.add('disabled');
                } else {
                    prevBtn.classList.remove('disabled');
                }

                if (currentSlide === totalSlides - 1) {
                    nextBtn.classList.add('disabled');
                } else {
                    nextBtn.classList.remove('disabled');
                }
            }

            function nextSlide() {
                if (currentSlide < totalSlides - 1) {
                    currentSlide++;
                    showSlide(currentSlide);
                }
            }

            function prevSlide() {
                if (currentSlide > 0) {
                    currentSlide--;
                    showSlide(currentSlide);
                }
            }

            // Event Listeners
            nextBtn.addEventListener('click', nextSlide);
            prevBtn.addEventListener('click', prevSlide);

            // Keyboard navigation
            document.addEventListener('keydown', function(e) {
                if (e.key === 'ArrowRight') {
                    nextSlide();
                } else if (e.key === 'ArrowLeft') {
                    prevSlide();
                }
            });

            // Initial setup
            showSlide(currentSlide);
        });
    </script>

</body>
</html>
