{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RTS_iP-pojY7"
   },
   "source": [
    "<center><p float=\"center\">\n",
    "  <img src=\"https://upload.wikimedia.org/wikipedia/commons/e/e9/4_RGB_McCombs_School_Brand_Branded.png\" width=\"300\" height=\"100\"/>\n",
    "  <img src=\"https://mma.prnewswire.com/media/1458111/Great_Learning_Logo.jpg?p=facebook\" width=\"200\" height=\"100\"/>\n",
    "</p></center>\n",
    "\n",
    "<h1><center><font size=10>Data Science and Business Analytics</center></font></h1>\n",
    "<h1><center>Ensemble Techniques: Boosting - Week 2</center></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHLo2PjNonm5"
   },
   "source": [
    "<center><img src=\"https://images.pexels.com/photos/6280594/pexels-photo-6280594.jpeg?auto=compress&cs=tinysrgb&w=1260&h=750&dpr=2\" width=\"800\" height=\"500\"></center>\n",
    "\n",
    "<b><h2><center>Bike Sharing Case Study</center></h2></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oYJ7QaVxpZ3z"
   },
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RJNKo6Wjpke5"
   },
   "source": [
    "### Context:\n",
    "\n",
    "Bike-sharing systems are a new generation of traditional bike rentals where the whole process from membership, rental and return has become automatic. Through these systems, the user can easily rent a bike from a particular position and return to another position. Currently, there are about over 500 bike-sharing programs around the world which are composed of over 500 thousand bicycles. Today, there exists a great interest in these systems due to their important role in traffic, environmental, and health issues.\n",
    "\n",
    "\n",
    "### Objective:\n",
    "'Travel Along' is a new bike-sharing company and wants to expand its customer count and provide better services at a reasonable cost. They have conducted several surveys and collated the data about weather, weekends, holidays, etc. from the past 2 years.\n",
    "\n",
    "As a recently hired data scientist at 'Travel Along', you have been asked to analyze the patterns in the data and figure out the key areas which can help the organization to grow and manage the customer demands. Further, you need to use this information to predict the count of bikes shared so that the company can take prior decisions for surge hours.\n",
    "- What are the different factors which affect the target variable? What business recommendations can we give based on the analysis?\n",
    "- How can we use different ensemble techniques - Bagging, Boosting, and Stacking to build a model to predict the count of bikes rented?\n",
    "\n",
    "### Data Description:\n",
    "\n",
    "The bike-sharing rental process is highly correlated to the environmental and seasonal settings. For instance, weather conditions, precipitation, the day of week, season, the hour of the day, etc. can affect the rental behaviors.\n",
    "\n",
    "- instant: record index\n",
    "- dteday : date\n",
    "- season : season (1:spring, 2:summer, 3:fall, 4:winter)\n",
    "- yr : year (0: 2011, 1:2012)\n",
    "- mnth : month ( 1 to 12)\n",
    "- hr : hour (0 to 23)\n",
    "- holiday : whether the day is holiday or not\n",
    "- weekday : day of the week\n",
    "- workingday : if day is neither weekend nor holiday then 1, otherwise is 0.\n",
    "- weathersit :\n",
    "\t- 1: Clear, Few clouds, Partly cloudy\n",
    "\t- 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n",
    "\t- 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n",
    "\t- 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n",
    "- temp : Normalized temperature in Celsius. The values are divided by 41 (max)\n",
    "- atemp: Normalized feeling temperature in Celsius. The values are divided to 50 (max). The “feel like” temperature relies on environmental data including the ambient air temperature, relative humidity, and wind speed to determine how weather conditions feel to bare skin.\n",
    "- hum: Normalized humidity. The values are divided by 100 (max)\n",
    "- windspeed: Normalized wind speed. The values are divided by 67 (max)\n",
    "- casual: count of casual users\n",
    "- registered: count of registered users\n",
    "- cnt: count of total rental bikes including both casual and registered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SyFLAFFxpke7"
   },
   "source": [
    "## Importing necessary libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V4OKDXvSpke7"
   },
   "source": [
    "**Let's start by importing libraries we need.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1741402239135,
     "user": {
      "displayName": "Deep Learning02",
      "userId": "04674597338778555609"
     },
     "user_tz": -330
    },
    "id": "cRBN2iFBpke7"
   },
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "\nXGBoost Library (libxgboost.dylib) could not be loaded.\nLikely causes:\n  * OpenMP runtime is not installed\n    - vcomp140.dll or libgomp-1.dll for Windows\n    - libomp.dylib for Mac OSX\n    - libgomp.so for Linux and other UNIX-like OSes\n    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\n\n  * You are running 32-bit Python on a 64-bit OS\n\nError message(s): [\"dlopen(/Users/nra29/VScode/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.dylib, 0x0006): Library not loaded: @rpath/libomp.dylib\\n  Referenced from: <89AD948E-E564-3266-867D-7AF89D6488F0> /Users/nra29/VScode/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.dylib\\n  Reason: tried: '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file)\"]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DecisionTreeRegressor\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaggingRegressor,RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, StackingRegressor\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XGBRegressor\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m metrics\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GridSearchCV, train_test_split\n",
      "File \u001b[0;32m~/VScode/.venv/lib/python3.12/site-packages/xgboost/__init__.py:6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"XGBoost: eXtreme Gradient Boosting library.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mContributors: https://github.com/dmlc/xgboost/blob/master/CONTRIBUTORS.md\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tracker  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m collective, dask\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      9\u001b[0m     Booster,\n\u001b[1;32m     10\u001b[0m     DataIter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     build_info,\n\u001b[1;32m     16\u001b[0m )\n",
      "File \u001b[0;32m~/VScode/.venv/lib/python3.12/site-packages/xgboost/tracker.py:9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01menum\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IntEnum, unique\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dict, Optional, Union\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _LIB, _check_call, make_jcargs\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_family\u001b[39m(addr: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m     13\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get network family from address.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/VScode/.venv/lib/python3.12/site-packages/xgboost/core.py:269\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# load the XGBoost library globally\u001b[39;00m\n\u001b[0;32m--> 269\u001b[0m _LIB \u001b[38;5;241m=\u001b[39m \u001b[43m_load_lib\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_call\u001b[39m(ret: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m    This function will raise exception when error occurs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;124;03m        return value from API calls\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/VScode/.venv/lib/python3.12/site-packages/xgboost/core.py:222\u001b[0m, in \u001b[0;36m_load_lib\u001b[0;34m()\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib_success:\n\u001b[1;32m    221\u001b[0m         libname \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(lib_paths[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 222\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m XGBoostError(\n\u001b[1;32m    223\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;124mXGBoost Library (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlibname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) could not be loaded.\u001b[39m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;124mLikely causes:\u001b[39m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;124m  * OpenMP runtime is not installed\u001b[39m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;124m    - vcomp140.dll or libgomp-1.dll for Windows\u001b[39m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;124m    - libomp.dylib for Mac OSX\u001b[39m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;124m    - libgomp.so for Linux and other UNIX-like OSes\u001b[39m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;124m    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\u001b[39m\n\u001b[1;32m    231\u001b[0m \n\u001b[1;32m    232\u001b[0m \u001b[38;5;124m  * You are running 32-bit Python on a 64-bit OS\u001b[39m\n\u001b[1;32m    233\u001b[0m \n\u001b[1;32m    234\u001b[0m \u001b[38;5;124mError message(s): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos_error_list\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    236\u001b[0m         )\n\u001b[1;32m    237\u001b[0m     _register_log_callback(lib)\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(ver: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]:\n",
      "\u001b[0;31mXGBoostError\u001b[0m: \nXGBoost Library (libxgboost.dylib) could not be loaded.\nLikely causes:\n  * OpenMP runtime is not installed\n    - vcomp140.dll or libgomp-1.dll for Windows\n    - libomp.dylib for Mac OSX\n    - libgomp.so for Linux and other UNIX-like OSes\n    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\n\n  * You are running 32-bit Python on a 64-bit OS\n\nError message(s): [\"dlopen(/Users/nra29/VScode/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.dylib, 0x0006): Library not loaded: @rpath/libomp.dylib\\n  Referenced from: <89AD948E-E564-3266-867D-7AF89D6488F0> /Users/nra29/VScode/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.dylib\\n  Reason: tried: '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file)\"]\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingRegressor,RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, StackingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cF6jj7mGqDcV"
   },
   "source": [
    "## Reading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23727,
     "status": "ok",
     "timestamp": 1741402262853,
     "user": {
      "displayName": "Deep Learning02",
      "userId": "04674597338778555609"
     },
     "user_tz": -330
    },
    "id": "78MVIFQmm7S5",
    "outputId": "b0603226-5d55-47af-99e9-9af1d5e42a21"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# #Loading dataset\n",
    "# data=pd.read_csv(\"/content/drive/MyDrive/datasets/hour.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 870,
     "status": "ok",
     "timestamp": 1741402263721,
     "user": {
      "displayName": "Deep Learning02",
      "userId": "04674597338778555609"
     },
     "user_tz": -330
    },
    "id": "FqbyHCwqpke8"
   },
   "outputs": [],
   "source": [
    "#Loading dataset\n",
    "data=pd.read_csv(\"hour.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnYU6n7wqFvh"
   },
   "source": [
    "## Overview of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "33mgCPVfpke8"
   },
   "source": [
    "### View the first 5 rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 179,
     "status": "ok",
     "timestamp": 1741402263903,
     "user": {
      "displayName": "Deep Learning02",
      "userId": "04674597338778555609"
     },
     "user_tz": -330
    },
    "id": "m3KFUbl3pke9",
    "outputId": "a65b4c40-5138-4b69-e6b8-7955d582dfd5"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWcPTI1zpke-"
   },
   "source": [
    "### Check data types and number of non-null values for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 95,
     "status": "ok",
     "timestamp": 1741402263997,
     "user": {
      "displayName": "Deep Learning02",
      "userId": "04674597338778555609"
     },
     "user_tz": -330
    },
    "id": "6Mlybvw3pke-",
    "outputId": "096619a3-1fb0-49a4-be68-715f4b43e2ba"
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1741402264008,
     "user": {
      "displayName": "Deep Learning02",
      "userId": "04674597338778555609"
     },
     "user_tz": -330
    },
    "id": "_hekXjBVpke_",
    "outputId": "84fa52b0-29d2-43cd-b2bf-944986d5a584"
   },
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EhMHyyhnpke_"
   },
   "source": [
    "- There are no missing values in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mtdoj0-gpke_"
   },
   "source": [
    "### Summary of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 91,
     "status": "ok",
     "timestamp": 1741402264156,
     "user": {
      "displayName": "Deep Learning02",
      "userId": "04674597338778555609"
     },
     "user_tz": -330
    },
    "id": "p6LcCvEQpke_",
    "outputId": "61f5dc91-5cde-438a-f83a-f956ceb7e328"
   },
   "outputs": [],
   "source": [
    "# Summary of continuous columns\n",
    "data[['temp','atemp','hum','windspeed','cnt']].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LvFsGRBNpkfA"
   },
   "source": [
    "### Number of unique values in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "executionInfo": {
     "elapsed": 41,
     "status": "ok",
     "timestamp": 1741402264198,
     "user": {
      "displayName": "Deep Learning02",
      "userId": "04674597338778555609"
     },
     "user_tz": -330
    },
    "id": "z7JIp2ilpkfA",
    "outputId": "fd9bc556-a81e-4270-d7a7-ef7056285fdc"
   },
   "outputs": [],
   "source": [
    "data.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "svngQ-L6pkfB"
   },
   "source": [
    "- We can drop 'instant' column as it is an ID variable and will not add value to the model.\n",
    "- We can drop 'dteday' column as it just contains dates of 731 days i.e. 2 years. This will not add value to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1741402264235,
     "user": {
      "displayName": "Deep Learning02",
      "userId": "04674597338778555609"
     },
     "user_tz": -330
    },
    "id": "5IuGhIj6pkfB"
   },
   "outputs": [],
   "source": [
    "#Dropping two columns from the dataframe\n",
    "data.drop(columns=['instant','dteday'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLMM9tuXpkfB"
   },
   "source": [
    "### Number of observations in each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1741402264253,
     "user": {
      "displayName": "Deep Learning02",
      "userId": "04674597338778555609"
     },
     "user_tz": -330
    },
    "id": "cIWSusE2pkfB",
    "outputId": "74ff1e2a-3c77-4c88-fd45-1b867a6bdd91",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cat_cols=['season','yr','holiday','workingday','weathersit']\n",
    "\n",
    "for column in cat_cols:\n",
    "    print(data[column].value_counts())\n",
    "    print('-'*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1741402264254,
     "user": {
      "displayName": "Deep Learning02",
      "userId": "04674597338778555609"
     },
     "user_tz": -330
    },
    "id": "sKBGQ71uuZ17"
   },
   "outputs": [],
   "source": [
    "df = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oUTSkXOZpkfC"
   },
   "source": [
    "## <a name='link2'>Exploratory Data Analysis (EDA) Summary</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ptvrb3uIqXJC"
   },
   "source": [
    "### **Note**: The EDA section has been covered multiple times in the previous case studies. In this case study, we will mainly focus on the model building aspects. We will only be looking at the key observations from EDA. The detailed EDA can be found in the <a href = #link1>appendix section</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6AUxpCDgqduR"
   },
   "source": [
    "**The below functions need to be defined to carry out the EDA.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1741402264289,
     "user": {
      "displayName": "Deep Learning02",
      "userId": "04674597338778555609"
     },
     "user_tz": -330
    },
    "id": "QGb5rv3UpkfD"
   },
   "outputs": [],
   "source": [
    "def histogram_boxplot(data, feature, figsize=(15, 10), kde=False, bins=None):\n",
    "    \"\"\"\n",
    "    Boxplot and histogram combined\n",
    "\n",
    "    data: dataframe\n",
    "    feature: dataframe column\n",
    "    figsize: size of figure (default (15,10))\n",
    "    kde: whether to show the density curve (default False)\n",
    "    bins: number of bins for histogram (default None)\n",
    "    \"\"\"\n",
    "    f2, (ax_box2, ax_hist2) = plt.subplots(\n",
    "        nrows=2,  # Number of rows of the subplot grid= 2\n",
    "        sharex=True,  # x-axis will be shared among all subplots\n",
    "        gridspec_kw={\"height_ratios\": (0.25, 0.75)},\n",
    "        figsize=figsize,\n",
    "    )  # creating the 2 subplots\n",
    "    sns.boxplot(\n",
    "        data=data, x=feature, ax=ax_box2, showmeans=True, color=\"violet\"\n",
    "    )  # boxplot will be created and a triangle will indicate the mean value of the column\n",
    "    sns.histplot(\n",
    "        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins\n",
    "    ) if bins else sns.histplot(\n",
    "        data=data, x=feature, kde=kde, ax=ax_hist2\n",
    "    )  # For histogram\n",
    "    ax_hist2.axvline(\n",
    "        data[feature].mean(), color=\"green\", linestyle=\"--\"\n",
    "    )  # Add mean to the histogram\n",
    "    ax_hist2.axvline(\n",
    "        data[feature].median(), color=\"black\", linestyle=\"-\"\n",
    "    )  # Add median to the histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1741402264299,
     "user": {
      "displayName": "Deep Learning02",
      "userId": "04674597338778555609"
     },
     "user_tz": -330
    },
    "id": "wfxXQ1Ghq0GS"
   },
   "outputs": [],
   "source": [
    "# function to create labeled barplots\n",
    "\n",
    "\n",
    "def labeled_barplot(data, feature, perc=False, n=None):\n",
    "    \"\"\"\n",
    "    Barplot with percentage at the top\n",
    "\n",
    "    data: dataframe\n",
    "    feature: dataframe column\n",
    "    perc: whether to display percentages instead of count (default is False)\n",
    "    n: displays the top n category levels (default is None, i.e., display all levels)\n",
    "    \"\"\"\n",
    "\n",
    "    total = len(data[feature])  # length of the column\n",
    "    count = data[feature].nunique()\n",
    "    if n is None:\n",
    "        plt.figure(figsize=(count + 2, 6))\n",
    "    else:\n",
    "        plt.figure(figsize=(n + 2, 6))\n",
    "\n",
    "    plt.xticks(rotation=90, fontsize=15)\n",
    "    ax = sns.countplot(\n",
    "        data=data,\n",
    "        x=feature,\n",
    "        palette=\"Paired\",\n",
    "        order=data[feature].value_counts().index[:n],\n",
    "    )\n",
    "\n",
    "    for p in ax.patches:\n",
    "        if perc == True:\n",
    "            label = \"{:.1f}%\".format(\n",
    "                100 * p.get_height() / total\n",
    "            )  # percentage of each class of the category\n",
    "        else:\n",
    "            label = p.get_height()  # count of each level of the category\n",
    "\n",
    "        x = p.get_x() + p.get_width() / 2  # width of the plot\n",
    "        y = p.get_height()  # height of the plot\n",
    "\n",
    "        ax.annotate(\n",
    "            label,\n",
    "            (x, y),\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            size=12,\n",
    "            xytext=(0, 5),\n",
    "            textcoords=\"offset points\",\n",
    "        )  # annotate the percentage\n",
    "\n",
    "    plt.show()  # show the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CHr021zhpkfD"
   },
   "source": [
    "### Univariate analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 850
    },
    "executionInfo": {
     "elapsed": 142,
     "status": "ok",
     "timestamp": 1741402264441,
     "user": {
      "displayName": "Deep Learning02",
      "userId": "04674597338778555609"
     },
     "user_tz": -330
    },
    "id": "K7K1LiYmpkfF",
    "outputId": "5c3a45ea-8259-4f9e-f25c-67aa0f578b0d"
   },
   "outputs": [],
   "source": [
    "histogram_boxplot(data,'windspeed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 850
    },
    "executionInfo": {
     "elapsed": 309,
     "status": "ok",
     "timestamp": 1741402264751,
     "user": {
      "displayName": "Deep Learning02",
      "userId": "04674597338778555609"
     },
     "user_tz": -330
    },
    "id": "d3_FPOq8pkfG",
    "outputId": "85ab7e0f-7836-4d34-9893-e0909aede840"
   },
   "outputs": [],
   "source": [
    "histogram_boxplot(data,'cnt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 541
    },
    "executionInfo": {
     "elapsed": 298,
     "status": "ok",
     "timestamp": 1741402265051,
     "user": {
      "displayName": "Deep Learning02",
      "userId": "04674597338778555609"
     },
     "user_tz": -330
    },
    "id": "n-o0L-5ZpkfJ",
    "outputId": "7836c8ce-d608-4336-8e3f-db344b251009"
   },
   "outputs": [],
   "source": [
    "labeled_barplot(data,'weathersit',perc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9XHU0XvSpkfK"
   },
   "source": [
    "- Season 1 has the highest percentage of observations i.e. 65.7%\n",
    "- Season 2 and season 3 have 26.1% and 8.2% observations respectively\n",
    "- We saw earlier that season 4 has only 3 observations in the data. Here, it shows 0% observations due to rounding off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 541
    },
    "executionInfo": {
     "elapsed": 148,
     "status": "ok",
     "timestamp": 1741402265200,
     "user": {
      "displayName": "Deep Learning02",
      "userId": "04674597338778555609"
     },
     "user_tz": -330
    },
    "id": "SWwwCdW1pkfL",
    "outputId": "c05533ee-ebb2-4b30-daec-a990d3ed9a2a"
   },
   "outputs": [],
   "source": [
    "labeled_barplot(data,'workingday',perc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1uqod7LdpkfL"
   },
   "source": [
    "- As expected, the number of observations for working days is higher than the number of observations for non-working days.\n",
    "- There are approx 68% observations for working days and 32% observations for non-working days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "609z-Rg8pkfL"
   },
   "source": [
    "### Bivariate analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q_qz1RHPNEuu"
   },
   "source": [
    "**Correlation Check**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 909
    },
    "executionInfo": {
     "elapsed": 1221,
     "status": "ok",
     "timestamp": 1741402266420,
     "user": {
      "displayName": "Deep Learning02",
      "userId": "04674597338778555609"
     },
     "user_tz": -330
    },
    "id": "ycR8zmSZpkfO",
    "outputId": "f2d58269-b98b-48e6-ae32-5421b716a539"
   },
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(16,10)})\n",
    "sns.heatmap(data.corr(numeric_only = True),\n",
    "            annot=True,\n",
    "            linewidths=.5,\n",
    "            center=0,\n",
    "            cbar=False,\n",
    "            cmap=\"Spectral\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4EXvu1RPpkfO"
   },
   "source": [
    "- We can see that temperature and feel like temperature are almost perfectly correlated\n",
    "- Month and season have a high positive correlation among them\n",
    "- As count is the addition of two columns - Casual and registered. We can drop these two columns because if we have casual and registered count then making a model won't make sense as we can simply add them. We would not have these 2 columns while predicting new observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dGDXfharpkfL"
   },
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(21,7)})\n",
    "sns.catplot(x=\"season\", y=\"cnt\", kind=\"swarm\", data=data, height=7, aspect=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MhDKuBtGpkfL"
   },
   "source": [
    "- The lowest number of bikes are rented in the first season\n",
    "- The highest number of bikes are shared in 3rd season\n",
    "- This can be due to the relatively high temperature in season 1 i.e. spring as compared to season 3 i.e. fall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JJFRgYldpkfL"
   },
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(21,7)})\n",
    "sns.catplot(x=\"weekday\", y=\"cnt\", kind=\"swarm\", data=data, height=7, aspect=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dr-XnuLspkfM"
   },
   "source": [
    "- Weekends i.e. weekday=0 and weekday=6 have a low count of bikes rented and it is less varying.\n",
    "- Working days have a higher count of bikes rented and have more variation in the count and there are some outliers for days from 1 to 5.\n",
    "- This can be due to closed schools/offices on weekends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-gfrWRidpkfM"
   },
   "outputs": [],
   "source": [
    "sns.catplot(x=\"hr\", y=\"cnt\", data=data, kind='bar', height=7, aspect=1.5, estimator=np.mean);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vt0uGd8zpkfN"
   },
   "source": [
    "- We can see the average number of bikes rented is high at 8 AM and 5-6 PM, this can be due to office/school/college timings.\n",
    "- The average number of bikes rented is very low for night time i.e. 12 AM to 5 AM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7mV81yylpkfN"
   },
   "outputs": [],
   "source": [
    "sns.catplot(x=\"mnth\", y=\"cnt\", data=data, kind='bar', height=6, aspect=1.6, estimator=np.mean);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2oe_ZxljpkfN"
   },
   "source": [
    "- The average number of bikes rented is low for months - December, January, February. This can be due to the cold weather in these months.\n",
    "- The average number of bikes rented is consistently high for months from May to October."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I1L10TqtpkfN"
   },
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(15,7)})\n",
    "pd.pivot_table(data=data, index=['yr', 'season'], values='cnt', aggfunc=np.sum).plot(kind='bar');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yv_KfhU3pkfO"
   },
   "source": [
    "- We can see that number of bikes rented is higher in the year 2012 for each season as compared to seasons in 2011.\n",
    "- This shows that bike-sharing is becoming more popular with time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CXxXw3fwpkfO"
   },
   "outputs": [],
   "source": [
    "sns.catplot(x=\"weathersit\", y='cnt', kind='bar', data=data, aspect=1.25, estimator=np.mean);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_SlGt-dpkfO"
   },
   "source": [
    "- As expected, the average count of bikes rented is much higher on clear or partly cloudy days compared to snowy or rainy days.\n",
    "- This implies that the average count of bikes rented is hugely affected by the weather."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ND-xRlT0sO-Q"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_gXSeD7fsonV"
   },
   "source": [
    "### Outlier Detection and Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rvGYcLsOssof"
   },
   "outputs": [],
   "source": [
    "# outlier detection using boxplot\n",
    "numeric_columns = data.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 12))\n",
    "\n",
    "for i, variable in enumerate(numeric_columns):\n",
    "    plt.subplot(4, 4, i + 1)\n",
    "    plt.boxplot(data[variable], whis=1.5)\n",
    "    plt.tight_layout()\n",
    "    plt.title(variable)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KdNURskis1WE"
   },
   "source": [
    "- There are quite a few outliers in the data.\n",
    "- However, we will not treat them as they are proper values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3LgJ-hXsaUV"
   },
   "source": [
    "**Actions for data pre-processing:**\n",
    "\n",
    "* As count is the addition of two columns - Casual and registered, we can drop these two columns because if we have casual and registered count then making a model won't make sense as we can simply add them.\n",
    "\n",
    "* We would not have these 2 columns while predicting new observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X2V2GoDqpkfP"
   },
   "outputs": [],
   "source": [
    "#Dropping columns - casual and registered\n",
    "data.drop(columns=['casual','registered'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t1qLz_lipkfP"
   },
   "source": [
    "### Data Preparataion for model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uofTCtHmpkfP"
   },
   "outputs": [],
   "source": [
    "# Separating features and the target column\n",
    "X = data.drop('cnt', axis=1)\n",
    "y = data['cnt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rz2YEWX1pkfP"
   },
   "outputs": [],
   "source": [
    "# Splitting the data into train and test sets in 70:30 ratio\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f0KBUB3lpkfP"
   },
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HJ6JLMV6pkfP"
   },
   "source": [
    "- We have 12,165 observations in the train set and 5,214 observations in the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ltFoTTimpkfQ"
   },
   "source": [
    "## Bagging - Model Building and Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Z1JbEt2pkfQ"
   },
   "source": [
    "- We'll fit different models on the train data and observe their performance.\n",
    "- We'll try to improve that performance by tuning some hyperparameters available for that algorithm.\n",
    "- We'll use GridSearchCv for hyperparameter tuning and `r_2 score` to optimize the model.\n",
    "- R-square - `Coefficient of determination` is used to evaluate the performance of a regression model. It is the amount of the variation in the output dependent attribute which is predictable from the input independent variables.\n",
    "- Let's start by creating a function to get model scores, so that we don't have to use the same codes repeatedly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nn8LvuUwoIz7"
   },
   "outputs": [],
   "source": [
    "# function to compute adjusted R-squared\n",
    "def adj_r2_score(predictors, targets, predictions):\n",
    "    r2 = r2_score(targets, predictions)\n",
    "    n = predictors.shape[0]\n",
    "    k = predictors.shape[1]\n",
    "    return 1 - ((1 - r2) * (n - 1) / (n - k - 1))\n",
    "\n",
    "\n",
    "# function to compute MAPE\n",
    "def mape_score(targets, predictions):\n",
    "    return np.mean(np.abs(targets - predictions) / targets) * 100\n",
    "\n",
    "\n",
    "# function to compute different metrics to check performance of a regression model\n",
    "def model_performance_regression(model, predictors, target):\n",
    "    \"\"\"\n",
    "    Function to compute different metrics to check regression model performance\n",
    "\n",
    "    model: regressor\n",
    "    predictors: independent variables\n",
    "    target: dependent variable\n",
    "    \"\"\"\n",
    "\n",
    "    # predicting using the independent variables\n",
    "    pred = model.predict(predictors)\n",
    "\n",
    "    r2 = r2_score(target, pred)  # to compute R-squared\n",
    "    adjr2 = adj_r2_score(predictors, target, pred)  # to compute adjusted R-squared\n",
    "    rmse = np.sqrt(mean_squared_error(target, pred))  # to compute RMSE\n",
    "    mae = mean_absolute_error(target, pred)  # to compute MAE\n",
    "    mape = mape_score(target, pred)  # to compute MAPE\n",
    "\n",
    "    # creating a dataframe of metrics\n",
    "    df_perf = pd.DataFrame(\n",
    "        {\n",
    "            \"RMSE\": rmse,\n",
    "            \"MAE\": mae,\n",
    "            \"R-squared\": r2,\n",
    "            \"Adj. R-squared\": adjr2,\n",
    "            \"MAPE\": mape,\n",
    "        },\n",
    "        index=[0],\n",
    "    )\n",
    "\n",
    "    return df_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w_wwpxaepkfQ"
   },
   "outputs": [],
   "source": [
    "##  Function to calculate r2_score and RMSE on train and test data\n",
    "def get_model_score(model, flag=True):\n",
    "    '''\n",
    "    model : classifier to predict values of X\n",
    "\n",
    "    '''\n",
    "    # defining an empty list to store train and test results\n",
    "    score_list=[]\n",
    "\n",
    "    pred_train = model.predict(X_train)\n",
    "    pred_test = model.predict(X_test)\n",
    "\n",
    "    train_r2=metrics.r2_score(y_train,pred_train)\n",
    "    test_r2=metrics.r2_score(y_test,pred_test)\n",
    "    train_rmse=np.sqrt(metrics.mean_squared_error(y_train,pred_train))\n",
    "    test_rmse=np.sqrt(metrics.mean_squared_error(y_test,pred_test))\n",
    "\n",
    "    #Adding all scores in the list\n",
    "    score_list.extend((train_r2,test_r2,train_rmse,test_rmse))\n",
    "\n",
    "    # If the flag is set to True then only the following print statements will be dispayed, the default value is True\n",
    "    if flag==True:\n",
    "        print(\"R-sqaure on training set : \",metrics.r2_score(y_train,pred_train))\n",
    "        print(\"R-square on test set : \",metrics.r2_score(y_test,pred_test))\n",
    "        print(\"RMSE on training set : \",np.sqrt(metrics.mean_squared_error(y_train,pred_train)))\n",
    "        print(\"RMSE on test set : \",np.sqrt(metrics.mean_squared_error(y_test,pred_test)))\n",
    "\n",
    "    # returning the list with train and test scores\n",
    "    return score_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lXLBfLUGpkfQ"
   },
   "source": [
    "### Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wO23Fi_IpkfQ"
   },
   "outputs": [],
   "source": [
    "dtree=DecisionTreeRegressor(random_state=1)\n",
    "dtree.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m5T-4vIcoIz7"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gj1yRPLjoIz8"
   },
   "outputs": [],
   "source": [
    "dtree_model_train_perf=model_performance_regression(dtree, X_train,y_train)\n",
    "print(\"Training performance \\n\",dtree_model_train_perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HLePcbMooIz8"
   },
   "outputs": [],
   "source": [
    "dtree_model_test_perf=model_performance_regression(dtree, X_test,y_test)\n",
    "print(\"Testing performance \\n\",dtree_model_test_perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y3Nbu3A2pkfR"
   },
   "source": [
    "- The Decision tree model with default parameters is overfitting the train data.\n",
    "- Let's see if we can reduce overfitting and improve performance on test data by tuning hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ra1li3uypkfR"
   },
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hwq0FSNOpkfR"
   },
   "outputs": [],
   "source": [
    "# Choose the type of classifier.\n",
    "dtree_tuned = DecisionTreeRegressor(random_state=1)\n",
    "\n",
    "# Grid of parameters to choose from\n",
    "parameters = {'max_depth': list(np.arange(2,20)) + [None],\n",
    "              'min_samples_leaf': [1, 3, 5, 7, 10],\n",
    "              'max_leaf_nodes' : [2, 3, 5, 10, 15] + [None],\n",
    "              'min_impurity_decrease': [0.001, 0.01, 0.1, 0.0]\n",
    "             }\n",
    "\n",
    "# Type of scoring used to compare parameter combinations\n",
    "scorer = metrics.make_scorer(metrics.r2_score)\n",
    "\n",
    "# Run the grid search\n",
    "grid_obj = GridSearchCV(dtree_tuned, parameters, scoring=scorer,cv=5)\n",
    "grid_obj = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Set the clf to the best combination of parameters\n",
    "dtree_tuned = grid_obj.best_estimator_\n",
    "\n",
    "# Fit the best algorithm to the data.\n",
    "dtree_tuned.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ht8bldcgoIz8"
   },
   "outputs": [],
   "source": [
    "dtree_tuned_model_train_perf = model_performance_regression(dtree_tuned, X_train,y_train)\n",
    "print(\"Training performance \\n\",dtree_tuned_model_train_perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0J8sU6SXoIz8"
   },
   "outputs": [],
   "source": [
    "dtree_tuned_model_test_perf = model_performance_regression(dtree_tuned, X_test,y_test)\n",
    "print(\"Testing performance \\n\",dtree_tuned_model_test_perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sg9gEsJipkfS"
   },
   "source": [
    "- The overfitting is reduced after hyperparameter tuning and the test score has increased by approx 2%.\n",
    "- RMSE is also reduced on test data and the model is generalizing better than the decision tree model with default parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z5jWUsKGpkfS"
   },
   "source": [
    "**Plotting the feature importance of each variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EVStVqOopkfS"
   },
   "outputs": [],
   "source": [
    "# importance of features in the tree building ( The importance of a feature is computed as the\n",
    "#(normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance )\n",
    "\n",
    "print(pd.DataFrame(dtree_tuned.feature_importances_, columns = [\"Imp\"], index = X_train.columns).sort_values(by = 'Imp', ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mE4t1dbMpkfS"
   },
   "outputs": [],
   "source": [
    "feature_names = X_train.columns\n",
    "importances = dtree_tuned.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), importances[indices], color='violet', align='center')\n",
    "plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7CaEHg0pkfS"
   },
   "source": [
    "- hr is the most important feature, in addition to temp and yr, for tuned decision tree model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4cjAcN7LpkfS"
   },
   "source": [
    "### Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pS9Fmr1HpkfT"
   },
   "outputs": [],
   "source": [
    "rf_estimator=RandomForestRegressor(random_state=1)\n",
    "rf_estimator.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-rYQT2cGoIz9"
   },
   "outputs": [],
   "source": [
    "rf_estimator_model_train_perf = model_performance_regression(rf_estimator, X_train,y_train)\n",
    "print(\"Training performance \\n\",rf_estimator_model_train_perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WawrAjO1oIz9"
   },
   "outputs": [],
   "source": [
    "rf_estimator_model_test_perf = model_performance_regression(rf_estimator, X_test,y_test)\n",
    "print(\"Testing performance \\n\",rf_estimator_model_test_perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eGWyxWlHpkfT"
   },
   "source": [
    "- Random forest is giving a good r2 score of 94% on the test data but it is slightly overfitting the train data.\n",
    "- Let's try to reduce this overfitting by hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o8GgdCFTpkfT"
   },
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_iz2v842pkfT"
   },
   "outputs": [],
   "source": [
    "# Choose the type of classifier.\n",
    "rf_tuned = RandomForestRegressor(random_state=1)\n",
    "\n",
    "# Grid of parameters to choose from\n",
    "parameters = {\n",
    "                'max_depth':[4, 6, 8, 10, None],\n",
    "                'max_features': ['sqrt','log2',None],\n",
    "                'n_estimators': [80, 90, 100, 110, 120]\n",
    "}\n",
    "\n",
    "# Type of scoring used to compare parameter combinations\n",
    "scorer = metrics.make_scorer(metrics.r2_score)\n",
    "\n",
    "# Run the grid search\n",
    "grid_obj = GridSearchCV(rf_tuned, parameters, scoring=scorer,cv=5)\n",
    "grid_obj = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Set the clf to the best combination of parameters\n",
    "rf_tuned = grid_obj.best_estimator_\n",
    "\n",
    "# Fit the best algorithm to the data.\n",
    "rf_tuned.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cCJoQO0goIz-"
   },
   "outputs": [],
   "source": [
    "rf_tuned_model_train_perf = model_performance_regression(rf_tuned, X_train, y_train)\n",
    "print(\"Training performance \\n\",rf_tuned_model_train_perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QGcEPg3FoIz-"
   },
   "outputs": [],
   "source": [
    "rf_tuned_model_test_perf = model_performance_regression(rf_tuned, X_test, y_test)\n",
    "print(\"Testing performance \\n\",rf_tuned_model_test_perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bh9GVZccpkfU"
   },
   "source": [
    "- No significant change in the result. The result is almost the same before or after the hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F2xJ6bd8pkfU"
   },
   "outputs": [],
   "source": [
    "# importance of features in the tree building ( The importance of a feature is computed as the\n",
    "#(normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance )\n",
    "\n",
    "print(pd.DataFrame(rf_tuned.feature_importances_, columns = [\"Imp\"], index = X_train.columns).sort_values(by = 'Imp', ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EomBVffGpkfU"
   },
   "outputs": [],
   "source": [
    "feature_names = X_train.columns\n",
    "importances = rf_tuned.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), importances[indices], color='violet', align='center')\n",
    "plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7_mP72xYpkfU"
   },
   "source": [
    "- hr is the most important feature, in addition to temp and yr, for the tuned random forest model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_M7GLyjOpkfV"
   },
   "source": [
    "## Boosting - Model Building and Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZw_di98pkfV"
   },
   "source": [
    "### AdaBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YElA_stqpkfV"
   },
   "outputs": [],
   "source": [
    "ab_regressor=AdaBoostRegressor(random_state=1)\n",
    "ab_regressor.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jhbOYWnsoIz-"
   },
   "outputs": [],
   "source": [
    "ab_regressor_model_train_perf = model_performance_regression(ab_regressor, X_train,y_train)\n",
    "print(\"Training performance \\n\",ab_regressor_model_train_perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "60D9nk-WoIz_"
   },
   "outputs": [],
   "source": [
    "ab_regressor_model_test_perf = model_performance_regression(ab_regressor, X_test,y_test)\n",
    "print(\"Testing performance \\n\",ab_regressor_model_test_perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8NWDbMSTpkfV"
   },
   "source": [
    "- AdaBoost is generalizing well but it is giving poor performance, in terms of r2 score as well as RMSE, as compared to the decision tree and random forest model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SlRV-tn-pkfW"
   },
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aDq6BJx0pkfW"
   },
   "outputs": [],
   "source": [
    "# Choose the type of classifier.\n",
    "ab_tuned = AdaBoostRegressor(random_state=1)\n",
    "\n",
    "# Grid of parameters to choose from\n",
    "parameters = {'n_estimators': np.arange(10,100,10),\n",
    "              'learning_rate': [1, 0.1, 0.5, 0.01],\n",
    "              }\n",
    "\n",
    "# Type of scoring used to compare parameter combinations\n",
    "scorer = metrics.make_scorer(metrics.r2_score)\n",
    "\n",
    "# Run the grid search\n",
    "grid_obj = GridSearchCV(ab_tuned, parameters, scoring=scorer,cv=5)\n",
    "grid_obj = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Set the clf to the best combination of parameters\n",
    "ab_tuned = grid_obj.best_estimator_\n",
    "\n",
    "# Fit the best algorithm to the data.\n",
    "ab_tuned.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0PotV8tqpkfW"
   },
   "outputs": [],
   "source": [
    "ab_tuned_model_train_perf = model_performance_regression(ab_tuned, X_train,y_train)\n",
    "print(\"Training performance \\n\",ab_tuned_model_train_perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WvZ8HnGcoIz_"
   },
   "outputs": [],
   "source": [
    "ab_tuned_model_test_perf = model_performance_regression(ab_tuned, X_test,y_test)\n",
    "print(\"Testing performance \\n\",ab_tuned_model_test_perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTTIJ9RopkfW"
   },
   "source": [
    "- We can see that there is no significant improvement in the model after hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iblPSSCYpkfW"
   },
   "outputs": [],
   "source": [
    "# importance of features in the tree building\n",
    "\n",
    "print(pd.DataFrame(ab_tuned.feature_importances_, columns = [\"Imp\"], index = X_train.columns).sort_values(by = 'Imp', ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fEKO-LBWpkfX"
   },
   "outputs": [],
   "source": [
    "feature_names = X_train.columns\n",
    "importances = ab_tuned.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), importances[indices], color='violet', align='center')\n",
    "plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OvuElLZ4pkfX"
   },
   "source": [
    "- hr is the most important feature here, followed by yr and atemp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J0LI-9gKpkfX"
   },
   "source": [
    "### Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c3QIzKLLpkfX"
   },
   "outputs": [],
   "source": [
    "gb_estimator=GradientBoostingRegressor(random_state=1)\n",
    "gb_estimator.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AuWaEz2gpkfX"
   },
   "outputs": [],
   "source": [
    "gb_estimator_model_train_perf = model_performance_regression(gb_estimator, X_train,y_train)\n",
    "print(\"Training performance \\n\",gb_estimator_model_train_perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "norfIy5voI0A"
   },
   "outputs": [],
   "source": [
    "gb_estimator_model_test_perf = model_performance_regression(gb_estimator, X_test, y_test)\n",
    "print(\"Testing performance \\n\",gb_estimator_model_test_perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aojwgIafpkfY"
   },
   "source": [
    "- Gradient boosting is generalizing well and giving decent results but not as good as random forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AbxL7YlzpkfY"
   },
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ENuCKyhpkfY"
   },
   "outputs": [],
   "source": [
    "# Choose the type of classifier.\n",
    "gb_tuned = GradientBoostingRegressor(random_state=1)\n",
    "\n",
    "# Grid of parameters to choose from\n",
    "parameters = {'n_estimators': np.arange(50,200,25),\n",
    "              'subsample':[0.7,0.8,0.9,1],\n",
    "              'max_features':[0.7,0.8,0.9,1],\n",
    "              'max_depth':[3,5,7,10]\n",
    "              }\n",
    "\n",
    "# Type of scoring used to compare parameter combinations\n",
    "scorer = metrics.make_scorer(metrics.r2_score)\n",
    "\n",
    "# Run the grid search\n",
    "grid_obj = GridSearchCV(gb_tuned, parameters, scoring=scorer,cv=5)\n",
    "grid_obj = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Set the clf to the best combination of parameters\n",
    "gb_tuned = grid_obj.best_estimator_\n",
    "\n",
    "# Fit the best algorithm to the data.\n",
    "gb_tuned.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p83J_Co4oI0B"
   },
   "outputs": [],
   "source": [
    "gb_tuned_model_train_perf = model_performance_regression(gb_tuned, X_train,y_train)\n",
    "print(\"Training performance \\n\",gb_tuned_model_train_perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v2NDXor8oI0B"
   },
   "outputs": [],
   "source": [
    "gb_tuned_model_test_perf = model_performance_regression(gb_tuned, X_test, y_test)\n",
    "print(\"Testing performance \\n\",gb_tuned_model_test_perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_mwD6pvppkfY"
   },
   "source": [
    "- We can see that the model has improved significantly in terms of r2 score and RMSE.\n",
    "- The r2 score has increased by approx 12% on the test data.\n",
    "- RMSE has decreased by more than 30 for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LOp3mewWpkfZ"
   },
   "outputs": [],
   "source": [
    "# importance of features in the tree building ( The importance of a feature is computed as the\n",
    "#(normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance )\n",
    "\n",
    "print(pd.DataFrame(gb_tuned.feature_importances_, columns = [\"Imp\"], index = X_train.columns).sort_values(by = 'Imp', ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CmH17bcVpkfZ"
   },
   "outputs": [],
   "source": [
    "feature_names = X_train.columns\n",
    "importances = gb_tuned.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), importances[indices], color='violet', align='center')\n",
    "plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7UFL7ZFpkfZ"
   },
   "source": [
    "- hr is the most important feature\n",
    "- temp, yr and workingday have almost equal importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4lhC3dUipkfZ"
   },
   "source": [
    "### XGBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I1gg-C36pkfZ"
   },
   "outputs": [],
   "source": [
    "xgb_estimator=XGBRegressor(random_state=1, verbosity = 0)\n",
    "xgb_estimator.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "juO-fkesoI0B"
   },
   "outputs": [],
   "source": [
    "xgb_estimator_model_train_perf = model_performance_regression(xgb_estimator, X_train, y_train)\n",
    "print(\"Training performance \\n\",xgb_estimator_model_train_perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yDkXRLCmoI0B"
   },
   "outputs": [],
   "source": [
    "xgb_estimator_model_test_perf = model_performance_regression(xgb_estimator, X_test,y_test)\n",
    "print(\"Testing performance \\n\",xgb_estimator_model_test_perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zZfl3Numpkfa"
   },
   "source": [
    "- XGBoost with default parameters is giving almost as good results as the tuned gradient boosting model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cnXZB3wLpkfa"
   },
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g0OwMXYNpkfb"
   },
   "outputs": [],
   "source": [
    "# Choose the type of classifier.\n",
    "xgb_tuned = XGBRegressor(random_state=1, verbosity = 0)\n",
    "\n",
    "# Grid of parameters to choose from\n",
    "parameters = {'n_estimators': [75,100,125,150],\n",
    "              'subsample':[0.7, 0.8, 0.9, 1],\n",
    "              'gamma':[0, 1, 3, 5],\n",
    "              'colsample_bytree':[0.7, 0.8, 0.9, 1],\n",
    "              'colsample_bylevel':[0.7, 0.8, 0.9, 1]\n",
    "              }\n",
    "\n",
    "# Type of scoring used to compare parameter combinations\n",
    "scorer = metrics.make_scorer(metrics.r2_score)\n",
    "\n",
    "# Run the grid search\n",
    "grid_obj = GridSearchCV(xgb_tuned, parameters, scoring=scorer,cv=5)\n",
    "grid_obj = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Set the clf to the best combination of parameters\n",
    "xgb_tuned = grid_obj.best_estimator_\n",
    "\n",
    "# Fit the best algorithm to the data.\n",
    "xgb_tuned.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DpPiuli-oI0C"
   },
   "outputs": [],
   "source": [
    "xgb_tuned_model_train_perf = model_performance_regression(xgb_tuned, X_train, y_train)\n",
    "print(\"Training performance \\n\",xgb_tuned_model_train_perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EGgqIEtloI0C"
   },
   "outputs": [],
   "source": [
    "xgb_tuned_model_test_perf = model_performance_regression(xgb_tuned, X_test, y_test)\n",
    "print(\"Testing performance \\n\",xgb_tuned_model_test_perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dIT7S5_-pkfb"
   },
   "outputs": [],
   "source": [
    "# importance of features in the tree building ( The importance of a feature is computed as the\n",
    "#(normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance )\n",
    "\n",
    "print(pd.DataFrame(xgb_tuned.feature_importances_, columns = [\"Imp\"], index = X_train.columns).sort_values(by = 'Imp', ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gxtW9F6vpkfb"
   },
   "outputs": [],
   "source": [
    "feature_names = X_train.columns\n",
    "importances = xgb_tuned.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), importances[indices], color='violet', align='center')\n",
    "plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J5iNarCMpkfc"
   },
   "source": [
    "- In XGBoost, workingday is the most important feature followed by features - hr and yr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0_kkgb5Opkfc"
   },
   "source": [
    "## Stacking Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z7g7Uo6Spkfc"
   },
   "source": [
    "**Now, let's build a stacking model with the tuned models - decision tree, random forest, and gradient boosting, then use XGBoost to get the final prediction.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DcePq0DBpkfc"
   },
   "outputs": [],
   "source": [
    "estimators=[('Decision Tree', dtree_tuned),('Random Forest', rf_tuned),\n",
    "           ('Gradient Boosting', gb_tuned)]\n",
    "final_estimator=XGBRegressor(random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FZ6YP0ktpkfc"
   },
   "outputs": [],
   "source": [
    "stacking_estimator=StackingRegressor(estimators=estimators, final_estimator=final_estimator,cv=5)\n",
    "stacking_estimator.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MEZIFLIKoI0D"
   },
   "outputs": [],
   "source": [
    "stacking_estimator_model_train_perf = model_performance_regression(stacking_estimator, X_train, y_train)\n",
    "print(\"Training performance \\n\",stacking_estimator_model_train_perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jXuhedc3oI0D"
   },
   "outputs": [],
   "source": [
    "stacking_estimator_model_test_perf = model_performance_regression(stacking_estimator, X_test, y_test)\n",
    "print(\"Testing performance \\n\",stacking_estimator_model_test_perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bzcOIbGWpkfd"
   },
   "source": [
    "## Comparing all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bfE_FHknoI0E"
   },
   "outputs": [],
   "source": [
    "# training performance comparison\n",
    "\n",
    "models_train_comp_df = pd.concat(\n",
    "    [dtree_model_train_perf.T, dtree_tuned_model_train_perf.T, rf_estimator_model_train_perf.T,rf_tuned_model_train_perf.T,\n",
    "    ab_regressor_model_train_perf.T,ab_tuned_model_train_perf.T,gb_estimator_model_train_perf.T,gb_tuned_model_train_perf.T,\n",
    "    xgb_estimator_model_train_perf.T,xgb_tuned_model_train_perf.T,stacking_estimator_model_train_perf.T],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "models_train_comp_df.columns = [\n",
    "    \"Decision Tree\",\n",
    "    \"Decision Tree Tuned\",\n",
    "    \"Random Forest Estimator\",\n",
    "    \"Random Forest Tuned\",\n",
    "    \"Adaboost Regressor\",\n",
    "    \"Adaboost Tuned\",\n",
    "    \"Gradient Boost Estimator\",\n",
    "    \"Gradient Boost Tuned\",\n",
    "    \"XGB\",\n",
    "    \"XGB Tuned\",\n",
    "    \"Stacking Classifier\"\n",
    "]\n",
    "\n",
    "print(\"Training performance comparison:\")\n",
    "models_train_comp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zpyJoHUPoI0E"
   },
   "outputs": [],
   "source": [
    "# Testing performance comparison\n",
    "\n",
    "models_test_comp_df = pd.concat(\n",
    "    [dtree_model_test_perf.T, dtree_tuned_model_test_perf.T, rf_estimator_model_test_perf.T,rf_tuned_model_test_perf.T,\n",
    "    ab_regressor_model_test_perf.T,ab_tuned_model_test_perf.T,gb_estimator_model_test_perf.T,gb_tuned_model_test_perf.T,\n",
    "    xgb_estimator_model_test_perf.T,xgb_tuned_model_test_perf.T,stacking_estimator_model_test_perf.T],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "models_test_comp_df.columns = [\n",
    "    \"Decision Tree\",\n",
    "    \"Decision Tree Tuned\",\n",
    "    \"Random Forest Estimator\",\n",
    "    \"Random Forest Tuned\",\n",
    "    \"Adaboost Regressor\",\n",
    "    \"Adaboost Tuned\",\n",
    "    \"Gradient Boost Estimator\",\n",
    "    \"Gradient Boost Tuned\",\n",
    "    \"XGB\",\n",
    "    \"XGB Tuned\",\n",
    "    \"Stacking Classifier\"\n",
    "]\n",
    "\n",
    "print(\"Testing performance comparison:\")\n",
    "models_test_comp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yv03CGidpkfe"
   },
   "source": [
    "- The tuned gradient boosting model is the best model here. It has the highest r2 score of approx 95.5% and the lowest RMSE of approx 39 on the test data.\n",
    "- Gradient boosting, XGBoost, and stacking regressor are the top 3 models. They are all giving a similar performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4xtVzthLpkfe"
   },
   "outputs": [],
   "source": [
    "# So plot observed and predicted values of the test data for the best model i.e. tuned gradient boosting model\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "y_pred=gb_tuned.predict(X_test)\n",
    "ax.scatter(y_test, y_pred, edgecolors=(0, 0, 1))\n",
    "ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=3)\n",
    "ax.set_xlabel('Observed')\n",
    "ax.set_ylabel('Predicted')\n",
    "ax.set_title(\"Observed vs Predicted\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LpXoQM5Tpkfe"
   },
   "source": [
    "- We can see that points are dense on the line where predicted is equal to the observed.\n",
    "- This implies that most of the predicted values are close to the true values with some exceptions as seen in the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WcNnZbs3pkfe"
   },
   "source": [
    "## Conclusions and Business Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gz_eDclppkff"
   },
   "source": [
    "- We can use this predictive model for any season and environmental parameters (which we know in advance) and can predict the count of the bikes to be rented. The ability to predict the number of hourly users can allow the entities (businesses/governments) that oversee these systems to manage them more efficiently and cost-effectively.\n",
    "- More bikes can be made available for the fall and winter seasons as the number of bikes rented is high in these seasons.\n",
    "- As the number of bikes rented is high for day timings compared to night timings, similarly, fall and winter seasons have more surges compared to other seasons. We can choose differential prices of bikes accordingly.\n",
    "- As most of the rentals are for commuting to workplaces and colleges daily, company can launch more stations near busy workplaces or schools/colleges to reach out to their main customers.\n",
    "- Number of bikes rented is heavily dependent on the weather. So, we should adjust the number of available bikes in an area based on the weather forecast.\n",
    "- Maintenance activities for bikes can be done at night due to low usage of bikes during the nighttime.\n",
    "- Company can provide offers or coupons like a monthly subscription to compensate for the low count on holidays or weekends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQawen1puuTC"
   },
   "source": [
    "## <a name='link1'>Appendix: Detailed Exploratory Data Analysis (EDA)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L3b1KDhjuno4"
   },
   "source": [
    "### Univariate analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ppl1iQViuno5"
   },
   "source": [
    "#### Observations on temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O9cQBWRAuno5"
   },
   "outputs": [],
   "source": [
    "histogram_boxplot(df, \"temp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JF5SoLJYuno5"
   },
   "source": [
    "- The temperature has an approx symmetric distribution with mean and median equal to 0.5\n",
    "- As evident from the boxplot, there are no outliers in the distribution for this variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8d98mQhuno5"
   },
   "source": [
    "#### Observations on 'feel like temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8VFN3Q9nuno5"
   },
   "outputs": [],
   "source": [
    "histogram_boxplot(df, \"atemp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dWuWMBdhuno5"
   },
   "source": [
    "- Same as temperature, the distribution for feel like the temperature is also symmetrically distributed\n",
    "- There are no outliers in the distribution of these variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2VCE_UfZuno5"
   },
   "outputs": [],
   "source": [
    "histogram_boxplot(df,\"hum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pnthSFObuno5"
   },
   "source": [
    "- Most of the values are concentrated in the middle i.e. 0.4 to 0.8\n",
    "- Humidity with the value equal to 0 is an outlier\n",
    "- The distribution is approx normally distributed with mean and median equal to 0.63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-j7F6QuKuno5"
   },
   "outputs": [],
   "source": [
    "histogram_boxplot(df,'windspeed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLcc6MPJuno5"
   },
   "source": [
    "- Wind speed has a right-skewed distribution and 0 has the highest count among all observations\n",
    "- Distribution is not symmetric but mean and median are approx equal with a value equal to 0.19\n",
    "- There are many outliers in this variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x5dzB7Jauno6"
   },
   "outputs": [],
   "source": [
    "histogram_boxplot(df,'cnt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "99QIgW5-uno6"
   },
   "outputs": [],
   "source": [
    "#Top 5 highest values\n",
    "df['cnt'].nlargest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-1xly4fuuno6"
   },
   "source": [
    "- The target variable i.e. the count of bikes rented has a right-skewed distribution\n",
    "- The range of values is very large with many observations being less than 10 counts and some being greater than 900 count\n",
    "- As evident from the boxplot, there are many outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g_QKU3VBuno6"
   },
   "outputs": [],
   "source": [
    "labeled_barplot(df, \"hr\",perc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k3bnAoKeuno6"
   },
   "source": [
    "- Each hour i.e. 0 to 23 has approx 4% observations in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_W2NeHO4uno6"
   },
   "outputs": [],
   "source": [
    "labeled_barplot(df,'mnth',perc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R55_DgFIuno6"
   },
   "source": [
    "- Each month i.e. 1 to 12 has approx 8.5% observations in the data\n",
    "- Month 2 has slightly less number of observations compared to other months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g3m4QLbfuno6"
   },
   "outputs": [],
   "source": [
    "labeled_barplot(df,'season',perc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ml4jM4pouno6"
   },
   "source": [
    "- Each season has approx 24% observations in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bP-4If5-uno6"
   },
   "outputs": [],
   "source": [
    "labeled_barplot(df,'yr',perc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ncv9QTZuno6"
   },
   "source": [
    "- Both years have approx equal number of observations in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G5HbLS5uuno6"
   },
   "outputs": [],
   "source": [
    "labeled_barplot(df,'weathersit',perc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EEXJUbbnuno6"
   },
   "source": [
    "- Season 1 has the highest percentage of observations i.e. 65.7%\n",
    "- Season 2 and season 3 have 26.1% and 8.2% observations respectively\n",
    "- We saw earlier that season 4 has only 3 observations in the data. Here, it shows 0% observations due to rounding off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V9OyysFzuno7"
   },
   "outputs": [],
   "source": [
    "labeled_barplot(df,'holiday',perc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zDNWVOk0uno7"
   },
   "source": [
    "- As expected the percentage for non-holidays is much more than holidays.\n",
    "- There are 97% non-holidays observations and only 3% for holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0NAnCx_6uno7"
   },
   "outputs": [],
   "source": [
    "labeled_barplot(df,'weekday',perc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_CMAY0ZQuno7"
   },
   "source": [
    "- Each weekday i.e. 0 to 6 has approx 14% observations in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MymXnIabuno7"
   },
   "outputs": [],
   "source": [
    "labeled_barplot(df,'workingday',perc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I2sqtZqTuno7"
   },
   "source": [
    "- As expected, the number of observations for working days is higher than the number of observations for non-working days.\n",
    "- There are approx 68% observations for working days and 32% observations for non-working days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O0P2rJDMuno7"
   },
   "source": [
    "### Bivariate analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ljaYdgQ_uno7"
   },
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(21,7)})\n",
    "sns.catplot(x=\"season\", y=\"cnt\", kind=\"swarm\", data=df, height=7, aspect=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BWsli1-Kuno7"
   },
   "source": [
    "- The lowest number of bikes are rented in the first season\n",
    "- The highest number of bikes are shared in 3rd season\n",
    "- This can be due to the relatively high temperature in season 1 i.e. spring as compared to season 3 i.e. fall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HJ8Q_eKluno7"
   },
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(21,7)})\n",
    "sns.catplot(x=\"weekday\", y=\"cnt\", kind=\"swarm\", data=df, height=7, aspect=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PE1fs0V4uno7"
   },
   "source": [
    "- Weekends i.e. weekday=0 and weekday=6 have a low count of bikes rented and it is less varying.\n",
    "- Working days have a higher count of bikes rented and have more variation in the count and there are some outliers for days from 1 to 5.\n",
    "- This can be due to closed schools/offices on weekends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-qR7VYsCuno7"
   },
   "outputs": [],
   "source": [
    "sns.pairplot(\n",
    "    df,\n",
    "    x_vars=[\"windspeed\", \"temp\", \"hum\"],\n",
    "    y_vars=[\"cnt\"],\n",
    "    height=4,\n",
    "    aspect=1\n",
    "    );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DDjz7qOJuno7"
   },
   "source": [
    "- We can see that count of bikes rented is low when the temperature is very low or very high. The same is true for humidity.\n",
    "- Count of bikes rented is high when wind speed is low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9EBu8JZEuno7"
   },
   "outputs": [],
   "source": [
    "sns.catplot(x=\"hr\", y=\"cnt\", data=df, kind='bar', height=7, aspect=1.5, estimator=np.mean);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yhpIejOuuno8"
   },
   "source": [
    "- We can see the average number of bikes rented is high at 8 AM and 5-6 PM, this can be due to office/school/college timings.\n",
    "- The average number of bikes rented is very low for night time i.e. 12 AM to 5 AM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0_fHEbg6uno8"
   },
   "outputs": [],
   "source": [
    "sns.catplot(x=\"mnth\", y=\"cnt\", data=df, kind='bar', height=6, aspect=1.6, estimator=np.mean);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PYAHuHbRuno8"
   },
   "source": [
    "- The average number of bikes rented is low for months - December, January, February. This can be due to the cold weather in these months.\n",
    "- The average number of bikes rented is consistently high for months from May to October."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MPYSScppuno8"
   },
   "outputs": [],
   "source": [
    "sns.catplot(x=\"yr\", y=\"cnt\", data=df, kind='bar', height=6, aspect=1.5, estimator=np.mean);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yfYkn8FNuno8"
   },
   "source": [
    "- The average count of bikes rented is high for the year 2012 as compared to 2011.\n",
    "- Let's check this for each season of both years and observe if the count in each season has increased or in just 1 or 2 seasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ox1hoZf7uno8"
   },
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(15,7)})\n",
    "pd.pivot_table(data=df, index=['yr', 'season'], values='cnt', aggfunc=np.sum).plot(kind='bar');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ChxMM1-Auno8"
   },
   "source": [
    "- We can see that number of bikes rented is higher in the year 2012 for each season as compared to seasons in 2011.\n",
    "- This shows that bike-sharing is becoming more popular with time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KUEKaJxhuno8"
   },
   "outputs": [],
   "source": [
    "sns.catplot(x=\"weathersit\", y='cnt', kind='bar', data=df, height=6, aspect=1.25, estimator=np.mean);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xrSB8wlcuno8"
   },
   "source": [
    "- As expected, the average count of bikes rented is much higher on clear or partly cloudy days compared to snowy or rainy days.\n",
    "- This implies that the average count of bikes rented is hugely affected by the weather."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IMlU93qMuno8"
   },
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(16,10)})\n",
    "sns.heatmap(df.corr(numeric_only = True),\n",
    "            annot=True,\n",
    "            linewidths=.5,\n",
    "            center=0,\n",
    "            cbar=False,\n",
    "            cmap=\"Spectral\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vQ0Yl48ouno8"
   },
   "source": [
    "- We can see that temperature and feel like temperature are almost perfectly correlated\n",
    "- Month and season have a high positive correlation among them\n",
    "- As count is the addition of two columns - Casual and registered. We can drop these two columns because if we have casual and registered count then making a model won't make sense as we can simply add them. We would not have these 2 columns while predicting new observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b7z7vXRyuno9"
   },
   "source": [
    "### To jump back to the EDA summary section, click <a href = #link2>here</a>."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "L3b1KDhjuno4",
    "ppl1iQViuno5",
    "T8d98mQhuno5",
    "O0P2rJDMuno7"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
