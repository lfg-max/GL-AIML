{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://upload.wikimedia.org/wikipedia/commons/e/e9/4_RGB_McCombs_School_Brand_Branded.png\" width=\"300\" height=\"100\"/>\n",
    "  <img src=\"https://mma.prnewswire.com/media/1458111/Great_Learning_Logo.jpg?p=facebook\" width=\"200\" height=\"100\"/></center>\n",
    "\n",
    "<center><font size=10>Artificial Intelligence and Machine Learning</center></font>\n",
    "<center><font size=6>Natural Language Processing with Generative AI - Retrieval Augmented Generation</center></font>\n",
    "\n",
    "<center><img src=\"https://i.ibb.co/pBF9nKpf/apple.png\" width=\"720\"></center>\n",
    "\n",
    "<center><font size=6>Apple HBR Report Document Q&A</center></font>\n",
    "\n",
    "# RAG LLM Application Notebook\n",
    "\n",
    "This notebook demonstrates the complete workflow of a Retrieval-Augmented Generation (RAG) LLM application. It covers data loading, chunking, embedding, vector database setup, question answering, and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Library Installation\n",
    "\n",
    "First, we need to install all the necessary Python libraries. This ensures that all dependencies for data processing, LLM interaction, and vector database operations are met. If you are running this in a Colab environment, these commands will typically install the packages.\n",
    "\n",
    "**Note**: If you are running this locally, ensure you have Ollama installed and the `llama3.2` model pulled (`ollama pull llama3.2`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All specified packages installed (or upgraded) successfully!\n",
      "Please ensure the model is pulled for Ollama with the command:\n"
     ]
    }
   ],
   "source": [
    "# Install necessary packages\n",
    "# %pip install -U -q huggingface_hub tiktoken pymupdf langchain-community langchain langchain-chroma langchain-huggingface ollama\n",
    "\n",
    "\n",
    "print(\"All specified packages installed (or upgraded) successfully!\")\n",
    "print(\"Please ensure the model is pulled for Ollama with the command:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Necessary Modules\n",
    "\n",
    "We import the `RAG_LLM` class from `functions.py` and constants from `config.py`. Make sure `functions.py`, `config.py`, and `prompt_templates.py` are in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules imported successfully.\n"
     ]
    }
   ],
   "source": [
    "from functions import RAG_LLM\n",
    "from config import APPLE_PDF_PATH, DEFAULT_K_RETRIEVER, DEFAULT_MAX_TOKENS, DEFAULT_TEMPERATURE\n",
    "import os\n",
    "\n",
    "print(\"Modules imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize the RAG_LLM System\n",
    "\n",
    "Here, we create an instance of our `RAG_LLM` class. This object will manage the entire RAG pipeline, including data loading, processing, retrieval, and LLM interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG_LLM initialized.\n",
      "RAG_LLM system initialized.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the RAG_LLM class\n",
    "rag_system = RAG_LLM()\n",
    "print(\"RAG_LLM system initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load the PDF Document\n",
    "\n",
    "We load the `HBR_How_Apple_Is_Organized_For_Innovation.pdf` document. Ensure this PDF file is available in the working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: HBR_How_Apple_Is_Organized_For_Innovation.pdf\n",
      "Successfully loaded 11 pages.\n",
      "PDF document loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load the PDF document\n",
    "documents = rag_system.load_data(pdf_path=APPLE_PDF_PATH)\n",
    "if not documents:\n",
    "    print(\"Failed to load documents. Please check the PDF path and file existence.\")\n",
    "else:\n",
    "    print(\"PDF document loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Chunk the Loaded Data\n",
    "\n",
    "The loaded document is chunked into smaller, overlapping segments. This is crucial for efficient retrieval and to fit content within the LLM's context window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking data with chunk_size=1024, chunk_overlap=20\n",
      "Created 16 chunks.\n",
      "Documents chunked successfully.\n"
     ]
    }
   ],
   "source": [
    "# Chunk the loaded data\n",
    "document_chunks = rag_system.chunk_data(documents)\n",
    "if not document_chunks:\n",
    "    print(\"Failed to chunk documents.\")\n",
    "else:\n",
    "    print(\"Documents chunked successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Embedding Model\n",
    "\n",
    "An embedding model (SentenceTransformer) is initialized to convert text chunks into numerical vectors, enabling semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing embedding model: mixedbread-ai/mxbai-embed-large-v1\n",
      "Embedding model initialized successfully.\n",
      "Embedding model created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create embedding model\n",
    "rag_system.create_embeddings()\n",
    "if not rag_system.embedding_model:\n",
    "    print(\"Failed to create embedding model.\")\n",
    "else:\n",
    "    print(\"Embedding model created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Set Up the Vector Database\n",
    "\n",
    "The Chroma vector database is set up using the chunked documents and the embedding model. This database will store the embeddings and facilitate quick retrieval of relevant context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up vector database in: vector_db_1024\n",
      "Vector database loaded from existing directory.\n",
      "Retriever initialized.\n",
      "Vector database set up and retriever initialized.\n"
     ]
    }
   ],
   "source": [
    "# Set up the vector database\n",
    "rag_system.setup_vector_database(document_chunks=document_chunks)\n",
    "if not rag_system.vectorstore:\n",
    "    print(\"Failed to set up vector database.\")\n",
    "else:\n",
    "    print(\"Vector database set up and retriever initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Demonstrate Question Answering with RAG\n",
    "\n",
    "Now, we can ask questions and see how the RAG system retrieves relevant information and generates answers based on the loaded document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query 1: Who are the authors of this article and who published this article ?\n",
      "Retrieving 3 relevant documents for the query.\n",
      "RAG prompt created.\n",
      "Generating LLM response using model: deepseek-r1\n"
     ]
    }
   ],
   "source": [
    "# Example Query 1\n",
    "user_input_1 = \"Who are the authors of this article and who published this article ?\"\n",
    "print(f\"\\nQuery 1: {user_input_1}\")\n",
    "llm_response_1 = rag_system.get_answer(user_input_1)\n",
    "print(f\"Response 1: \\n{llm_response_1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Query 2\n",
    "user_input_2 = \"List down the three leadership characteristics in bulleted points and explain each one of the characteristics under two lines.\"\n",
    "print(f\"\\nQuery 2: {user_input_2}\")\n",
    "# Adjust max_tokens to allow for a more complete answer for a list\n",
    "llm_response_2 = rag_system.get_answer(user_input_2, max_tokens=150, temperature=0.1)\n",
    "print(f\"Response 2: \\n{llm_response_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Query 3 (expected to be \"I don't know\" if not in context)\n",
    "user_input_3 = \"Can you explain specific examples from the article where Apple's approach to leadership has led to successful innovations?\"\n",
    "print(f\"\\nQuery 3: {user_input_3}\")\n",
    "llm_response_3 = rag_system.get_answer(user_input_3)\n",
    "print(f\"Response 3: \\n{llm_response_3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Demonstrate Output Evaluation (LLM-as-a-Judge)\n",
    "\n",
    "Finally, we demonstrate how the RAG system can evaluate its own answers for 'groundedness' (adherence to context) and 'relevance' (how well it answers the question)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Query 1\n",
    "user_input_1 = \"Who are the authors of this article and who published this article ?\"\n",
    "print(\"\\nEvaluating Query 1:\")\n",
    "rag_system.calculate_rating(question=user_input_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Query 2\n",
    "user_input_2 = \"List down the three leadership characteristics in bulleted points and explain each one of the characteristics under two lines.\"\n",
    "print(\"\\nEvaluating Query 2:\")\n",
    "# Using parameters that yield a better answer for evaluation\n",
    "rag_system.calculate_rating(question=user_input_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Query 3\n",
    "user_input_3 = \"Can you explain specific examples from the article where Apple's approach to leadership has led to successful innovations?\"\n",
    "print(\"\\nEvaluating Query 3:\")\n",
    "rag_system.calculate_rating(question=user_input_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook provides a comprehensive walkthrough of setting up and using a RAG LLM system for document Q&A and evaluation. You can modify the parameters and queries to experiment further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1203585a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
