{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://upload.wikimedia.org/wikipedia/commons/e/e9/4_RGB_McCombs_School_Brand_Branded.png\" width=\"300\" height=\"100\"/>\n",
    "  <img src=\"https://mma.prnewswire.com/media/1458111/Great_Learning_Logo.jpg?p=facebook\" width=\"200\" height=\"100\"/></center>\n",
    "\n",
    "<center><font size=10>Artificial Intelligence and Machine Learning</center></font>\n",
    "<center><font size=6>Natural Language Processing with Generative AI - Retrieval Augmented Generation</center></font>\n",
    "\n",
    "<center><img src=\"https://i.ibb.co/pBF9nKpf/apple.png\" width=\"720\"></center>\n",
    "\n",
    "<center><font size=6>Apple HBR Report Document Q&A</center></font>\n",
    "\n",
    "# RAG LLM Application Notebook\n",
    "\n",
    "This notebook demonstrates the complete workflow of a Retrieval-Augmented Generation (RAG) LLM application. It covers data loading, chunking, embedding, vector database setup, question answering, and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Library Installation\n",
    "\n",
    "First, we need to install all the necessary Python libraries. This ensures that all dependencies for data processing, LLM interaction, and vector database operations are met. If you are running this in a Colab environment, these commands will typically install the packages.\n",
    "\n",
    "**Note**: If you are running this locally, ensure you have Ollama installed and the `llama3.2` model pulled (`ollama pull llama3.2`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All specified packages installed (or upgraded) successfully!\n",
      "Please ensure the model is pulled for Ollama with the command:\n"
     ]
    }
   ],
   "source": [
    "# Install necessary packages\n",
    "# %pip install -r requirements.txt\n",
    "\n",
    "\n",
    "print(\"All specified packages installed (or upgraded) successfully!\")\n",
    "print(\"Please ensure the model is pulled for Ollama with the command:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Necessary Modules\n",
    "\n",
    "We import the `RAG_LLM` class from `functions.py` and constants from `config.py`. Make sure `functions.py`, `config.py`, and `prompt_templates.py` are in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nra29/VScode/GL-AIML/CaseStudy/NLP_GAI_W4/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules imported successfully.\n"
     ]
    }
   ],
   "source": [
    "from functions import RAG_LLM\n",
    "from config import APPLE_PDF_PATH, DEFAULT_K_RETRIEVER, DEFAULT_MAX_TOKENS, DEFAULT_TEMPERATURE\n",
    "import os\n",
    "\n",
    "print(\"Modules imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize the RAG_LLM System\n",
    "\n",
    "Here, we create an instance of our `RAG_LLM` class. This object will manage the entire RAG pipeline, including data loading, processing, retrieval, and LLM interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama client initialized.\n",
      "Google Generative AI SDK configured successfully.\n",
      "Default model for this instance is set to: 'qwen3:4b-instruct'\n",
      "RAG_LLM initialized.\n",
      "RAG_LLM system initialized.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the RAG_LLM class\n",
    "rag_system = RAG_LLM()\n",
    "print(\"RAG_LLM system initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load the PDF Document\n",
    "\n",
    "We load the `HBR_How_Apple_Is_Organized_For_Innovation.pdf` document. Ensure this PDF file is available in the working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: HBR_How_Apple_Is_Organized_For_Innovation.pdf\n",
      "Successfully loaded 11 pages.\n",
      "PDF document loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load the PDF document\n",
    "documents = rag_system.load_data(pdf_path=APPLE_PDF_PATH)\n",
    "if not documents:\n",
    "    print(\"Failed to load documents. Please check the PDF path and file existence.\")\n",
    "else:\n",
    "    print(\"PDF document loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Chunk the Loaded Data\n",
    "\n",
    "The loaded document is chunked into smaller, overlapping segments. This is crucial for efficient retrieval and to fit content within the LLM's context window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking data with chunk_size=1024, chunk_overlap=20\n",
      "Created 16 chunks.\n",
      "Documents chunked successfully.\n"
     ]
    }
   ],
   "source": [
    "# Chunk the loaded data\n",
    "document_chunks = rag_system.chunk_data(documents)\n",
    "if not document_chunks:\n",
    "    print(\"Failed to chunk documents.\")\n",
    "else:\n",
    "    print(\"Documents chunked successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Embedding Model\n",
    "\n",
    "An embedding model (SentenceTransformer) is initialized to convert text chunks into numerical vectors, enabling semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing embedding model: mixedbread-ai/mxbai-embed-large-v1\n",
      "Embedding model initialized successfully.\n",
      "Embedding model created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create embedding model\n",
    "rag_system.create_embeddings()\n",
    "if not rag_system.embedding_model:\n",
    "    print(\"Failed to create embedding model.\")\n",
    "else:\n",
    "    print(\"Embedding model created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Set Up the Vector Database\n",
    "\n",
    "The Chroma vector database is set up using the chunked documents and the embedding model. This database will store the embeddings and facilitate quick retrieval of relevant context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up vector database in: vector_db_1024\n",
      "Vector database loaded from existing directory.\n",
      "Retriever initialized.\n",
      "Vector database set up and retriever initialized.\n"
     ]
    }
   ],
   "source": [
    "# Set up the vector database\n",
    "rag_system.setup_vector_database(document_chunks=document_chunks)\n",
    "if not rag_system.vectorstore:\n",
    "    print(\"Failed to set up vector database.\")\n",
    "else:\n",
    "    print(\"Vector database set up and retriever initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Demonstrate Question Answering with RAG\n",
    "\n",
    "Now, we can ask questions and see how the RAG system retrieves relevant information and generates answers based on the loaded document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query 1: Who are the authors of this article and who published this article ?\n",
      "Retrieving 3 relevant documents for the query.\n",
      "RAG prompt created.\n",
      "Generating LLM response using model: qwen3:4b-instruct\n",
      "Ollama response generated.\n",
      "Response 1: \n",
      "The authors of the article are Joel M. Podolny and Morten T. Hansen. The article was published by Harvard Business Review.\n"
     ]
    }
   ],
   "source": [
    "# Example Query 1\n",
    "user_input_1 = \"Who are the authors of this article and who published this article ?\"\n",
    "print(f\"\\nQuery 1: {user_input_1}\")\n",
    "llm_response_1 = rag_system.get_answer(user_input_1)\n",
    "print(f\"Response 1: \\n{llm_response_1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c12c13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default model for this instance has been changed to: 'gemini-1.5-flash-latest'\n"
     ]
    }
   ],
   "source": [
    "rag_system.set_model(\"gemini-1.5-flash-latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37933c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query 1: Who are the authors of this article and who published this article ?\n",
      "Retrieving 3 relevant documents for the query.\n",
      "RAG prompt created.\n",
      "Generating LLM response using model: gemini-1.5-flash-latest\n",
      "Gemini response generated.\n",
      "Response 1: \n",
      "Joel M. Podolny and Morten T. Hansen are the authors.  Harvard Business Review published the article.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example Query 1\n",
    "user_input_1 = \"Who are the authors of this article and who published this article ?\"\n",
    "print(f\"\\nQuery 1: {user_input_1}\")\n",
    "llm_response_1 = rag_system.get_answer(user_input_1)\n",
    "print(f\"Response 1: \\n{llm_response_1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query 2: List down the three leadership characteristics in bulleted points and explain each one of the characteristics under two lines.\n",
      "Retrieving 3 relevant documents for the query.\n",
      "RAG prompt created.\n",
      "Generating LLM response using model: gemini-1.5-flash-latest\n",
      "Gemini response generated.\n",
      "Response 2: \n",
      "* Deep expertise: Apple leaders possess extensive knowledge in their respective fields.  This allows for informed decision-making and effective guidance.\n",
      "* Immersion in details: Leaders are deeply involved in the specifics of their functions. This ensures a thorough understanding of ongoing projects and potential challenges.\n",
      "* Collaborative debate:  Leaders actively engage in discussions with colleagues across various teams. This fosters innovation and facilitates efficient problem-solving.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example Query 2\n",
    "user_input_2 = \"List down the three leadership characteristics in bulleted points and explain each one of the characteristics under two lines.\"\n",
    "print(f\"\\nQuery 2: {user_input_2}\")\n",
    "# Adjust max_tokens to allow for a more complete answer for a list\n",
    "llm_response_2 = rag_system.get_answer(user_input_2, max_tokens=150, temperature=0.1)\n",
    "print(f\"Response 2: \\n{llm_response_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query 3: Can you explain specific examples from the article where Apple's approach to leadership has led to successful innovations?\n",
      "Retrieving 3 relevant documents for the query.\n",
      "RAG prompt created.\n",
      "Generating LLM response using model: gemini-1.5-flash-latest\n",
      "Gemini response generated.\n",
      "Response 3: \n",
      "I don't know\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example Query 3 (expected to be \"I don't know\" if not in context)\n",
    "user_input_3 = \"Can you explain specific examples from the article where Apple's approach to leadership has led to successful innovations?\"\n",
    "print(f\"\\nQuery 3: {user_input_3}\")\n",
    "llm_response_3 = rag_system.get_answer(user_input_3)\n",
    "print(f\"Response 3: \\n{llm_response_3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Demonstrate Output Evaluation (LLM-as-a-Judge)\n",
    "\n",
    "Finally, we demonstrate how the RAG system can evaluate its own answers for 'groundedness' (adherence to context) and 'relevance' (how well it answers the question)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Query 1:\n",
      "\n",
      "--- Calculating Ratings for Question: 'Who are the authors of this article and who published this article ?' ---\n",
      "Retrieving 3 relevant documents for the query.\n",
      "RAG prompt created.\n",
      "Generating LLM response using model: gemini-1.5-flash-latest\n",
      "Gemini response generated.\n",
      "Rating overall answer quality (groundedness and relevance)...\n",
      "Rating groundedness...\n",
      "Retrieving 3 relevant documents for the query.\n",
      "Generating LLM response using model: gemini-1.5-flash-latest\n",
      "Gemini response generated.\n",
      "Rating relevance...\n",
      "Retrieving 3 relevant documents for the query.\n",
      "Generating LLM response using model: gemini-1.5-flash-latest\n",
      "Gemini response generated.\n",
      "\n",
      "--- Results ---\n",
      "Question: \n",
      " Who are the authors of this article and who published this article ?\n",
      "\n",
      "Answer: \n",
      " Joel M. Podolny and Morten T. Hansen are the authors.  Harvard Business Review published the article.\n",
      "\n",
      "\n",
      "Groundedness Rating: \n",
      " Steps to evaluate the answer:\n",
      "\n",
      "1. **Identify the authors:** Check if the answer correctly identifies the authors mentioned in the context.\n",
      "2. **Identify the publisher:** Check if the answer correctly identifies the publisher mentioned in the context.\n",
      "3. **Verify information source:** Ensure that the identified authors and publisher are explicitly mentioned within the provided context.\n",
      "\n",
      "\n",
      "Step-by-step explanation:\n",
      "\n",
      "1. The context clearly states \"AUTHORS Joel M. Podolny Dean, Apple University Morten T. Hansen Faculty, Apple University\".  The answer correctly identifies these two individuals as the authors.\n",
      "2. The context mentions \"Harvard Business Review Novemberâ€“December 2020\". The answer correctly identifies Harvard Business Review as the publisher.\n",
      "3. Both the authors and the publisher are explicitly named in the provided text.\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "The answer completely adheres to the metric.  It extracts the required information directly from the provided context without any inference or external knowledge.\n",
      "\n",
      "\n",
      "Score:\n",
      "\n",
      "\n",
      "\n",
      "Relevance Rating: \n",
      " Steps to evaluate the context based on the relevance metric:\n",
      "\n",
      "1. **Identify the main aspects of the question:** The question asks for the authors and the publisher of the article.\n",
      "\n",
      "2. **Locate the relevant information in the context:** The context contains the names of the authors (\"Joel M. Podolny\" and \"Morten T. Hansen\") and the publisher (\"Harvard Business Review\") in clearly labeled sections.\n",
      "\n",
      "3. **Check for completeness:** The context provides all the necessary information to answer both parts of the question.  There is no extraneous information included that is not relevant to the question.\n",
      "\n",
      "4. **Check for accuracy:** The information provided is accurate and consistent.\n",
      "\n",
      "\n",
      "Step-by-step explanation of whether the context adheres to the metric:\n",
      "\n",
      "1. The question explicitly asks for the authors and publisher.\n",
      "2. The context clearly states \"AUTHORS\" followed by the names Joel M. Podolny and Morten T. Hansen.\n",
      "3. The context also\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Query 1\n",
    "user_input_1 = \"Who are the authors of this article and who published this article ?\"\n",
    "print(\"\\nEvaluating Query 1:\")\n",
    "rag_system.calculate_rating(question=user_input_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Query 2:\n",
      "\n",
      "--- Calculating Ratings for Question: 'List down the three leadership characteristics in bulleted points and explain each one of the characteristics under two lines.' ---\n",
      "Retrieving 3 relevant documents for the query.\n",
      "RAG prompt created.\n",
      "Generating LLM response using model: gemini-1.5-flash-latest\n",
      "Gemini response generated.\n",
      "Rating overall answer quality (groundedness and relevance)...\n",
      "Rating groundedness...\n",
      "Retrieving 3 relevant documents for the query.\n",
      "Generating LLM response using model: gemini-1.5-flash-latest\n",
      "Gemini response generated.\n",
      "Rating relevance...\n",
      "Retrieving 3 relevant documents for the query.\n",
      "Generating LLM response using model: gemini-1.5-flash-latest\n",
      "Gemini response generated.\n",
      "\n",
      "--- Results ---\n",
      "Question: \n",
      " List down the three leadership characteristics in bulleted points and explain each one of the characteristics under two lines.\n",
      "\n",
      "Answer: \n",
      " * Deep expertise: Apple leaders possess extensive knowledge in their respective fields.  This allows for informed decision-making and effective guidance.\n",
      "* Immersion in details: Leaders are deeply involved in the specifics of their functions. This ensures a thorough understanding of ongoing projects and potential challenges.\n",
      "* Collaborative debate:  Leaders actively engage in discussions with colleagues across various teams. This fosters innovation and efficient problem-solving.\n",
      "\n",
      "\n",
      "Groundedness Rating: \n",
      " Steps to evaluate the answer based on the metric:\n",
      "\n",
      "1. **Identify the claims** made in the AI's answer.\n",
      "2. **Locate the evidence** for each claim within the provided context.\n",
      "3. **Assess whether each claim** is directly supported by the context, without inference or external knowledge.\n",
      "\n",
      "\n",
      "Step-by-step explanation:\n",
      "\n",
      "The AI's answer presents three leadership characteristics:\n",
      "\n",
      "* **Deep expertise:** The context mentions that Apple leaders are \"expected to possess deep expertise\" and that they are \"experts leading experts\".  This directly supports the AI's claim.\n",
      "* **Immersion in details:** The context states that leaders \"had been immersed in details\" and that they \"know which details are important and where to focus their attention\". This directly supports the AI's claim.\n",
      "* **Collaborative debate:** The context highlights the importance of \"collaborative debate\" among leaders and mentions that hundreds of specialist teams collaborate on projects. This directly supports\n",
      "\n",
      "Relevance Rating: \n",
      " Steps to evaluate the context based on the relevance metric:\n",
      "\n",
      "1. **Identify the main aspects of the question:** The question asks for three leadership characteristics in bullet points, with a brief explanation of each (under two lines).\n",
      "\n",
      "2. **Check if the context addresses all the main aspects:** The provided context discusses Apple's leadership model, focusing on the characteristics of its leaders.  It mentions \"experts leading experts,\" \"immersion in the details,\" and collaborative work.  However, it doesn't explicitly list these as three distinct characteristics in bullet points with concise explanations.  The context provides a detailed explanation of the leadership model, but not in the format requested.\n",
      "\n",
      "3. **Assess if the context contains only the important aspects:** The context contains much more information than is strictly necessary to answer the question.  While the extra information is relevant to understanding Apple's leadership approach, it's not directly required to list and explain three characteristics.\n",
      "\n",
      "4. **Determine if the\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Query 2\n",
    "user_input_2 = \"List down the three leadership characteristics in bulleted points and explain each one of the characteristics under two lines.\"\n",
    "print(\"\\nEvaluating Query 2:\")\n",
    "# Using parameters that yield a better answer for evaluation\n",
    "rag_system.calculate_rating(question=user_input_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Query 3:\n",
      "\n",
      "--- Calculating Ratings for Question: 'Can you explain specific examples from the article where Apple's approach to leadership has led to successful innovations?' ---\n",
      "Retrieving 3 relevant documents for the query.\n",
      "RAG prompt created.\n",
      "Generating LLM response using model: gemini-1.5-flash-latest\n",
      "Gemini response generated.\n",
      "Rating overall answer quality (groundedness and relevance)...\n",
      "Rating groundedness...\n",
      "Retrieving 3 relevant documents for the query.\n",
      "Generating LLM response using model: gemini-1.5-flash-latest\n",
      "Gemini response generated.\n",
      "Rating relevance...\n",
      "Retrieving 3 relevant documents for the query.\n",
      "Generating LLM response using model: gemini-1.5-flash-latest\n",
      "Gemini response generated.\n",
      "\n",
      "--- Results ---\n",
      "Question: \n",
      " Can you explain specific examples from the article where Apple's approach to leadership has led to successful innovations?\n",
      "\n",
      "Answer: \n",
      " I don't know\n",
      "\n",
      "\n",
      "Groundedness Rating: \n",
      " Steps to evaluate the answer based on the metric:\n",
      "\n",
      "1. **Identify the question**: Determine what the user is asking.\n",
      "2. **Analyze the context**: Identify if the context provides information relevant to the question.\n",
      "3. **Compare the answer to the context**: Check if the answer is derived solely from the information present in the context.\n",
      "4. **Assess the completeness**: Evaluate if the answer fully addresses the question based on the context.\n",
      "\n",
      "\n",
      "Step-by-step explanation:\n",
      "\n",
      "1. The question asks for specific examples from the article illustrating how Apple's leadership approach has driven successful innovations.\n",
      "\n",
      "2. The context discusses Apple's organizational structure and leadership model, highlighting its shift to a functional structure under Steve Jobs and its evolution under Tim Cook.  It mentions the challenges of growth and how Apple adapted its leadership approach (discretionary leadership model).  However, the article does *not* provide specific examples of innovations directly linked to these leadership decisions.  The article\n",
      "\n",
      "Relevance Rating: \n",
      " Steps to evaluate the context based on the relevance metric:\n",
      "\n",
      "1. **Identify the main aspects of the question:** The question asks for specific examples from the article illustrating how Apple's leadership approach has driven successful innovations.  This requires identifying instances where leadership decisions directly contributed to the creation or success of specific Apple products or technologies.\n",
      "\n",
      "2. **Analyze the context for relevant information:** The provided text extensively discusses Apple's organizational structure and leadership model under Steve Jobs and Tim Cook. It details the shift to a functional structure, the challenges of scaling this model, and the evolution of leadership styles to address those challenges. However, the context lacks specific examples of innovations directly linked to these leadership decisions.  The article mentions the iPhone's launch in relation to the number of VPs, but doesn't connect it causally to a specific leadership decision or strategy.\n",
      "\n",
      "3. **Assess whether the context provides sufficient information to answer the question:** The context provides a detailed background on Apple's\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Query 3\n",
    "user_input_3 = \"Can you explain specific examples from the article where Apple's approach to leadership has led to successful innovations?\"\n",
    "print(\"\\nEvaluating Query 3:\")\n",
    "rag_system.calculate_rating(question=user_input_3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
