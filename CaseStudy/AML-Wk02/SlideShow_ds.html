<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Boosting Algorithms: AdaBoost, GBM, XGBoost</title>
    <style>
        :root {
            --primary: #2c3e50;
            --secondary: #3498db;
            --accent: #e74c3c;
            --light: #ecf0f1;
            --dark: #2c3e50;
            --success: #27ae60;
            --warning: #f39c12;
        }
        
        * {
            box-sizing: border-box;
            margin: 0;
            padding: 0;
        }
        
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #1a2a6c, #b21f1f, #1a2a6c);
            color: var(--light);
            line-height: 1.6;
            height: 100vh;
            overflow: hidden;
        }
        
        .slideshow-container {
            width: 100%;
            height: 100vh;
            display: flex;
            flex-direction: column;
            position: relative;
        }
        
        .slide {
            width: 90%;
            max-width: 1200px;
            height: 80vh;
            margin: 5vh auto;
            background: rgba(44, 62, 80, 0.92);
            border-radius: 15px;
            padding: 30px;
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.4);
            display: none;
            position: relative;
            overflow-y: auto;
        }
        
        .slide.active {
            display: block;
            animation: fadeIn 0.8s ease;
        }
        
        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(20px); }
            to { opacity: 1; transform: translateY(0); }
        }
        
        .slide-header {
            text-align: center;
            margin-bottom: 30px;
            padding-bottom: 15px;
            border-bottom: 2px solid var(--secondary);
        }
        
        .slide-header h1 {
            color: var(--secondary);
            font-size: 2.8rem;
            margin-bottom: 10px;
            text-shadow: 0 2px 4px rgba(0,0,0,0.3);
        }
        
        .slide-header h2 {
            color: var(--light);
            font-size: 1.8rem;
            font-weight: 400;
        }
        
        .content {
            display: flex;
            flex-wrap: wrap;
            gap: 30px;
            margin-top: 20px;
        }
        
        .explanation {
            flex: 1;
            min-width: 300px;
            background: rgba(52, 152, 219, 0.1);
            padding: 20px;
            border-radius: 10px;
            border-left: 4px solid var(--secondary);
        }
        
        .technical {
            flex: 1;
            min-width: 300px;
            background: rgba(231, 76, 60, 0.1);
            padding: 20px;
            border-radius: 10px;
            border-left: 4px solid var(--accent);
        }
        
        h3 {
            color: var(--secondary);
            margin-bottom: 15px;
            font-size: 1.8rem;
        }
        
        .technical h3 {
            color: var(--accent);
        }
        
        ul {
            padding-left: 20px;
            margin-bottom: 20px;
        }
        
        li {
            margin-bottom: 12px;
        }
        
        .algorithm-visual {
            width: 100%;
            display: flex;
            justify-content: center;
            margin: 20px 0;
        }
        
        .visual-container {
            width: 100%;
            max-width: 800px;
            height: 250px;
            background: rgba(0, 0, 0, 0.2);
            border-radius: 10px;
            display: flex;
            align-items: center;
            justify-content: center;
            position: relative;
            overflow: hidden;
        }
        
        .hyperparameters {
            width: 100%;
            margin-top: 20px;
            background: rgba(39, 174, 96, 0.1);
            padding: 20px;
            border-radius: 10px;
            border-left: 4px solid var(--success);
        }
        
        .hyperparameters h3 {
            color: var(--success);
        }
        
        .param-grid {
            display: grid;
            grid-template-columns: repeat(auto-fill, minmax(250px, 1fr));
            gap: 15px;
            margin-top: 15px;
        }
        
        .param-card {
            background: rgba(255, 255, 255, 0.08);
            padding: 15px;
            border-radius: 8px;
            transition: transform 0.3s ease;
        }
        
        .param-card:hover {
            transform: translateY(-5px);
            background: rgba(255, 255, 255, 0.12);
        }
        
        .param-card h4 {
            color: var(--warning);
            margin-bottom: 8px;
        }
        
        .nav-buttons {
            position: absolute;
            bottom: 30px;
            left: 50%;
            transform: translateX(-50%);
            display: flex;
            gap: 15px;
            z-index: 100;
        }
        
        .nav-btn {
            background: var(--secondary);
            color: white;
            border: none;
            padding: 12px 25px;
            border-radius: 50px;
            font-size: 1.1rem;
            cursor: pointer;
            transition: all 0.3s ease;
            box-shadow: 0 4px 8px rgba(0,0,0,0.3);
        }
        
        .nav-btn:hover {
            background: #2980b9;
            transform: translateY(-3px);
        }
        
        .nav-btn:active {
            transform: translateY(1px);
        }
        
        .prev-btn {
            background: var(--primary);
        }
        
        .prev-btn:hover {
            background: #1a252f;
        }
        
        .slide-counter {
            position: absolute;
            top: 20px;
            right: 20px;
            background: rgba(0, 0, 0, 0.3);
            padding: 8px 15px;
            border-radius: 20px;
            font-size: 1.1rem;
        }
        
        .comparison {
            width: 100%;
            margin-top: 30px;
            overflow-x: auto;
        }
        
        table {
            width: 100%;
            border-collapse: collapse;
            background: rgba(0, 0, 0, 0.2);
        }
        
        th {
            background: var(--secondary);
            padding: 15px;
            text-align: left;
        }
        
        td {
            padding: 15px;
            border-bottom: 1px solid rgba(255, 255, 255, 0.1);
        }
        
        tr:hover {
            background: rgba(52, 152, 219, 0.1);
        }
        
        .legend {
            display: flex;
            justify-content: center;
            gap: 20px;
            margin: 20px 0;
        }
        
        .legend-item {
            display: flex;
            align-items: center;
            gap: 8px;
        }
        
        .legend-color {
            width: 20px;
            height: 20px;
            border-radius: 4px;
        }
        
        .color-layman {
            background: rgba(52, 152, 219, 0.7);
        }
        
        .color-technical {
            background: rgba(231, 76, 60, 0.7);
        }
        
        .color-hyper {
            background: rgba(39, 174, 96, 0.7);
        }
        
        .algorithm-title {
            text-align: center;
            margin: 30px 0;
            font-size: 2.5rem;
            color: var(--warning);
            text-shadow: 0 2px 4px rgba(0,0,0,0.3);
        }
        
        .algorithm-subtitle {
            text-align: center;
            margin-bottom: 30px;
            color: var(--light);
            font-size: 1.4rem;
            font-weight: 300;
        }
        
        .pros-cons {
            display: flex;
            gap: 30px;
            margin-top: 20px;
        }
        
        .pros, .cons {
            flex: 1;
            padding: 20px;
            border-radius: 10px;
        }
        
        .pros {
            background: rgba(39, 174, 96, 0.1);
            border-left: 4px solid var(--success);
        }
        
        .cons {
            background: rgba(231, 76, 60, 0.1);
            border-left: 4px solid var(--accent);
        }
        
        @media (max-width: 768px) {
            .slide {
                width: 95%;
                height: 85vh;
                padding: 20px;
            }
            
            .content {
                flex-direction: column;
            }
            
            .slide-header h1 {
                font-size: 2.2rem;
            }
            
            .nav-buttons {
                width: 90%;
                justify-content: center;
            }
            
            .param-grid {
                grid-template-columns: 1fr;
            }
        }
    </style>
</head>
<body>
    <div class="slideshow-container">
        <!-- Slide 1: Title -->
        <div class="slide active" id="slide1">
            <div class="slide-header">
                <h1>Boosting Algorithms</h1>
                <h2>AdaBoost, Gradient Boosting & XGBoost</h2>
            </div>
            <div class="algorithm-subtitle">A Comprehensive Guide for Machine Learning Practitioners</div>
            
            <div class="content">
                <div class="explanation">
                    <h3>What is Boosting?</h3>
                    <p>Boosting is a powerful ensemble technique that combines multiple weak learners (simple models) to create a strong learner (highly accurate model).</p>
                    <p>Imagine a team of specialists where each member focuses on correcting the mistakes of previous members. This sequential improvement is the essence of boosting.</p>
                </div>
                
                <div class="technical">
                    <h3>Technical Foundation</h3>
                    <p>Boosting algorithms:</p>
                    <ul>
                        <li>Build models sequentially (not in parallel like bagging)</li>
                        <li>Assign higher weights to misclassified observations</li>
                        <li>Combine weak learners through weighted majority vote</li>
                        <li>Reduce both bias and variance in models</li>
                        <li>Are highly effective for structured/tabular data</li>
                    </ul>
                </div>
            </div>
            
            <div class="algorithm-visual">
                <div class="visual-container">
                    <svg width="100%" height="100%">
                        <rect width="100%" height="100%" fill="rgba(0,0,0,0.1)" />
                        <text x="50%" y="50%" text-anchor="middle" fill="#fff" font-size="20">Ensemble Learning Concept Visualization</text>
                    </svg>
                </div>
            </div>
            
            <div class="slide-counter">1/12</div>
        </div>
        
        <!-- Slide 2: AdaBoost Introduction -->
        <div class="slide" id="slide2">
            <div class="algorithm-title">AdaBoost</div>
            <div class="algorithm-subtitle">Adaptive Boosting</div>
            
            <div class="legend">
                <div class="legend-item">
                    <div class="legend-color color-layman"></div>
                    <span>Layman Explanation</span>
                </div>
                <div class="legend-item">
                    <div class="legend-color color-technical"></div>
                    <span>Technical Details</span>
                </div>
            </div>
            
            <div class="content">
                <div class="explanation">
                    <h3>How AdaBoost Works</h3>
                    <p>Think of AdaBoost like a learning process where:</p>
                    <ul>
                        <li>A teacher focuses more on students who answered incorrectly</li>
                        <li>Each subsequent quiz targets previous mistakes</li>
                        <li>Teachers with better accuracy have more influence on final grades</li>
                        <li>The final grade combines all teachers' opinions with weighting</li>
                    </ul>
                </div>
                
                <div class="technical">
                    <h3>Technical Process</h3>
                    <p>AdaBoost algorithm steps:</p>
                    <ol>
                        <li>Initialize equal weights for all samples</li>
                        <li>Train weak learner (usually decision stump) on weighted data</li>
                        <li>Calculate model error and update sample weights</li>
                        <li>Increase weights of misclassified samples</li>
                        <li>Repeat for T iterations</li>
                        <li>Combine weak learners via weighted majority vote</li>
                    </ol>
                    <p>Model weight: αₜ = ½ ln((1 - errₜ)/errₜ)</p>
                </div>
            </div>
            
            <div class="algorithm-visual">
                <div class="visual-container">
                    <svg width="100%" height="100%">
                        <rect width="100%" height="100%" fill="rgba(52,152,219,0.1)" />
                        <text x="50%" y="50%" text-anchor="middle" fill="#fff" font-size="20">AdaBoost Sequential Weight Adjustment</text>
                    </svg>
                </div>
            </div>
            
            <div class="slide-counter">2/12</div>
        </div>
        
        <!-- Slide 3: AdaBoost Hyperparameters -->
        <div class="slide" id="slide3">
            <div class="algorithm-title">AdaBoost</div>
            <div class="algorithm-subtitle">Key Hyperparameters & Characteristics</div>
            
            <div class="hyperparameters">
                <h3>Critical Hyperparameters</h3>
                <div class="param-grid">
                    <div class="param-card">
                        <h4>base_estimator</h4>
                        <p>The weak learner to use (default: Decision Tree with max_depth=1)</p>
                    </div>
                    <div class="param-card">
                        <h4>n_estimators</h4>
                        <p>Number of weak learners to train (default: 50)</p>
                    </div>
                    <div class="param-card">
                        <h4>learning_rate</h4>
                        <p>Shrinks contribution of each classifier (default: 1.0)</p>
                    </div>
                    <div class="param-card">
                        <h4>algorithm</h4>
                        <p>'SAMME' or 'SAMME.R' - affects weight calculation</p>
                    </div>
                </div>
            </div>
            
            <div class="pros-cons">
                <div class="pros">
                    <h3>Advantages</h3>
                    <ul>
                        <li>Simple to implement</li>
                        <li>Feature selection not required</li>
                        <li>Resistant to overfitting</li>
                        <li>Works with any classifier</li>
                        <li>Fast training time</li>
                    </ul>
                </div>
                
                <div class="cons">
                    <h3>Limitations</h3>
                    <ul>
                        <li>Sensitive to noisy data</li>
                        <li>Weak with imbalanced datasets</li>
                        <li>Performance plateaus with complex problems</li>
                        <li>Suboptimal for regression tasks</li>
                    </ul>
                </div>
            </div>
            
            <div class="slide-counter">3/12</div>
        </div>
        
        <!-- Slide 4: Gradient Boosting Introduction -->
        <div class="slide" id="slide4">
            <div class="algorithm-title">Gradient Boosting Machines (GBM)</div>
            <div class="algorithm-subtitle">Gradient Descent + Boosting</div>
            
            <div class="content">
                <div class="explanation">
                    <h3>How GBM Works</h3>
                    <p>Imagine a golfer learning to putt:</p>
                    <ul>
                        <li>First attempt: Hits the ball toward the hole</li>
                        <li>Measures how far they missed (residual error)</li>
                        <li>Next attempt focuses on correcting the residual</li>
                        <li>Each subsequent putt corrects previous mistakes</li>
                        <li>Final position is the sum of all adjustments</li>
                    </ul>
                </div>
                
                <div class="technical">
                    <h3>Technical Process</h3>
                    <p>GBM algorithm steps:</p>
                    <ol>
                        <li>Make initial prediction (mean for regression, log(odds) for classification)</li>
                        <li>Calculate residuals (actual - predicted)</li>
                        <li>Build tree to predict residuals</li>
                        <li>Update predictions with learning rate</li>
                        <li>Repeat for M trees</li>
                        <li>Final prediction: initial + sum of tree predictions</li>
                    </ol>
                    <p>Uses gradient descent to minimize loss function</p>
                </div>
            </div>
            
            <div class="algorithm-visual">
                <div class="visual-container">
                    <svg width="100%" height="100%">
                        <rect width="100%" height="100%" fill="rgba(231,76,60,0.1)" />
                        <text x="50%" y="50%" text-anchor="middle" fill="#fff" font-size="20">GBM Residual Correction Process</text>
                    </svg>
                </div>
            </div>
            
            <div class="slide-counter">4/12</div>
        </div>
        
        <!-- Slide 5: GBM Hyperparameters -->
        <div class="slide" id="slide5">
            <div class="algorithm-title">Gradient Boosting Machines</div>
            <div class="algorithm-subtitle">Key Hyperparameters & Characteristics</div>
            
            <div class="hyperparameters">
                <h3>Critical Hyperparameters</h3>
                <div class="param-grid">
                    <div class="param-card">
                        <h4>n_estimators</h4>
                        <p>Number of boosting stages (trees) (default: 100)</p>
                    </div>
                    <div class="param-card">
                        <h4>learning_rate</h4>
                        <p>Shrinkage factor for tree contributions (default: 0.1)</p>
                    </div>
                    <div class="param-card">
                        <h4>max_depth</h4>
                        <p>Maximum depth of individual trees (default: 3)</p>
                    </div>
                    <div class="param-card">
                        <h4>subsample</h4>
                        <p>Fraction of samples used for fitting trees (default: 1.0)</p>
                    </div>
                    <div class="param-card">
                        <h4>min_samples_split</h4>
                        <p>Minimum samples required to split a node (default: 2)</p>
                    </div>
                    <div class="param-card">
                        <h4>max_features</h4>
                        <p>Number of features considered for splits (default: all)</p>
                    </div>
                </div>
            </div>
            
            <div class="pros-cons">
                <div class="pros">
                    <h3>Advantages</h3>
                    <ul>
                        <li>Handles heterogeneous features well</li>
                        <li>Powerful predictive performance</li>
                        <li>Supports custom loss functions</li>
                        <li>Works for both regression and classification</li>
                        <li>Robust to outliers with appropriate loss</li>
                    </ul>
                </div>
                
                <div class="cons">
                    <h3>Limitations</h3>
                    <ul>
                        <li>Computationally expensive</li>
                        <li>Sensitive to hyperparameters</li>
                        <li>Sequential training limits parallelization</li>
                        <li>Can overfit if not properly regularized</li>
                        <li>Longer training times with large datasets</li>
                    </ul>
                </div>
            </div>
            
            <div class="slide-counter">5/12</div>
        </div>
        
        <!-- Slide 6: XGBoost Introduction -->
        <div class="slide" id="slide6">
            <div class="algorithm-title">XGBoost</div>
            <div class="algorithm-subtitle">Extreme Gradient Boosting</div>
            
            <div class="content">
                <div class="explanation">
                    <h3>How XGBoost Works</h3>
                    <p>Think of XGBoost as a highly optimized factory:</p>
                    <ul>
                        <li>Uses advanced techniques for faster production (parallel tree building)</li>
                        <li>Has quality control checks (regularization)</li>
                        <li>Optimizes resource allocation (hardware optimization)</li>
                        <li>Can handle missing materials (missing values)</li>
                        <li>Produces higher quality outputs than traditional methods</li>
                    </ul>
                </div>
                
                <div class="technical">
                    <h3>Technical Innovations</h3>
                    <p>XGBoost improvements over GBM:</p>
                    <ul>
                        <li>Regularized objective function: L(θ) = Σloss(yᵢ,ŷᵢ) + ΣΩ(fₖ)</li>
                        <li>Approximate greedy algorithm for split finding</li>
                        <li>Parallel tree construction using column blocks</li>
                        <li>Hardware optimization (cache-aware access)</li>
                        <li>Weighted quantile sketch for handling sparse data</li>
                        <li>Built-in cross-validation</li>
                    </ul>
                </div>
            </div>
            
            <div class="algorithm-visual">
                <div class="visual-container">
                    <svg width="100%" height="100%">
                        <rect width="100%" height="100%" fill="rgba(243,156,18,0.1)" />
                        <text x="50%" y="50%" text-anchor="middle" fill="#fff" font-size="20">XGBoost System Architecture</text>
                    </svg>
                </div>
            </div>
            
            <div class="slide-counter">6/12</div>
        </div>
        
        <!-- Slide 7: XGBoost Tree Building -->
        <div class="slide" id="slide7">
            <div class="algorithm-title">XGBoost</div>
            <div class="algorithm-subtitle">Tree Building Process</div>
            
            <div class="content">
                <div class="technical">
                    <h3>Similarity Score & Gain</h3>
                    <p>XGBoost uses a novel approach to tree building:</p>
                    
                    <p><strong>Similarity Score (SS)</strong> - Measures quality of a node:</p>
                    <p>Regression: SS = (Σ Residuals)² / (Number of samples + λ)</p>
                    <p>Classification: SS = (Σ Residuals)² / [Σ(p*(1-p)) + λ]</p>
                    
                    <p><strong>Gain</strong> - Measures improvement from a split:</p>
                    <p>Gain = [SS(left) + SS(right) - SS(root)]</p>
                    
                    <p>Where λ is regularization parameter (alpha)</p>
                    
                    <p><strong>Tree Building Process:</strong></p>
                    <ol>
                        <li>Calculate similarity score for root node</li>
                        <li>For each feature, find best split point</li>
                        <li>Calculate gain for each possible split</li>
                        <li>Select split with maximum gain</li>
                        <li>Recurse on child nodes until stopping condition</li>
                        <li>Calculate output value for leaf nodes</li>
                    </ol>
                </div>
            </div>
            
            <div class="algorithm-visual">
                <div class="visual-container">
                    <svg width="100%" height="100%">
                        <rect width="100%" height="100%" fill="rgba(39,174,96,0.1)" />
                        <text x="50%" y="50%" text-anchor="middle" fill="#fff" font-size="20">XGBoost Tree Splitting Visualization</text>
                    </svg>
                </div>
            </div>
            
            <div class="slide-counter">7/12</div>
        </div>
        
        <!-- Slide 8: XGBoost Hyperparameters -->
        <div class="slide" id="slide8">
            <div class="algorithm-title">XGBoost</div>
            <div class="algorithm-subtitle">Key Hyperparameters</div>
            
            <div class="hyperparameters">
                <h3>Critical Hyperparameters</h3>
                <div class="param-grid">
                    <div class="param-card">
                        <h4>eta (learning_rate)</h4>
                        <p>Step size shrinkage (default: 0.3)</p>
                    </div>
                    <div class="param-card">
                        <h4>gamma</h4>
                        <p>Minimum loss reduction to make split (default: 0)</p>
                    </div>
                    <div class="param-card">
                        <h4>max_depth</h4>
                        <p>Maximum tree depth (default: 6)</p>
                    </div>
                    <div class="param-card">
                        <h4>lambda (reg_lambda)</h4>
                        <p>L2 regularization term (default: 1)</p>
                    </div>
                    <div class="param-card">
                        <h4>alpha (reg_alpha)</h4>
                        <p>L1 regularization term (default: 0)</p>
                    </div>
                    <div class="param-card">
                        <h4>colsample_bytree</h4>
                        <p>Fraction of columns per tree (default: 1)</p>
                    </div>
                    <div class="param-card">
                        <h4>colsample_bylevel</h4>
                        <p>Fraction of columns per level (default: 1)</p>
                    </div>
                    <div class="param-card">
                        <h4>colsample_bynode</h4>
                        <p>Fraction of columns per split (default: 1)</p>
                    </div>
                </div>
            </div>
            
            <div class="slide-counter">8/12</div>
        </div>
        
        <!-- Slide 9: XGBoost Pros/Cons -->
        <div class="slide" id="slide9">
            <div class="algorithm-title">XGBoost</div>
            <div class="algorithm-subtitle">Advantages & Limitations</div>
            
            <div class="pros-cons">
                <div class="pros">
                    <h3>Advantages</h3>
                    <ul>
                        <li>State-of-the-art performance on structured data</li>
                        <li>Extremely fast execution (parallel processing)</li>
                        <li>Built-in regularization prevents overfitting</li>
                        <li>Handles missing values automatically</li>
                        <li>Extensive hyperparameter tuning options</li>
                        <li>Supports distributed training</li>
                        <li>Feature importance scores</li>
                    </ul>
                </div>
                
                <div class="cons">
                    <h3>Limitations</h3>
                    <ul>
                        <li>Complex hyperparameter tuning</li>
                        <li>Less interpretable than simpler models</li>
                        <li>Memory intensive with large datasets</li>
                        <li>Not ideal for unstructured data (images, text)</li>
                        <li>Can be sensitive to noisy data</li>
                        <li>Longer training times than AdaBoost</li>
                    </ul>
                </div>
            </div>
            
            <div class="hyperparameters">
                <h3>When to Use XGBoost</h3>
                <ul>
                    <li>Tabular datasets with mix of feature types</li>
                    <li>Competitions (Kaggle) where performance is critical</li>
                    <li>When you need fast prediction after training</li>
                    <li>Problems with missing values in features</li>
                    <li>When feature importance is valuable</li>
                </ul>
            </div>
            
            <div class="slide-counter">9/12</div>
        </div>
        
        <!-- Slide 10: Algorithm Comparison -->
        <div class="slide" id="slide10">
            <div class="algorithm-title">Algorithm Comparison</div>
            <div class="algorithm-subtitle">AdaBoost vs GBM vs XGBoost</div>
            
            <div class="comparison">
                <table>
                    <thead>
                        <tr>
                            <th>Characteristic</th>
                            <th>AdaBoost</th>
                            <th>Gradient Boosting</th>
                            <th>XGBoost</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Training Approach</td>
                            <td>Sequential, sample reweighting</td>
                            <td>Sequential, residual fitting</td>
                            <td>Sequential with parallel elements</td>
                        </tr>
                        <tr>
                            <td>Base Learner</td>
                            <td>Decision Stumps (typically)</td>
                            <td>Shallow Trees</td>
                            <td>Trees (depth controlled)</td>
                        </tr>
                        <tr>
                            <td>Speed</td>
                            <td>Fastest</td>
                            <td>Moderate</td>
                            <td>Fast (optimized)</td>
                        </tr>
                        <tr>
                            <td>Regularization</td>
                            <td>Limited</td>
                            <td>Some</td>
                            <td>Extensive (L1/L2)</td>
                        </tr>
                        <tr>
                            <td>Handling Missing Values</td>
                            <td>Requires preprocessing</td>
                            <td>Requires preprocessing</td>
                            <td>Built-in handling</td>
                        </tr>
                        <tr>
                            <td>Hyperparameters</td>
                            <td>Few, simple</td>
                            <td>Moderate</td>
                            <td>Many, complex</td>
                        </tr>
                        <tr>
                            <td>Best For</td>
                            <td>Simple problems, binary classification</td>
                            <td>General purpose boosting</td>
                            <td>Competitions, large datasets</td>
                        </tr>
                        <tr>
                            <td>Implementation</td>
                            <td>scikit-learn</td>
                            <td>scikit-learn</td>
                            <td>XGBoost library</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            
            <div class="slide-counter">10/12</div>
        </div>
        
        <!-- Slide 11: Practical Considerations -->
        <div class="slide" id="slide11">
            <div class="algorithm-title">Practical Implementation</div>
            <div class="algorithm-subtitle">Best Practices & Tips</div>
            
            <div class="content">
                <div class="explanation">
                    <h3>Implementation Tips</h3>
                    <ul>
                        <li>Always scale numerical features</li>
                        <li>Handle categorical features properly (one-hot encoding)</li>
                        <li>Use early stopping to prevent overfitting</li>
                        <li>Balance datasets for classification problems</li>
                        <li>Use cross-validation for reliable evaluation</li>
                        <li>Start with default parameters then tune</li>
                    </ul>
                </div>
                
                <div class="technical">
                    <h3>Hyperparameter Tuning Strategy</h3>
                    <ol>
                        <li>Set learning_rate low (0.01-0.1) and n_estimators high</li>
                        <li>Tune tree-specific parameters (max_depth, min_child_weight)</li>
                        <li>Tune regularization parameters (gamma, lambda, alpha)</li>
                        <li>Adjust sampling parameters (subsample, colsample_by*)</li>
                        <li>Reduce learning_rate and increase n_estimators</li>
                    </ol>
                    <p>Use Bayesian optimization or random search for efficiency</p>
                </div>
            </div>
            
            <div class="hyperparameters">
                <h3>Common Pitfalls to Avoid</h3>
                <ul>
                    <li>Using too many trees without regularization</li>
                    <li>Setting learning rate too high</li>
                    <li>Not using early stopping</li>
                    <li>Ignoring class imbalance in classification</li>
                    <li>Not tuning hyperparameters systematically</li>
                    <li>Overlooking feature engineering</li>
                </ul>
            </div>
            
            <div class="slide-counter">11/12</div>
        </div>
        
        <!-- Slide 12: Summary & Conclusion -->
        <div class="slide" id="slide12">
            <div class="slide-header">
                <h1>Summary & Key Takeaways</h1>
                <h2>Boosting Algorithms in Practice</h2>
            </div>
            
            <div class="content">
                <div class="explanation">
                    <h3>Core Concepts</h3>
                    <ul>
                        <li>Boosting combines weak learners to create strong models</li>
                        <li>Sequential training focuses on previous errors</li>
                        <li>Weighted combination of learners improves accuracy</li>
                        <li>Different boosting algorithms suit different problems</li>
                    </ul>
                </div>
                
                <div class="technical">
                    <h3>Algorithm Selection Guide</h3>
                    <ul>
                        <li><strong>AdaBoost:</strong> Simple problems, binary classification</li>
                        <li><strong>GBM:</strong> General purpose regression/classification</li>
                        <li><strong>XGBoost:</strong> Competitions, large datasets, top performance</li>
                    </ul>
                </div>
            </div>
            
            <div class="hyperparameters">
                <h3>Final Recommendations</h3>
                <ul>
                    <li>Start with XGBoost for most structured data problems</li>
                    <li>Use AdaBoost for simpler models and faster training</li>
                    <li>Always use cross-validation and early stopping</li>
                    <li>Invest time in hyperparameter tuning</li>
                    <li>Monitor feature importance for insights</li>
                    <li>Combine with other techniques (feature engineering, stacking)</li>
                </ul>
            </div>
            
            <div class="algorithm-visual">
                <div class="visual-container">
                    <svg width="100%" height="100%">
                        <rect width="100%" height="100%" fill="rgba(52,152,219,0.1)" />
                        <text x="50%" y="50%" text-anchor="middle" fill="#fff" font-size="24">Boosting Algorithms: Powerful Tools for Machine Learning</text>
                    </svg>
                </div>
            </div>
            
            <div class="slide-counter">12/12</div>
        </div>
        
        <!-- Navigation buttons -->
        <div class="nav-buttons">
            <button class="nav-btn prev-btn" id="prevBtn">Previous</button>
            <button class="nav-btn" id="nextBtn">Next</button>
        </div>
    </div>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            const slides = document.querySelectorAll('.slide');
            const prevBtn = document.getElementById('prevBtn');
            const nextBtn = document.getElementById('nextBtn');
            let currentSlide = 0;
            
            // Show initial slide
            showSlide(currentSlide);
            
            // Next button functionality
            nextBtn.addEventListener('click', function() {
                if (currentSlide < slides.length - 1) {
                    currentSlide++;
                    showSlide(currentSlide);
                }
            });
            
            // Previous button functionality
            prevBtn.addEventListener('click', function() {
                if (currentSlide > 0) {
                    currentSlide--;
                    showSlide(currentSlide);
                }
            });
            
            // Keyboard navigation
            document.addEventListener('keydown', function(e) {
                if (e.key === 'ArrowRight') {
                    if (currentSlide < slides.length - 1) {
                        currentSlide++;
                        showSlide(currentSlide);
                    }
                } else if (e.key === 'ArrowLeft') {
                    if (currentSlide > 0) {
                        currentSlide--;
                        showSlide(currentSlide);
                    }
                }
            });
            
            // Function to show a specific slide
            function showSlide(n) {
                slides.forEach(slide => slide.classList.remove('active'));
                slides[n].classList.add('active');
                
                // Update button states
                prevBtn.disabled = n === 0;
                nextBtn.disabled = n === slides.length - 1;
            }
        });
    </script>
</body>
</html>